{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"iChrisBirch Documentation","text":"<p>Welcome to the iChrisBirch application documentation.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>Quick Start Guide - Get running in under 5 minutes with modern Traefik deployment and browser-trusted HTTPS</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Project Layout - Overall project structure and organization</li> <li>Configuration - Environment and settings management</li> <li>Logging Configuration - Structlog setup, request tracing, and log viewing</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Setup - Getting started with development</li> <li>Testing - Testing strategy and guidelines</li> <li>Adding New Apps - How to add new features</li> </ul>"},{"location":"#docker-deployment","title":"Docker &amp; Deployment","text":"<ul> <li>Docker Development - Docker Compose development workflow</li> <li>Docker Documentation - Comprehensive Docker containerization guide</li> <li>Docker Architecture - Multi-stage Docker builds and container strategy</li> <li>Docker Compose - Service orchestration across environments</li> <li>Docker Quick Reference - Common commands and troubleshooting</li> </ul>"},{"location":"#traefik-reverse-proxy-modern-implementation","title":"Traefik Reverse Proxy (Modern Implementation)","text":"<ul> <li>CLI Management Guide - Simplified CLI interface with eliminated command duplication</li> <li>Traefik Deployment Guide - Modern reverse proxy with mkcert browser-trusted certificates</li> <li>Migration Complete Summary - nginx\u2192Traefik migration success with CLI simplification</li> </ul>"},{"location":"#devops","title":"DevOps","text":"<ul> <li>CI/CD - Continuous integration and deployment</li> <li>Homelab Deployment - Production deployment with Cloudflare Tunnel</li> <li>AWS - Cloud infrastructure and services (legacy)</li> <li>Terraform - Infrastructure as code</li> <li>Backups - Data backup and recovery</li> </ul>"},{"location":"#api","title":"API","text":"<ul> <li>API Documentation - Backend API reference</li> <li>Authentication Architecture - Modern API key authentication system</li> <li>Authentication Strategies - Auth implementation details</li> </ul>"},{"location":"#frontend","title":"Frontend","text":"<ul> <li>CSS - Styling guidelines and architecture</li> <li>CSS BEM - BEM methodology for CSS</li> <li>HTML5 Semantic - Semantic HTML structure</li> </ul>"},{"location":"#tools-utilities","title":"Tools &amp; Utilities","text":"<ul> <li>Alembic - Database migrations</li> <li>Scheduler - Background job processing</li> <li>Documentation Tools - Docs generation and maintenance</li> <li>Troubleshooting - Common issues and solutions</li> </ul> <p>Docs here</p>"},{"location":"AWS/","title":"AWS","text":""},{"location":"AWS/#iam","title":"IAM","text":""},{"location":"AWS/#groups","title":"Groups","text":"<p><code>admin</code> - Administrator access <code>developer</code> - Access to services</p> <ul> <li>GROUP: <code>admin</code></li> <li>POLICY: <code>AWSAdministratorAccess</code></li> <li>GROUP: <code>developer</code> group has POLICIES</li> <li>POLICY: <code>AllowPassRoleS3DatabaseBackups</code><ul> <li>Allow to assume the <code>S3DatabaseBackups</code> ROLE</li> <li>ROLE: <code>S3DatabaseBackups</code> - S3 Full Access</li> </ul> </li> <li>POLICY: <code>AWSKeyManagementServiceUser</code><ul> <li>Allow to view and use all KMS keys</li> <li>Allow grant to AWSResources (like S3)</li> </ul> </li> <li>POLICY: <code>AmazonRDSFullAccess</code></li> <li>POLICY: <code>AmazonS3FullAccess</code></li> <li>POLICY: <code>AmazonDynamoDBFullAccess</code></li> <li>ROLE: <code>AWSTrustedAdvisorRole</code></li> </ul> <p></p>"},{"location":"AWS/#ec2","title":"EC2","text":""},{"location":"AWS/#ichrisbirch-instances","title":"ichrisbirch instances","text":"<p>US East 1 Security Group: ichrisbirch-sg Key name: ichrisbirch-webserver Ubuntu 22.04</p>"},{"location":"add_new_app/","title":"Adding A New Application","text":"<p>For this document example we will be creating a new app called <code>Items</code></p>  db table <code>items</code>  sqlalchemy model <code>Item</code>  pydantic schema <code>Item</code>  app endpoint <code>/items</code>  api endpoint <code>/items/</code>"},{"location":"add_new_app/#sqlalchemy-model","title":"Sqlalchemy Model","text":"<p> Import new models into <code>ichrisbirch/alembic/env.py</code> </p> <p> Import new models into <code>ichrisbirch/models/__init__.py</code> For easy reference from the module level.</p> <pre><code>from ichrisbirch import models\n\nitem = models.Item(**data)\n</code></pre>"},{"location":"add_new_app/#pydantic-schema","title":"Pydantic Schema","text":"<p> Import new schemas into <code>ichrisbirch/schemas/__init__.py</code> For easy reference from the module level.</p> <pre><code>from ichrisbirch import schemas\n\nitem = schemas.ItemCreate(**data)\n</code></pre>"},{"location":"add_new_app/#application-blueprint","title":"Application Blueprint","text":""},{"location":"add_new_app/#app-routes","title":"App Routes","text":""},{"location":"add_new_app/#application-blueprint-to-app-factory","title":"Application Blueprint to App Factory","text":""},{"location":"add_new_app/#api-router","title":"API Router","text":""},{"location":"add_new_app/#api-endpoints","title":"API Endpoints","text":"<p> Import in <code>ichrisbirch/schemas/__init__.py</code> For easy reference from the module level.</p>"},{"location":"add_new_app/#api-router-to-api-factory","title":"API Router to API Factory","text":""},{"location":"add_new_app/#html-basehtml-and-indexhtml","title":"HTML base.html and index.html","text":""},{"location":"add_new_app/#navigation-link","title":"Navigation Link","text":"<p> Add link to navigation in <code>ichrisbirch/app/templates/base.html</code></p>"},{"location":"add_new_app/#stylesheet","title":"Stylesheet","text":""},{"location":"add_new_app/#tests","title":"Tests","text":""},{"location":"add_new_app/#testing-data","title":"Testing Data","text":"<p>:material-test: Add testing data into <code>tests/testing_data</code>  Import the test data in <code>tests/testing_data/__init__.py</code>  Add testing data to <code>tests/conftest.py/get_test_data()</code></p>"},{"location":"add_new_app/#api-endpoints-tests","title":"API Endpoints Tests","text":""},{"location":"add_new_app/#app-routes-tests","title":"App Routes Tests","text":""},{"location":"add_new_app/#frontend-tests","title":"Frontend Tests","text":""},{"location":"admin/","title":"Admin Dashboard","text":"<p>The admin dashboard provides administrative features for monitoring and managing the ichrisbirch application.</p>"},{"location":"admin/#access","title":"Access","text":"<p>URL: <code>https://app.docker.localhost/admin/</code> (development)</p> <p>Requirements:</p> <ul> <li>Must be logged in as an admin user</li> <li>Admin users have <code>is_admin=True</code> in the database</li> </ul>"},{"location":"admin/#features","title":"Features","text":""},{"location":"admin/#live-logs","title":"Live Logs","text":"<p>URL: <code>/admin/logs/</code></p> <p>Real-time log streaming from all services via WebSocket.</p>"},{"location":"admin/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Flask login sets JWT cookie: When you log in via the Flask app, a JWT token is requested from the API and stored as a cookie on the parent domain (e.g., <code>docker.localhost</code>)</p> </li> <li> <p>WebSocket connects with cookie auth: The JavaScript WebSocket client connects to <code>wss://api.docker.localhost/admin/log-stream/</code>. The browser automatically sends the <code>access_token</code> cookie.</p> </li> <li> <p>API validates JWT and streams logs: The WebSocket endpoint validates the JWT token, checks for admin privileges, then streams log lines from all services.</p> </li> </ol>"},{"location":"admin/#technical-details","title":"Technical Details","text":"<ul> <li>WebSocket endpoint: <code>/admin/log-stream/</code></li> <li>Authentication: JWT cookie (<code>access_token</code>)</li> <li>Admin check: User must have <code>is_admin=True</code></li> <li>Log source: Reads from <code>LOG_DIR</code> (default: <code>/var/log/ichrisbirch</code>)</li> <li>ANSI stripping: Color codes removed server-side for clean transmission</li> <li>Client colorization: JavaScript re-applies colors based on log level</li> </ul>"},{"location":"admin/#log-format","title":"Log Format","text":"<p>Logs use structlog's ConsoleRenderer format:</p> <pre><code>2026-01-14T08:14:21Z [info     ] user_login_success    filename=auth.py func_name=login lineno=45 user_id=123\n</code></pre> <p>The client-side JavaScript colorizes log levels:</p> <ul> <li><code>[debug   ]</code> - Green</li> <li><code>[info    ]</code> - Light blue</li> <li><code>[warning ]</code> - Yellow</li> <li><code>[error   ]</code> - Red</li> <li><code>[critical]</code> - Magenta</li> </ul>"},{"location":"admin/#log-graphs","title":"Log Graphs","text":"<p>URL: <code>/admin/log-graphs/</code></p> <p>Analytics and visualization of log data.</p>"},{"location":"admin/#capabilities","title":"Capabilities","text":"<ul> <li>Reads all <code>*.log</code> files from <code>LOG_DIR</code></li> <li>Parses structlog format into structured data with Polars</li> <li>Generates charts showing:</li> <li>Log count by level</li> <li>Log timeline</li> <li>Logs by source file</li> <li>Error patterns</li> </ul>"},{"location":"admin/#data-schema","title":"Data Schema","text":"<p>Each log line is parsed into:</p> Field Type Description <code>log_level</code> Categorical DEBUG, INFO, WARNING, ERROR, CRITICAL <code>timestamp</code> Datetime When the log was created <code>filename</code> String Source file name <code>func_name</code> String Function that logged <code>lineno</code> Int16 Line number <code>message</code> String Log message/event <code>source</code> String Log file source (api, app, scheduler, etc.)"},{"location":"admin/#traefik-dashboard","title":"Traefik Dashboard","text":"<p>Development URL: <code>https://dashboard.docker.localhost/</code> Credentials: <code>dev</code> / <code>devpass</code></p> <p>Test URL: <code>https://dashboard.test.localhost:8443/</code> Credentials: <code>test</code> / <code>testpass</code></p> <p>The Traefik dashboard provides:</p> <ul> <li>Real-time router and service status</li> <li>Request/response metrics</li> <li>Health check status</li> <li>Configuration details</li> </ul>"},{"location":"admin/#configuration","title":"Configuration","text":""},{"location":"admin/#enabling-file-logging","title":"Enabling File Logging","text":"<p>For the live logs feature to work, services must write to log files:</p> <pre><code># docker-compose.dev.yml\napi:\n  environment:\n    - LOG_FILE=/var/log/ichrisbirch/api.log\n  volumes:\n    - ichrisbirch_logs:/var/log/ichrisbirch\n\napp:\n  environment:\n    - LOG_FILE=/var/log/ichrisbirch/app.log\n  volumes:\n    - ichrisbirch_logs:/var/log/ichrisbirch\n</code></pre>"},{"location":"admin/#jwt-cookie-for-websocket-auth","title":"JWT Cookie for WebSocket Auth","text":"<p>The JWT cookie is automatically set when logging into the Flask app:</p> <pre><code># ichrisbirch/app/routes/auth.py\nresponse.set_cookie(\n    'access_token',\n    f'Bearer {access_token}',\n    httponly=True,\n    secure=request.is_secure,\n    samesite='Lax',\n    domain=parent_domain,  # e.g., 'docker.localhost'\n)\n</code></pre> <p>The cookie is set on the parent domain to allow cross-subdomain access (app.docker.localhost \u2192 api.docker.localhost).</p>"},{"location":"admin/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin/#live-logs-not-showing","title":"Live Logs Not Showing","text":"<ol> <li>Check LOG_FILE is set: Verify the environment variable is configured in docker-compose</li> <li>Check volume mount: Ensure <code>ichrisbirch_logs</code> volume is mounted to <code>/var/log/ichrisbirch</code></li> <li>Check admin access: Verify you're logged in as an admin user</li> <li>Check browser console: Look for WebSocket connection errors</li> </ol>"},{"location":"admin/#websocket-connection-failed","title":"WebSocket Connection Failed","text":"<ol> <li>Check JWT cookie: Open browser DevTools \u2192 Application \u2192 Cookies \u2192 Look for <code>access_token</code></li> <li>Check cookie domain: Cookie should be on parent domain (e.g., <code>docker.localhost</code>)</li> <li>Re-login: Log out and log back in to refresh the JWT cookie</li> <li>Check Traefik: Ensure Traefik is routing WebSocket connections correctly</li> </ol>"},{"location":"admin/#log-graphs-empty","title":"Log Graphs Empty","text":"<ol> <li>Check LOG_DIR exists: The directory <code>/var/log/ichrisbirch</code> must exist and contain <code>.log</code> files</li> <li>Check log format: Logs must be in structlog ConsoleRenderer format</li> <li>Check file permissions: The app user must be able to read the log files</li> </ol>"},{"location":"admin/#api-endpoints","title":"API Endpoints","text":""},{"location":"admin/#websocket-log-stream","title":"WebSocket Log Stream","text":"<pre><code>GET wss://api.docker.localhost/admin/log-stream/\n</code></pre> <p>Authentication: <code>access_token</code> cookie with valid JWT</p> <p>Response: Stream of log lines (one per WebSocket message)</p> <p>Close codes:</p> <ul> <li><code>1008</code> (Policy Violation): No token, invalid token, or non-admin user</li> <li><code>1000</code> (Normal): Client disconnected</li> </ul>"},{"location":"admin/#log-graphs-data","title":"Log Graphs Data","text":"<p>The log graphs page uses server-side rendering with Polars DataFrames, so there's no separate API endpoint for the data.</p>"},{"location":"alembic/","title":"Alembic Revision","text":"<p>Run in <code>ichrisbirch/ichrisbirch</code> (where <code>alembic.ini</code> is located)</p>"},{"location":"alembic/#first-run","title":"First Run","text":"<pre><code>export ENVIRONMENT='development' # or 'production'\nalembic revision --autogenerate -m 'Create initial tables'\nalembic upgrade head\n</code></pre>"},{"location":"alembic/#subsequent-runs","title":"Subsequent Runs","text":"<ol> <li> <p>Make the changes to the models and schemas</p> </li> <li> <p>Run a revision to pickup changes in code <code>alembic revision --autogenerate -m 'Add notes field to tasks table'</code></p> <p>Note If this doesn't work perfectly, you must edit the revision file</p> </li> <li> <p>Run the upgrade in the environments</p> </li> </ol> <p>Locally</p> <pre><code>export ENVIRONMENT='development'\nalembic upgrade head\n</code></pre> <p>EC2</p> <pre><code>export ENVIRONMENT='production'\nalembic upgrade head\n</code></pre>"},{"location":"alembic/#troubleshooting","title":"Troubleshooting","text":"<p>Error Alembic is not able to upgrade to the latest because the revisions got out of sync.  </p> <p>Solution Find the last revision that was successfully run (manually by inspecting the database) and then run: <code>alembic stamp &lt;revision&gt;</code> to set the current revision to the last successful one. Then run the upgrade again: <code>alembic upgrade head</code></p>"},{"location":"alembic/#sqlalchemy-create_all-vs-alembic-upgrade","title":"sqlalchemy create_all vs alembic upgrade","text":"<p><code>SQLAlchemy</code> and <code>Alembic</code> are two powerful tools in the Python ecosystem used for database handling and migrations, respectively. They are often used together in projects to manage database schemas and perform database operations. Understanding the difference between <code>SQLAlchemy</code>'s <code>create_all</code> method and <code>Alembic</code>'s <code>upgrade</code> function is crucial for effectively managing database schema changes and migrations.</p>"},{"location":"alembic/#sqlalchemycreate_all","title":"<code>SQLAlchemy.create_all</code>","text":"<p><code>SQLAlchemy</code> is an SQL toolkit and Object-Relational Mapping (ORM) library for Python. It provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access.</p> <ul> <li>What it does: The <code>create_all</code> method in <code>SQLAlchemy</code> is used to create all tables that have been defined in your SQLAlchemy models but don't yet exist in the database. It doesn't consider the current state of the database schema. Instead, it blindly attempts to create all the tables (and associated schema elements like indexes) based on the models you've defined. If a table already exists, it simply skips the creation for that table.</li> <li>Usage scenario: <code>create_all</code> is particularly useful in simple projects or during the initial setup of a project's database where you are starting with an empty database and want to construct the schema based on your models' definitions.</li> </ul>"},{"location":"alembic/#alembicupgrade","title":"<code>Alembic.upgrade</code>","text":"<p><code>Alembic</code> is a lightweight database migration tool for usage with <code>SQLAlchemy</code>. It allows you to manage changes to your database schema over time, enabling versioning of your database similarly to how you version your source code.</p> <ul> <li>What it does: The <code>upgrade</code> function in Alembic applies one or more migrations (changes) to the database schema, moving it to a new version. These migrations are written as scripts which define how to apply a change (e.g., add a table, alter a column) and how to revert it. The <code>upgrade</code> command considers the current version of your database and applies all new migrations in sequence up to the latest version or to a specified version.</li> <li>Usage scenario: <code>Alembic.upgrade</code> is used in iterative development and production environments where the state of the database schema evolves over time. It ensures that schema changes are applied in a controlled and versioned manner, allowing for smooth transitions across different versions of your schema as your application grows and changes.</li> </ul>"},{"location":"alembic/#key-differences","title":"Key Differences","text":"<ul> <li>Version control: Alembic allows for version-controlled schema changes, making it possible to migrate your database schema forwards or backwards as needed. <code>SQLAlchemy.create_all</code> does not consider versions of your schema.</li> <li>Sensitivity to existing schema: <code>create_all</code> essentially ignores the current schema state (it won't modify or delete existing tables), while Alembic <code>upgrade</code> scripts can be tailored to alter the current schema precisely and incrementally.</li> <li>Purpose and scope: <code>create_all</code> is a more blunt instrument, best suited for initial schema creation. Alembic, with its <code>upgrade</code> (and corresponding <code>downgrade</code>) commands, supports a more nuanced and controlled approach to database schema evolution.</li> </ul> <p>In summary, while <code>SQLAlchemy.create_all</code> is useful for initial schema creation in simple scenarios, <code>Alembic.upgrade</code> provides a robust framework for managing schema changes over time in a version-controlled, incremental, and reversible manner. For complex projects and in production environments, integrating Alembic for migration management alongside SQLAlchemy for ORM capabilities is considered best practice.</p>"},{"location":"alembic/#how-to-deal-with-database-that-has-got-out-of-sync-with-alembic-revisions-and-alembic-report-target-database-is-not-up-to-date-how-to-find-what-version-of-the-revision-the-database-matches","title":"How to deal with database that has got out of sync with alembic revisions and alembic report Target database is not up to date.  How to find what version of the revision the database matches","text":"<p>When your database schema has gotten out of sync with Alembic revisions, the message \"Target database is not up to date\" typically indicates Alembic detects mismatches between the expected schema version (from your migration scripts) and the current state of your database. Handling this scenario involves a few steps to identify the disparity and resolve it. Here's how you can approach this situation:</p>"},{"location":"alembic/#1-identify-current-database-version","title":"1. Identify Current Database Version","text":"<p>First, check the current schema version of your database. Alembic uses a table (<code>alembic_version</code> by default) to track the current revision of the schema in your database.</p> <p>You can manually check this table:</p> <pre><code>SELECT * FROM alembic_version;\n</code></pre> <p>Or use Alembic's <code>current</code> command:</p> <pre><code>alembic current\n</code></pre> <p>This command displays the current revision that the database is on.</p>"},{"location":"alembic/#2-compare-with-alembic-revision-history","title":"2. Compare with Alembic Revision History","text":"<p>Next, list all the revisions known to Alembic to see where the current database version stands in relation to the migration history.</p> <p>Run the following command to show your migrations history:</p> <pre><code>alembic history\n</code></pre> <p>This command will print a list of revisions. Find where the revision from your database fits within this list. This will inform you whether the database is ahead, behind, or has diverged (if the current revision doesn't exist in your migration chain).</p>"},{"location":"alembic/#3-identify-divergences-or-missing-revisions","title":"3. Identify Divergences or Missing Revisions","text":"<p>If the database's current revision doesn't exist in the migration history from <code>alembic history</code>, it suggests that the database might have applied a revision that has since been deleted or was created from a different branch of your code.</p> <p>In cases where the database is behind, and simply applying newer migrations is required, you can proceed to use <code>alembic upgrade</code> with the target revision you want to apply.</p> <p>However, if the database is ahead or has diverged, you need to assess how to reconcile the differences.</p>"},{"location":"alembic/#4-resolving-divergences","title":"4. Resolving Divergences","text":""},{"location":"alembic/#option-a-manually-adjust-the-databaseschema","title":"Option A: Manually adjust the database/schema","text":"<p>If possible, manually adjust the database schema to match a specific version in your Alembic history, and use <code>alembic stamp</code> to mark it as such, without applying migrations. This can be necessary if the database was manually altered.</p> <pre><code>alembic stamp &lt;revision_id&gt;\n</code></pre>"},{"location":"alembic/#option-b-generate-a-new-migration","title":"Option B: Generate a new migration","text":"<p>You can attempt to autogenerate a new migration that takes into account the current state of the database and the desired state as per your models.</p> <p>Run:</p> <pre><code>alembic revision --autogenerate -m \"Fixing divergence\"\n</code></pre> <p>Review the generated script closely, as it may not be perfect and could require manual adjustments.</p>"},{"location":"alembic/#option-c-revert-to-a-known-good-state","title":"Option C: Revert to a known good state","text":"<p>If all else fails and you have the ability to reset your database to a known good state (usually viable only in development or with available backups), you can revert the database back to a specific revision and reapply migrations from there.</p>"},{"location":"alembic/#5-apply-missing-migrations","title":"5. Apply Missing Migrations","text":"<p>After resolving discrepancies, ensure that your database is at the correct revision and that all necessary migrations are applied.</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"alembic/#caution","title":"Caution","text":"<p>Dealing with schema mismatches can lead to data loss or corruption if not handled carefully. Always back up your database before attempting to resolve discrepancies and thoroughly test any changes in a safe environment prior to application in production.</p> <p>Navigating database schema issues can be challenging, but by systematically checking the current state, comparing it with your migration history, and carefully applying corrections, you can resolve discrepancies and get your project back on track.</p>"},{"location":"alembic/#continue","title":"continue","text":""},{"location":"alembic/#6-validate-schema-after-adjustments","title":"6. Validate Schema After Adjustments","text":"<p>After applying the necessary adjustments and migrations, it\u2019s essential to validate that the database schema now matches the expectations of your application and Alembic migrations. Conduct thorough testing to ensure that:</p> <ul> <li>All expected tables, columns, indexes, and other database objects are correctly defined.</li> <li>The application can successfully interact with the database without encountering schema-related errors.</li> <li>Running <code>alembic current</code> reports the correct and expected revision.</li> </ul>"},{"location":"alembic/#7-consider-future-prevention-strategies","title":"7. Consider Future Prevention Strategies","text":"<p>To avoid future discrepancies between your database schema and Alembic revisions, consider implementing strategies that ensure better synchronization and tracking:</p> <ul> <li>Continuous Integration (CI) Checks: Implement CI pipelines that run tests against a database that is always migrated from scratch using current Alembic migrations. This helps catch issues where migrations do not match the application models or are missing.</li> <li>Code Review Processes: Incorporate migration script reviews as part of your development process to catch potential issues early on.</li> <li>Restricted Database Access: Limit who can manually alter the database schema. Ideally, all changes should go through Alembic migrations to ensure that the schema evolution is recorded and versioned.</li> <li>Documenting Manual Changes: In the unavoidable scenario where manual database changes are made, document these changes meticulously. Consider creating corresponding Alembic migrations, even if they are marked as already applied, to ensure the migration history remains an accurate record of the schema's evolution.</li> </ul>"},{"location":"alembic/#8-additional-tools-and-practices","title":"8. Additional Tools and Practices","text":"<ul> <li>Alembic Autogenerate Revisions: While the <code>--autogenerate</code> feature is powerful, it should not be blindly trusted. Always review the generated migration scripts to ensure they accurately represent the desired schema changes and do not inadvertently drop or alter objects.</li> <li>Model Comparison Extensions: For complex projects, consider using or developing tools that help compare the SQLAlchemy models directly against the actual database schema, identifying discrepancies without relying solely on Alembic's version history.</li> <li>Regular Audits: Schedule regular audits of your database schema versus your models and migrations. This proactive approach can help identify issues before they become problematic.</li> <li>Environment Parity: Aim for parity between your development, staging, and production environments in terms of how migrations are applied and managed, reducing the risk of discrepancies arising from differences in how environments are handled.</li> </ul>"},{"location":"alembic/#conclusion","title":"Conclusion","text":"<p>Getting a database schema back in sync with Alembic revisions involves careful diagnosis, proper tool usage, and strategic resolution of discrepancies. It requires a clear understanding of your current schema state, how it deviates from the expected state, and the steps needed to safely reconcile these differences. Implementing preventative strategies and maintaining meticulous records of changes are key to minimizing future synchronization issues, ensuring that your database schema evolution remains manageable, trackable, and aligned with your application's requirements.</p>"},{"location":"alembic/#how-does-alembic-determine-the-order-of-migrations","title":"How Does Alembic Determine the Order of Migrations","text":"<p>Alembic determines the order in which to apply migrations using a couple of key concepts: revision identifiers and down_revision attributes within the migration scripts. These elements create a directed acyclic graph (DAG) of migrations, establishing a clear lineage or path through your migration history. Here's how these components work together to manage migration order:</p>"},{"location":"alembic/#revision-identifiers","title":"Revision Identifiers","text":"<p>Each Alembic migration script is assigned a unique revision identifier (often a hash) when the migration is generated. This identifier uniquely distinguishes each migration in the series of changes made over time.</p>"},{"location":"alembic/#down-revision-attribute","title":"Down Revision Attribute","text":"<p>Within each migration script, there's an attribute named <code>down_revision</code>. This attribute specifies the identifier of the migration that directly precedes the current one in the migration history. The <code>down_revision</code> effectively points back to the migration's parent in the version history tree.</p> <p>For the very first migration in a project, <code>down_revision</code> will be <code>None</code>, indicating that there is no parent migration (i.e., it's the root of the migration tree).</p>"},{"location":"alembic/#upgrade-and-downgrade-sequences","title":"Upgrade and Downgrade Sequences","text":"<p>Given these two components, Alembic constructs a sequence of migrations:</p> <ul> <li> <p>Upgrade: To migrate forward, Alembic starts from the earliest migration whose <code>down_revision</code> is <code>None</code> and follows the chain of <code>revision</code> to <code>down_revision</code> links, applying each migration in turn until it reaches the specified target migration or the latest migration if no target is specified.</p> </li> <li> <p>Downgrade: For migrating backward, Alembic reverses the process, using the current revision as a starting point and following the chain of <code>down_revision</code> values in reverse to apply the <code>downgrade()</code> operations defined in each migration script, until it reaches the specified target revision.</p> </li> </ul>"},{"location":"alembic/#handling-branches","title":"Handling Branches","text":"<p>Alembic also supports branching in migrations. When branches are present, there may be multiple migration scripts with the same <code>down_revision</code>. In this scenario, Alembic uses a \"merge\" migration to bring the divergent branches back into a single linear path. The merge migration specifies multiple <code>down_revision</code> values, identifying each of the branch tips that it reconciles.</p> <p>When applying migrations:</p> <ol> <li> <p>Linear Migrations: In simple, linear migrations, Alembic applies migrations in the straightforward sequence dictated by the single parent-child (<code>down_revision</code> to <code>revision</code>) relationships.</p> </li> <li> <p>Branched Migrations: In branched scenarios, Alembic will apply migrations from each branch as required, until it encounters a merge point. At the merge point, it ensures that all required branches are up to date before applying the merge migration, thus reconciling the branches and continuing forward in a linear fashion from there.</p> </li> </ol>"},{"location":"alembic/#version-table","title":"Version Table","text":"<p>Alembic tracks the current version of the database schema in the <code>alembic_version</code> table, recording which migrations have been applied. This table is crucial for determining the starting point for any migration operation, be it an upgrade or downgrade.</p> <p>In summary, Alembic determines the order of migrations through a combination of unique revision identifiers, parent-child (down_revision) relationships creating a logical sequence, and support for merging branched histories. This structure allows Alembic to manage complex migrations histories with precision and ensure the database schema evolves coherently with the application's requirements.</p>"},{"location":"alembic/#removing-a-revision-that-has-not-been-applied-to-the-database","title":"Removing a Revision That Has Not Been Applied to the Database","text":"<p>If you have an Alembic revision that hasn't been applied to any database yet and you wish to remove it, the process is fairly straightforward since you only need to deal with the revision script(s) in your migrations folder. Here's how you can do it:</p>"},{"location":"alembic/#steps-to-remove-an-unapplied-alembic-revision","title":"Steps to Remove an Unapplied Alembic Revision","text":"<ol> <li> <p>Locate the Revision File: In your project, navigate to the <code>versions</code> directory within your Alembic migrations folder. This folder contains all the revision scripts generated by Alembic.</p> </li> <li> <p>Identify the Revision Script: Each file in the <code>versions</code> directory corresponds to a specific revision. The filename usually starts with the revision ID (a sequence of letters and numbers generated by Alembic) followed by an underscore and a brief description of the migration, e.g., <code>ae1027a6acf_migration_description.py</code>. Identify the script file for the revision you wish to remove. Make sure this is the correct revision by opening the file and verifying its contents, including the revision ID, the <code>down_revision</code>, and the changes it introduces.</p> </li> <li> <p>Delete the Revision File: Simply delete the identified Python script file from the <code>versions</code> directory. This removes the revision from your migrations history, as far as Alembic is concerned.</p> </li> <li> <p>Check if Downstream Revisions Exist: If the revision you're removing has \"child\" revisions (i.e., revisions that list it as their <code>down_revision</code>), you will need to decide how to handle those. You cannot simply delete a revision if later revisions depend on it without risking inconsistencies in your migration path. If such downstream revisions exist, consider the following options:</p> </li> <li>Delete the Downstream Revisions Too: If the downstream revisions also haven't been applied and aren't necessary, you can delete them as well.</li> <li> <p>Rebase the Downstream Revisions: If the downstream revisions need to be kept, you may need to edit their <code>down_revision</code> attributes to reflect the removal of the parent revision. This might involve setting their <code>down_revision</code> to the removed revision's parent or to a new merge revision if the history is more complex.</p> </li> <li> <p>Regenerate Dependency Graph (Optional): If you modified the <code>down_revision</code> of any subsequent migrations, or if you're not sure about the consistency of your migration scripts, you might want to regenerate the Alembic dependency graph. However, this is more about verifying that your revisions are consistent and there are no \u201corphaned\u201d migrations. Alembic doesn't automatically generate a visual graph, but you can check consistency by running <code>alembic history</code> to make sure it outputs a coherent history from your base revision to the head, without any missing links.</p> </li> <li> <p>Update Database Schema Manually if Necessary: If the deleted migration or any of its downstream migrations had been applied to any other environment's database (development, staging, etc.), you'll need to manually adjust those database schemas and possibly the <code>alembic_version</code> table to ensure consistency. This step applies only if the migration was mistakenly said to be unapplied when, in fact, it had been applied somewhere.</p> </li> </ol>"},{"location":"alembic/#delete-caution","title":"Delete Caution","text":"<ul> <li>Be extra careful to ensure that the migration has indeed not been applied to any environment. Removing applied migrations can lead to inconsistencies and errors.</li> <li>Always have a backup of your database and current migration scripts before deleting or modifying them.</li> <li>Remember to communicate with your team about any changes to the migration scripts, especially if other developers might have applied the deleted migration in their local environment.</li> </ul> <p>In summary, removing an unapplied Alembic revision is as simple as deleting its script file from the <code>versions</code> directory, but care should be taken to handle dependency and consistency issues that might arise from doing so.</p>"},{"location":"authentication-architecture/","title":"Authentication Architecture Migration - Complete","text":""},{"location":"authentication-architecture/#overview","title":"Overview","text":"<p>The iChrisBirch project has successfully migrated from database-based service accounts to modern API key authentication following industry best practices.</p>"},{"location":"authentication-architecture/#migration-summary","title":"Migration Summary","text":""},{"location":"authentication-architecture/#previous-architecture-deprecated","title":"Previous Architecture (Deprecated)","text":"<ul> <li>Database service accounts: <code>APIServiceAccount</code> class managing database users</li> <li>Mixed authentication patterns: Inconsistent auth across different modules  </li> <li>Test infrastructure issues: Broken imports and dependencies</li> <li>Circular dependencies: Service account creation caused recursive loops</li> </ul>"},{"location":"authentication-architecture/#current-architecture","title":"Current Architecture \u2705","text":"<p>Modern API Key Authentication:</p> <ul> <li>Internal Service Auth: <code>X-Internal-Service</code> + <code>X-Service-Key</code> headers</li> <li>User Authentication: JWT tokens, OAuth2, and application headers</li> <li>Consistent client usage: <code>LoggingAPIClient</code> throughout codebase</li> <li>Professional logging: Extensive debugging capabilities preserved</li> </ul>"},{"location":"authentication-architecture/#authentication-methods","title":"Authentication Methods","text":""},{"location":"authentication-architecture/#internal-service-authentication","title":"Internal Service Authentication","text":"<p>Used for service-to-service communication:</p> <pre><code># Headers-based authentication\nheaders = {\n    'X-Internal-Service': 'scheduler',\n    'X-Service-Key': settings.auth.internal_service_key\n}\n\n# Modern client usage\nwith logging_internal_service_client() as client:\n    users = client.resource('users', schemas.User)\n    user = users.get_by_email(email)\n</code></pre>"},{"location":"authentication-architecture/#user-authentication","title":"User Authentication","text":"<p>Multiple methods supported:</p> <ul> <li>JWT Tokens: Standard Bearer token authentication</li> <li>Application Headers: <code>X-User-ID</code> + <code>X-Application-ID</code></li> <li>OAuth2: External authentication provider support</li> </ul>"},{"location":"authentication-architecture/#factory-functions","title":"Factory Functions","text":"<p>Standardized client creation:</p> <pre><code>from ichrisbirch.api.client.logging_client import (\n    logging_internal_service_client,\n    logging_user_client,\n    logging_flask_session_client\n)\n\n# Internal service operations\nwith logging_internal_service_client() as client:\n    # Service-to-service calls\n\n# User-specific operations  \nwith logging_user_client(user_token) as client:\n    # User-specific API calls\n\n# Flask session operations\nwith logging_flask_session_client() as client:\n    # Flask login integration\n</code></pre>"},{"location":"authentication-architecture/#key-components","title":"Key Components","text":""},{"location":"authentication-architecture/#loggingapiclient","title":"LoggingAPIClient","text":"<p>Modern replacement for QueryAPI with identical interface:</p> <ul> <li>Extensive logging: Environment info, auth details, request/response logging</li> <li>Custom methods preserved: <code>get_generic()</code>, <code>post_action()</code> functionality</li> <li>Context management: Proper session lifecycle management</li> <li>Error handling: Comprehensive exception handling and logging</li> </ul>"},{"location":"authentication-architecture/#fastapi-endpoints","title":"FastAPI Endpoints","text":"<p>Authentication dependencies:</p> <ul> <li><code>CurrentUser</code>: Requires valid user authentication</li> <li><code>AdminUser</code>: Requires admin user authentication  </li> <li><code>AdminOrInternalServiceAccess</code>: Allows admin users OR internal service auth</li> <li>Internal service verification: <code>verify_internal_service()</code> dependency</li> </ul>"},{"location":"authentication-architecture/#flask-integration","title":"Flask Integration","text":"<p>Seamless integration with Flask login system:</p> <ul> <li>User loading: Internal service client for user lookups</li> <li>Session management: Proper Flask session integration</li> <li>Login/logout: Standard Flask-Login patterns preserved</li> </ul>"},{"location":"authentication-architecture/#configuration","title":"Configuration","text":""},{"location":"authentication-architecture/#environment-variables","title":"Environment Variables","text":"<p>Required settings in all environments:</p> <pre><code># Internal service authentication\nAUTH_INTERNAL_SERVICE_KEY=&lt;secure-api-key&gt;\n\n# API endpoints  \nAPI_URL=http://localhost:8000  # or appropriate URL for environment\n</code></pre>"},{"location":"authentication-architecture/#settings-structure","title":"Settings Structure","text":"<pre><code># ichrisbirch/config.py\nclass AuthSettings:\n    def __init__(self):\n        self.internal_service_key = os.environ['AUTH_INTERNAL_SERVICE_KEY']\n        # Other auth settings...\n\nclass Settings:\n    def __init__(self):\n        self.auth = AuthSettings()\n        self.api_url = os.environ['API_URL']\n</code></pre>"},{"location":"authentication-architecture/#usage-patterns","title":"Usage Patterns","text":""},{"location":"authentication-architecture/#chat-authentication","title":"Chat Authentication","text":"<pre><code># ichrisbirch/chat/auth.py\nwith logging_internal_service_client() as client:\n    users = client.resource('users', schemas.User)\n    if user_data := users.get_generic(['email', username]):\n        user = models.User(**user_data)\n        return user\n</code></pre>"},{"location":"authentication-architecture/#flask-login-integration","title":"Flask Login Integration","text":"<pre><code># ichrisbirch/app/login.py\ndef get_users_api():\n    return logging_internal_service_client(base_url=settings.api_url)\n\n@login_manager.user_loader  \ndef load_user(alternative_id):\n    with get_users_api() as client:\n        users = client.resource('users', schemas.User)\n        return users.get_generic(['alt', alternative_id])\n</code></pre>"},{"location":"authentication-architecture/#scheduler-jobs","title":"Scheduler Jobs","text":"<pre><code># Service jobs use internal authentication\nwith logging_internal_service_client() as client:\n    habits = client.resource('habits', schemas.Habit)\n    active_habits = habits.list(params={'current': True})\n</code></pre>"},{"location":"authentication-architecture/#security-features","title":"Security Features","text":""},{"location":"authentication-architecture/#internal-service-protection","title":"Internal Service Protection","text":"<ul> <li>API key validation: Cryptographically secure key verification</li> <li>Service identification: Named service identification in headers</li> <li>Request logging: Comprehensive audit trail for service calls</li> <li>Rate limiting ready: Infrastructure prepared for rate limiting</li> </ul>"},{"location":"authentication-architecture/#user-access-controls","title":"User Access Controls","text":"<ul> <li>Role-based access: Admin vs regular user permissions</li> <li>Own-data restrictions: Users can only access their own data</li> <li>Cross-user prevention: Prevents unauthorized access to other users' data</li> <li>Admin overrides: Admin users can access all data when appropriate</li> </ul>"},{"location":"authentication-architecture/#benefits-achieved","title":"Benefits Achieved","text":""},{"location":"authentication-architecture/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>\u2705 Industry standard authentication: API keys instead of database users</li> <li>\u2705 Consistent patterns: Same authentication method across all modules</li> <li>\u2705 Clean architecture: No circular dependencies or global state issues</li> <li>\u2705 Professional logging: Enhanced debugging and monitoring capabilities</li> </ul>"},{"location":"authentication-architecture/#operational-benefits","title":"Operational Benefits","text":"<ul> <li>\u2705 Simplified deployment: No database user creation required</li> <li>\u2705 Better security: API keys are more secure than database accounts</li> <li>\u2705 Easier maintenance: Single authentication configuration point</li> <li>\u2705 Scalability ready: Authentication works across distributed systems</li> </ul>"},{"location":"authentication-architecture/#developer-experience","title":"Developer Experience","text":"<ul> <li>\u2705 Clear patterns: Consistent client usage across all code</li> <li>\u2705 Better testing: Clean test patterns without database dependencies</li> <li>\u2705 Enhanced debugging: Extensive logging preserved and improved</li> <li>\u2705 Modern tooling: Following current industry best practices</li> </ul>"},{"location":"authentication-architecture/#migration-complete","title":"Migration Complete","text":"<p>All components successfully migrated:</p> <ul> <li>Chat authentication: \u2705 Using modern internal service client</li> <li>Flask login system: \u2705 Integrated with LoggingAPIClient  </li> <li>API endpoints: \u2705 Support internal service and user authentication</li> <li>Test infrastructure: \u2705 Clean test patterns without service account dependencies</li> <li>Configuration: \u2705 Simplified environment variable configuration</li> </ul> <p>The authentication architecture is now modern, secure, and follows industry best practices while maintaining all existing functionality and debugging capabilities.</p>"},{"location":"backups/","title":"Backups","text":"<p>PostgreSQL database backups are managed through Python classes that handle backup creation, S3 upload, metadata tracking, and restoration.</p>"},{"location":"backups/#cli-commands","title":"CLI Commands","text":"<pre><code># Create a backup (uploads to S3 by default)\nicb db backup pre-migration\n\n# Create a backup and keep local copy\nicb db backup pre-migration --save-local\n\n# Create local-only backup (skip S3 upload)\nicb db backup pre-migration --skip-upload --save-local\n\n# List recent backups in S3\nicb db list\n\n# Restore from backup\nicb db restore latest --target-host localhost --target-port 5432 --target-username postgres --target-password secret\nicb db restore backup-2024-01-15.dump --target-host db.example.com --target-port 5432 --target-username postgres --target-password secret\n</code></pre>"},{"location":"backups/#web-interface","title":"Web Interface","text":"<p>Admin users can create and view backups from the Flask admin panel:</p> <ul> <li>URL: <code>/admin/backups/</code></li> <li>Features: Create backups, browse S3 bucket contents, navigate folders</li> </ul>"},{"location":"backups/#api-endpoints","title":"API Endpoints","text":"<p>Admin-authenticated endpoints for backup operations:</p> <pre><code># Create backup\nPOST /admin/backups/\n{\n  \"description\": \"pre-migration\",\n  \"upload_to_s3\": true,\n  \"save_local\": false\n}\n\n# List backup history\nGET /admin/backups/\n\n# Get specific backup\nGET /admin/backups/{id}/\n</code></pre>"},{"location":"backups/#scheduled-backups","title":"Scheduled Backups","text":"<p>The scheduler service creates automatic daily backups at 1:30 AM. See <code>ichrisbirch/scheduler/jobs.py</code>.</p>"},{"location":"backups/#architecture","title":"Architecture","text":""},{"location":"backups/#databasebackup-class","title":"DatabaseBackup Class","text":"<p>Located in <code>ichrisbirch/database/backup.py</code>:</p> <ul> <li>Creates PostgreSQL dumps using <code>pg_dump</code></li> <li>Collects metadata: table snapshots, database size, postgres version</li> <li>Computes SHA256 checksum</li> <li>Uploads to S3 with environment-based prefix</li> <li>Persists backup history to <code>admin.backup_history</code> table</li> </ul>"},{"location":"backups/#databaserestore-class","title":"DatabaseRestore Class","text":"<p>Located in <code>ichrisbirch/database/restore.py</code>:</p> <ul> <li>Downloads backups from S3</li> <li>Restores using <code>pg_restore</code></li> <li>Records restore operations to <code>admin.backup_restore</code> table</li> </ul>"},{"location":"backups/#backup-history","title":"Backup History","text":"<p>All backups are tracked in the <code>admin.backup_history</code> table with:</p> <ul> <li>Filename, description, backup type (manual/scheduled)</li> <li>Size, duration, checksum</li> <li>S3 key and/or local path</li> <li>Table snapshot (row counts per table)</li> <li>PostgreSQL version and database size</li> <li>Success status and error messages</li> </ul>"},{"location":"backups/#s3-bucket-structure","title":"S3 Bucket Structure","text":"<pre><code>{bucket}/\n  development/\n    postgres/\n      backup-2024-01-15T1200-description.dump\n  testing/\n    postgres/\n      ...\n  production/\n    postgres/\n      ...\n</code></pre>"},{"location":"backups/#running-backups-directly","title":"Running Backups Directly","text":"<p>For advanced usage, backups can be run as Python modules:</p> <pre><code># With all options visible\npython -m ichrisbirch.database.backup --description \"pre-migration\" --save-local\npython -m ichrisbirch.database.restore --filename latest --target-host localhost ...\n</code></pre>"},{"location":"cicd/","title":"CI/CD Pipeline","text":"<p>This document details the Continuous Integration and Continuous Deployment workflows for the ichrisbirch project.</p>"},{"location":"cicd/#overview","title":"Overview","text":"<p>The project uses GitHub Actions for CI/CD with the following workflows:</p> Workflow File Trigger Purpose Validate Project <code>validate-project.yml</code> Push to master, PRs Run linting, type checking, and tests Deploy MkDocs <code>deploy-docs.yml</code> Push to master (docs changes) Build and deploy documentation Sync Documentation Code <code>sync-docs.yml</code> Push to master Sync code snippets in documentation"},{"location":"cicd/#validate-project-workflow","title":"Validate Project Workflow","text":"<p>The main CI workflow that validates code quality and runs tests.</p>"},{"location":"cicd/#workflow-stages","title":"Workflow Stages","text":"<pre><code>1. Checkout &amp; Setup\n   \u251c\u2500\u2500 Checkout repository\n   \u251c\u2500\u2500 Set up Python 3.13\n   \u251c\u2500\u2500 Install UV package manager\n   \u2514\u2500\u2500 Install dependencies (uv sync)\n\n2. Code Quality\n   \u251c\u2500\u2500 Ruff linting\n   \u251c\u2500\u2500 Ruff formatting check\n   \u2514\u2500\u2500 MyPy type checking\n\n3. Docker Environment Setup\n   \u251c\u2500\u2500 Set up Docker Compose\n   \u251c\u2500\u2500 Configure AWS credentials (OIDC)\n   \u251c\u2500\u2500 Create Docker network (proxy)\n   \u2514\u2500\u2500 Start Docker Compose containers\n\n4. Test Execution\n   \u251c\u2500\u2500 Run pytest with coverage\n   \u2514\u2500\u2500 Generate coverage reports\n\n5. Cleanup\n   \u2514\u2500\u2500 Stop Docker Compose containers\n</code></pre>"},{"location":"cicd/#docker-compose-in-ci","title":"Docker Compose in CI","text":"<p>The CI environment uses a special Docker Compose configuration that differs from local development:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.test.yml -f docker-compose.ci.yml \\\n  --project-name icb-test up -d\n</code></pre> <p>Three compose files are layered:</p> <ol> <li><code>docker-compose.yml</code> - Base production configuration</li> <li><code>docker-compose.test.yml</code> - Test-specific overrides (different ports, tmpfs for speed)</li> <li><code>docker-compose.ci.yml</code> - CI-specific overrides (removes local bind mounts)</li> </ol>"},{"location":"cicd/#ci-specific-configuration-docker-composeciyml","title":"CI-Specific Configuration (<code>docker-compose.ci.yml</code>)","text":"<p>The CI override file addresses differences between local and CI environments:</p> Issue Local Development CI Environment CI Override Solution AWS credentials <code>~/.config/aws</code> mounted Environment variables via OIDC Remove bind mount Docker network External <code>proxy</code> network Network doesn't exist Create as bridge network File mounts Local paths exist Only repo checkout Use volumes only Traefik dashboard Enabled with auth Not needed Disabled"},{"location":"cicd/#container-startup-sequence","title":"Container Startup Sequence","text":"<p>The workflow starts containers in a specific order to handle dependencies:</p> <pre><code># Phase 1: Start database services first\ndocker compose ... up -d --build postgres redis\nsleep 10  # Wait for health checks\n\n# Phase 2: Start application services\ndocker compose ... up -d --build api app chat scheduler\nsleep 30  # Wait for services to initialize\n</code></pre> <p>Why this order matters:</p> <ul> <li><code>postgres</code> and <code>redis</code> must be healthy before application services start</li> <li><code>api</code> initializes the shared virtual environment used by other services</li> <li><code>scheduler</code> creates the <code>apscheduler_jobs</code> table needed by scheduler tests</li> <li>Sleep intervals allow health checks to complete</li> </ul>"},{"location":"cicd/#test-environment-detection","title":"Test Environment Detection","text":"<p>The test fixtures detect CI environment and adjust behavior:</p> <pre><code># In tests/environment.py\n@property\ndef is_ci(self) -&gt; bool:\n    return os.environ.get('CI', '').lower() == 'true'\n</code></pre> <p>CI-specific behavior:</p> <ul> <li>Skip Docker Compose startup (containers pre-started by workflow)</li> <li>Skip Docker Compose teardown (handled by workflow cleanup step)</li> <li>Longer wait times for container health checks</li> </ul>"},{"location":"cicd/#environment-variables","title":"Environment Variables","text":"<p>Key environment variables set in CI:</p> <pre><code>env:\n  ENVIRONMENT: testing\n  CI: true  # Detected by test fixtures\n  AWS_REGION: us-east-2\n  # AWS credentials via OIDC (not stored as secrets)\n</code></pre>"},{"location":"cicd/#aws-authentication","title":"AWS Authentication","text":"<p>The workflow uses OIDC (OpenID Connect) for AWS authentication instead of long-lived credentials:</p> <pre><code>- name: Configure AWS Credentials\n  uses: aws-actions/configure-aws-credentials@v4\n  with:\n    role-to-assume: arn:aws:iam::${{ vars.AWS_ACCOUNT_ID }}:role/github-actions-${{ github.event.repository.name }}-role\n    aws-region: ${{ vars.AWS_REGION }}\n</code></pre> <p>This provides temporary credentials that expire after the workflow completes.</p>"},{"location":"cicd/#deploy-mkdocs-workflow","title":"Deploy MkDocs Workflow","text":"<p>Builds and deploys documentation to GitHub Pages.</p>"},{"location":"cicd/#trigger-conditions","title":"Trigger Conditions","text":"<pre><code>on:\n  push:\n    branches: [main]\n    paths:\n      - 'docs/**'\n      - 'mkdocs.yml'\n      - 'tools/docs/**'\n</code></pre>"},{"location":"cicd/#custom-mkdocs-plugins","title":"Custom MkDocs Plugins","text":"<p>The project uses custom MkDocs plugins that must be installed:</p> <ul> <li><code>code_sync</code> - Synchronizes code snippets in documentation</li> <li><code>diagrams</code> - Generates architecture diagrams</li> </ul> <p>These are registered as entry points in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"mkdocs.plugins\"]\ncode_sync = \"tools.docs.mkdocs_code_sync_plugin:CodeSyncPlugin\"\ndiagrams = \"tools.docs.mkdocs_diagrams_plugin:DiagramGeneratorPlugin\"\n</code></pre> <p>The workflow uses <code>uv sync</code> to install all dependencies including these plugins.</p>"},{"location":"cicd/#troubleshooting-ci-failures","title":"Troubleshooting CI Failures","text":""},{"location":"cicd/#common-issues","title":"Common Issues","text":""},{"location":"cicd/#1-docker-compose-containers-not-starting","title":"1. Docker Compose containers not starting","text":"<pre><code>Error: Failed to start Docker Compose test environment\n</code></pre> <p>Check:</p> <ul> <li>Docker Compose CI override file exists</li> <li>Proxy network creation step succeeded</li> <li>Container health checks are passing</li> </ul>"},{"location":"cicd/#2-missing-database-table","title":"2. Missing database table","text":"<pre><code>Error: relation \"apscheduler_jobs\" does not exist\n</code></pre> <p>Ensure scheduler container is started:</p> <pre><code>docker compose ... up -d --build api app chat scheduler\n</code></pre>"},{"location":"cicd/#3-mkdocs-plugin-not-found","title":"3. MkDocs plugin not found","text":"<pre><code>Error: The \"code_sync\" plugin is not installed\n</code></pre> <p>Ensure workflow uses <code>uv sync</code> instead of just <code>pip install mkdocs-material</code>.</p>"},{"location":"cicd/#4-aws-credential-issues","title":"4. AWS credential issues","text":"<pre><code>Error: Unable to locate credentials\n</code></pre> <p>Check:</p> <ul> <li>OIDC role is configured correctly in AWS</li> <li>Repository variables <code>AWS_ACCOUNT_ID</code> and <code>AWS_REGION</code> are set</li> <li>IAM role trust policy allows the repository</li> </ul>"},{"location":"cicd/#viewing-ci-logs","title":"Viewing CI Logs","text":"<p>Use the tracking script to view workflow logs:</p> <pre><code># List recent runs\n./scripts/track-gh-actions-workflow.sh list\n\n# Watch a running workflow\n./scripts/track-gh-actions-workflow.sh watch\n\n# Get failure logs\n./scripts/track-gh-actions-workflow.sh logs\n</code></pre> <p>Or use the GitHub CLI directly:</p> <pre><code># List runs\ngh run list\n\n# View specific run\ngh run view &lt;run-id&gt;\n\n# Get failed job logs\ngh run view &lt;run-id&gt; --log-failed\n</code></pre>"},{"location":"cicd/#local-testing-before-push","title":"Local Testing Before Push","text":"<p>To catch CI issues before pushing:</p> <pre><code># Run linting\nuv run ruff check .\nuv run ruff format --check .\n\n# Run type checking\nuv run mypy ichrisbirch/\n\n# Run tests with the same configuration as CI\n./cli/ichrisbirch testing start\nuv run pytest --cov=ichrisbirch\n./cli/ichrisbirch testing stop\n</code></pre>"},{"location":"cicd/#workflow-files-reference","title":"Workflow Files Reference","text":"File Purpose <code>.github/workflows/validate-project.yml</code> Main CI workflow <code>.github/workflows/deploy-docs.yml</code> Documentation deployment <code>.github/workflows/sync-docs.yml</code> Code sync in docs <code>docker-compose.ci.yml</code> CI-specific Docker Compose overrides <code>tests/environment.py</code> Test environment with CI detection"},{"location":"cli-traefik-usage/","title":"CLI Management Guide","text":"<p>This guide covers the comprehensive CLI interface for managing the iChrisBirch application using modern Traefik deployment.</p>"},{"location":"cli-traefik-usage/#quick-reference","title":"\ud83d\ude80 Quick Reference","text":"<pre><code># Start any environment (uses Traefik + HTTPS by default)\nichrisbirch dev start               # Development environment\nichrisbirch testing start          # Testing environment  \nichrisbirch prod start             # Production environment\n\n# Check status with URLs\nichrisbirch dev status\n\n# Run health checks\nichrisbirch dev health\n\n# SSL certificate management\nichrisbirch ssl-manager &lt;command&gt; &lt;env&gt;\n</code></pre>"},{"location":"cli-traefik-usage/#modern-cli-architecture","title":"\ud83d\udccb Modern CLI Architecture","text":"<p>The CLI has been completely refactored to eliminate confusing command duplication. The following changes have been made:</p>"},{"location":"cli-traefik-usage/#what-changed","title":"\u2705 What Changed","text":"<ul> <li>Removed all <code>traefik-*</code> commands that duplicated regular environment commands</li> <li>Updated all environment commands (<code>dev</code>, <code>testing</code>, <code>prod</code>) to use Traefik + HTTPS by default</li> <li>Made <code>ssl-manager</code> a top-level command for certificate management</li> <li>Simplified the interface to hide implementation details from users</li> </ul>"},{"location":"cli-traefik-usage/#removed-commands-no-longer-needed","title":"\u274c Removed Commands (No Longer Needed)","text":"<ul> <li><code>ichrisbirch traefik start &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; start</code></li> <li><code>ichrisbirch traefik stop &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; stop</code> </li> <li><code>ichrisbirch traefik status &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; status</code></li> <li><code>ichrisbirch traefik health &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; health</code></li> <li><code>ichrisbirch traefik logs &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; logs</code></li> </ul>"},{"location":"cli-traefik-usage/#environment-management-commands","title":"\ud83d\udd27 Environment Management Commands","text":""},{"location":"cli-traefik-usage/#development-environment","title":"Development Environment","text":"Command Description Example <code>dev start</code> Start development with HTTPS <code>ichrisbirch dev start</code> <code>dev stop</code> Stop development environment <code>ichrisbirch dev stop</code> <code>dev restart</code> Restart development environment <code>ichrisbirch dev restart</code> <code>dev rebuild</code> Rebuild images, restart, and initialize database <code>ichrisbirch dev rebuild</code> <code>dev status</code> Show service status, URLs, and credentials <code>ichrisbirch dev status</code> <code>dev logs</code> View service logs <code>ichrisbirch dev logs [service]</code> <code>dev health</code> Run comprehensive health checks <code>ichrisbirch dev health</code> <p>Dev Credentials Display:</p> <p>The <code>dev start</code>, <code>dev status</code>, and <code>dev rebuild</code> commands now display development credentials from the <code>.env</code> file:</p> <pre><code>Dev Credentials:\n  Regular: user@example.com / password123\n  Admin:   admin@example.com / adminpass123\n</code></pre>"},{"location":"cli-traefik-usage/#testing-environment","title":"Testing Environment","text":"Command Description Example <code>test run</code> Run tests (reuses containers) <code>ichrisbirch test run [path] [args]</code> <code>test cov</code> Run tests with coverage <code>ichrisbirch test cov</code> <code>testing start</code> Start testing environment <code>ichrisbirch testing start</code> <code>testing stop</code> Stop testing environment <code>ichrisbirch testing stop</code> <code>testing restart</code> Restart testing environment <code>ichrisbirch testing restart</code> <code>testing status</code> Show service status and HTTPS URLs <code>ichrisbirch testing status</code> <code>testing logs</code> View service logs <code>ichrisbirch testing logs [service]</code> <code>testing health</code> Run comprehensive health checks <code>ichrisbirch testing health</code> <p>Test Run Behavior:</p> <p>The <code>test run</code> command uses a clean start strategy for reliability:</p> <ol> <li>Full cleanup - Stops any existing containers before starting</li> <li>Fresh start - New containers with clean database each run</li> <li>No race conditions - Handles back-to-back runs reliably (e.g., pre-commit hooks)</li> <li>Time trade-off - ~50s startup time accepted for reliability</li> </ol> <p>Note: Test and dev environments use separate proxy networks (<code>proxy-test</code> and <code>proxy-dev</code>) to avoid Traefik routing conflicts when running simultaneously.</p>"},{"location":"cli-traefik-usage/#production-environment","title":"Production Environment","text":"Command Description Example <code>prod start</code> Start production with HTTPS <code>ichrisbirch prod start</code> <code>prod stop</code> Stop production environment <code>ichrisbirch prod stop</code> <code>prod restart</code> Restart production environment <code>ichrisbirch prod restart</code> <code>prod status</code> Show service status and HTTPS URLs <code>ichrisbirch prod status</code> <code>prod logs</code> View service logs <code>ichrisbirch prod logs [service]</code> <code>prod health</code> Run comprehensive health checks <code>ichrisbirch prod health</code>"},{"location":"cli-traefik-usage/#ssl-certificate-management","title":"SSL Certificate Management","text":"Command Description Example <code>ssl-manager generate</code> Generate SSL certificates with mkcert <code>ichrisbirch ssl-manager generate dev</code> <code>ssl-manager validate</code> Validate existing certificates <code>ichrisbirch ssl-manager validate dev</code> <code>ssl-manager info</code> Show certificate information <code>ichrisbirch ssl-manager info dev</code> <code>ssl-manager help</code> Show SSL manager help <code>ichrisbirch ssl-manager help</code>"},{"location":"cli-traefik-usage/#environment-details","title":"\ud83c\udf10 Environment Details","text":""},{"location":"cli-traefik-usage/#development-environment-dev","title":"Development Environment (<code>dev</code>)","text":"<p>Characteristics:</p> <ul> <li>Domain: <code>*.docker.localhost</code> (browser-trusted with mkcert)</li> <li>Port: 443 (standard HTTPS)</li> <li>Dashboard: <code>https://dashboard.docker.localhost/</code> (dev/devpass)</li> </ul> <p>Services:</p> <ul> <li>API: <code>https://api.docker.localhost/</code></li> <li>App: <code>https://app.docker.localhost/</code></li> <li>Chat: <code>https://chat.docker.localhost/</code></li> </ul> <p>Example Usage:</p> <pre><code># Start development environment\nichrisbirch dev start\n\n# Check what's running\nichrisbirch dev status\n\n# Verify everything is healthy\nichrisbirch dev health\n\n# View API logs\nichrisbirch dev logs api\n\n# Stop when done\nichrisbirch dev stop\n</code></pre>"},{"location":"cli-traefik-usage/#testing-environment-testing","title":"Testing Environment (<code>testing</code>)","text":"<p>Characteristics:</p> <ul> <li>Domain: <code>*.test.localhost</code> (browser-trusted with mkcert)</li> <li>Port: 8443 (custom HTTPS port)</li> <li>Dashboard: <code>https://dashboard.test.localhost:8443/</code> (test/testpass)</li> </ul> <p>Services:</p> <ul> <li>API: <code>https://api.test.localhost:8443/</code></li> <li>App: <code>https://app.test.localhost:8443/</code></li> <li>Chat: <code>https://chat.test.localhost:8443/</code></li> </ul>"},{"location":"cli-traefik-usage/#production-environment-prod","title":"Production Environment (<code>prod</code>)","text":"<p>Characteristics:</p> <ul> <li>Domain: <code>*.ichrisbirch.com</code> via Cloudflare Tunnel</li> <li>TLS: Handled by Cloudflare (Traefik receives HTTP internally)</li> <li>Secrets: Fetched from AWS SSM Parameter Store</li> </ul> <p>Services:</p> <ul> <li>API: <code>https://api.ichrisbirch.com/</code></li> <li>App: <code>https://app.ichrisbirch.com/</code></li> <li>Chat: <code>https://chat.ichrisbirch.com/</code></li> </ul> <p>Production Start:</p> <pre><code>ichrisbirch prod start   # Fetches SSM secrets, then starts services\n</code></pre> <p>Note: See Homelab Deployment Guide for complete production setup including Cloudflare Tunnel configuration</p>"},{"location":"cli-traefik-usage/#ssl-certificate-management-with-mkcert","title":"\ud83d\udd12 SSL Certificate Management with mkcert","text":""},{"location":"cli-traefik-usage/#modern-browser-trusted-certificates","title":"Modern Browser-Trusted Certificates","text":"<p>The SSL manager now uses mkcert when available to generate browser-trusted certificates that work without security warnings.</p>"},{"location":"cli-traefik-usage/#prerequisites","title":"Prerequisites","text":"<pre><code># Install mkcert (macOS)\nbrew install mkcert\n\n# Install mkcert (Linux)\ncurl -JLO \"https://dl.filippo.io/mkcert/latest?for=linux/amd64\"\nchmod +x mkcert-v*-linux-amd64\nsudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\n# Install the local CA\nmkcert -install\n</code></pre>"},{"location":"cli-traefik-usage/#generate-certificates","title":"Generate Certificates","text":"<pre><code># Generate certificates for development\nichrisbirch ssl-manager generate dev\n\n# Generate certificates for all environments\nichrisbirch ssl-manager generate all\n\n# View certificate information\nichrisbirch ssl-manager info dev\n</code></pre>"},{"location":"cli-traefik-usage/#what-mkcert-provides","title":"What mkcert Provides","text":"<ul> <li>Browser-trusted certificates: No security warnings in Chrome, Safari, Firefox</li> <li>Proper Subject Alternative Names: Wildcard + specific subdomains</li> <li>Local Certificate Authority: Installed in system trust stores</li> <li>Long validity: Certificates valid for 2+ years</li> </ul>"},{"location":"cli-traefik-usage/#certificate-domains","title":"Certificate Domains","text":"<p>Development certificates include:</p> <ul> <li><code>docker.localhost</code></li> <li><code>*.docker.localhost</code></li> <li><code>api.docker.localhost</code></li> <li><code>app.docker.localhost</code></li> <li><code>chat.docker.localhost</code></li> <li><code>dashboard.docker.localhost</code></li> </ul>"},{"location":"cli-traefik-usage/#detailed-command-reference","title":"\ud83d\ude80 Detailed Command Reference","text":""},{"location":"cli-traefik-usage/#ichrisbirch-dev-start","title":"<code>ichrisbirch dev start</code>","text":"<p>Starts the development environment with full Traefik + HTTPS setup.</p> <p>What it does:</p> <ul> <li>Generates SSL certificates if missing (using mkcert when available)</li> <li>Creates Docker networks (<code>proxy-dev</code> for development, separate from <code>proxy-test</code>)</li> <li>Starts all services with Docker Compose</li> <li>Waits for services to become healthy</li> <li>Displays HTTPS access URLs and dev credentials</li> </ul> <p>Example Output:</p> <pre><code>Starting DEV environment with Docker Compose + Traefik (HTTPS)\n[+] Running 8/8\n \u2714 Network icb-dev-proxy                Created\n \u2714 Container icb-dev-postgres   Healthy\n \u2714 Container icb-dev-redis      Started\n \u2714 Container icb-dev-api        Started\n \u2714 Container icb-dev-app        Started\n \u2714 Container icb-dev-chat       Started\n \u2714 Container icb-dev-traefik    Started\n\nDevelopment environment started with HTTPS:\n  API:       https://api.docker.localhost/\n  APP:       https://app.docker.localhost/\n  CHAT:      https://chat.docker.localhost/\n  DASHBOARD: https://dashboard.docker.localhost/ (user: dev, pass: devpass)\n\nDev Credentials:\n  Regular: user@example.com / password123\n  Admin:   admin@example.com / adminpass123\n\nUse ichrisbirch dev logs to view live container logs\nUse ichrisbirch dev status to check service status\nUse ichrisbirch dev health to run health checks\n</code></pre>"},{"location":"cli-traefik-usage/#ichrisbirch-dev-status","title":"<code>ichrisbirch dev status</code>","text":"<p>Shows detailed status of all services with HTTPS URLs and health information.</p> <p>Example Output:</p> <pre><code>Checking DEV environment status...\n\nContainer Status:\n[\u2713] icb-dev-traefik    (Up 2 minutes)\n[\u2713] icb-dev-postgres   (Up 2 minutes (healthy))\n[\u2713] icb-dev-redis      (Up 2 minutes (healthy))\n[\u2713] icb-dev-api        (Up 2 minutes (healthy))\n[\u2713] icb-dev-app        (Up 2 minutes (healthy))\n[\u2713] icb-dev-chat       (Up 2 minutes)\n[\u2713] icb-dev-scheduler  (Up 2 minutes)\n\nDevelopment environment URLs:\n  API:       https://api.docker.localhost/\n  APP:       https://app.docker.localhost/\n  CHAT:      https://chat.docker.localhost/\n  DASHBOARD: https://dashboard.docker.localhost/\n\nDatabase Info:\n  PostgreSQL: localhost:5434 (external access)\n  Redis:      localhost:6379 (external access)\n</code></pre>"},{"location":"cli-traefik-usage/#ichrisbirch-dev-health","title":"<code>ichrisbirch dev health</code>","text":"<p>Runs comprehensive health checks for the development environment.</p> <p>What it checks:</p> <ul> <li>Docker container status and health</li> <li>DNS resolution for domains</li> <li>HTTPS endpoint accessibility (with proper SSL validation)</li> <li>WebSocket support (for chat service)</li> <li>Database connectivity</li> <li>API health endpoints</li> </ul> <p>Example Output:</p> <pre><code>\ud83c\udfe5 Running health check for DEV environment\nHealth Check for dev Environment\n========================================\n\n[INFO] Checking Docker containers for dev environment\n[\u2713] Container: icb-dev-traefik (Up 2 minutes)\n[\u2713] Container: icb-dev-api (Up 2 minutes (healthy))\n[\u2713] Container: icb-dev-app (Up 2 minutes (healthy))\n[\u2713] Container: icb-dev-chat (Up 2 minutes)\n\n[INFO] Checking DNS resolution for api.docker.localhost\n[\u2713] DNS: api.docker.localhost found in /etc/hosts (127.0.0.1)\n\n[INFO] Checking API Health at https://api.docker.localhost/health\n[\u2713] API Health: HTTP 200 (OK)\n[INFO] Checking App Frontend at https://app.docker.localhost/\n[\u2713] App Frontend: HTTP 200 (OK)\n[INFO] Checking Chat Service at https://chat.docker.localhost/\n[\u2713] Chat Service: HTTP 200 (OK)\n\n[INFO] Checking WebSocket support for Chat Service\n[!] Chat Service WebSocket: HTTP 400 (May not support WebSocket)\n</code></pre>"},{"location":"cli-traefik-usage/#ichrisbirch-dev-logs-service","title":"<code>ichrisbirch dev logs [service]</code>","text":"<p>View logs for all services or a specific service.</p> <p>Usage:</p> <pre><code>ichrisbirch dev logs          # All services\nichrisbirch dev logs api      # API service only\nichrisbirch dev logs traefik  # Traefik logs\n</code></pre> <p>Features:</p> <ul> <li>Persistent viewing: Watch loop automatically reconnects when containers restart</li> <li>Real-time log following (Ctrl+C to exit)</li> <li>Service-specific filtering</li> <li>Colored output for better readability (service names colorized)</li> <li>Uses structlog format (timestamp, level, event, context)</li> </ul>"},{"location":"cli-traefik-usage/#ichrisbirch-ssl-manager-generate-envall","title":"<code>ichrisbirch ssl-manager generate &lt;env|all&gt;</code>","text":"<p>Generates SSL certificates for the specified environment using mkcert when available.</p> <p>Usage:</p> <pre><code>ichrisbirch ssl-manager generate dev    # Development certificates\nichrisbirch ssl-manager generate all    # All environments\n</code></pre> <p>With mkcert (Recommended):</p> <ul> <li>Generates browser-trusted certificates</li> <li>Includes proper Subject Alternative Names</li> <li>Valid for 2+ years</li> <li>No security warnings in browsers</li> </ul> <p>Fallback (OpenSSL):</p> <ul> <li>Self-signed certificates</li> <li>May show browser warnings</li> <li>Requires manual trust configuration</li> </ul> <p>Example Output:</p> <pre><code>[INFO] Generating SSL certificate for dev environment\n[INFO] Using mkcert for trusted local development certificates\n\nCreated a new certificate valid for the following names \ud83d\udcdc\n - \"docker.localhost\"\n - \"*.docker.localhost\"\n - \"api.docker.localhost\"\n - \"app.docker.localhost\"\n - \"chat.docker.localhost\"\n - \"dashboard.docker.localhost\"\n\nThe certificate is at \"dev.crt\" and the key at \"dev.key\" \u2705\nIt will expire on 21 January 2028 \ud83d\uddd3\n\n[SUCCESS] mkcert certificate generated for dev environment\n[INFO] This certificate will be trusted by browsers without warnings\n</code></pre>"},{"location":"cli-traefik-usage/#ichrisbirch-ssl-manager-info-envall","title":"<code>ichrisbirch ssl-manager info &lt;env|all&gt;</code>","text":"<p>Displays detailed information about SSL certificates.</p> <p>Example Output:</p> <pre><code>[INFO] Certificate information for dev environment\n\nCertificate file: /path/to/deploy-containers/traefik/certs/dev.crt\nPrivate key file: /path/to/deploy-containers/traefik/certs/dev.key\n\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number: 4e:1c:4c:e9:91:ae:d6:da:1f:91:b1:fc:07:13:9d:f7\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: O=mkcert development CA, CN=mkcert user@hostname\n        Validity\n            Not Before: Oct 21 23:45:19 2025 GMT\n            Not After : Jan 22 00:45:19 2028 GMT\n        Subject: O=mkcert development certificate\n\nValid from: Oct 21 23:45:19 2025 GMT\nValid until: Jan 22 00:45:19 2028 GMT\n\n[SUCCESS] Certificate is valid and not expiring soon\n</code></pre>"},{"location":"cli-traefik-usage/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"cli-traefik-usage/#combining-commands","title":"Combining Commands","text":"<pre><code># Complete environment restart with health check\nichrisbirch dev restart &amp;&amp; ichrisbirch dev health\n\n# Generate certificates and start environment\nichrisbirch ssl-manager generate dev &amp;&amp; ichrisbirch dev start\n\n# Check status across all environments\nfor env in dev testing prod; do\n  echo \"=== $env ===\"\n  ichrisbirch $env status\ndone\n</code></pre>"},{"location":"cli-traefik-usage/#development-workflows","title":"Development Workflows","text":"<pre><code># Daily development startup\nichrisbirch dev start\n\n# Quick status check\nichrisbirch dev status\n\n# View API logs while developing\nichrisbirch dev logs api\n\n# Clean shutdown at end of day\nichrisbirch dev stop\n</code></pre>"},{"location":"cli-traefik-usage/#debugging-workflows","title":"Debugging Workflows","text":"<pre><code># Troubleshoot service issues\nichrisbirch dev status          # Check overall status\nichrisbirch dev logs api        # Check specific service logs\nichrisbirch dev health          # Run comprehensive health check\n\n# SSL certificate issues\nichrisbirch ssl-manager validate dev    # Check certificate validity\nichrisbirch ssl-manager info dev        # Show certificate details\nichrisbirch ssl-manager generate dev    # Regenerate if needed\n</code></pre>"},{"location":"cli-traefik-usage/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"cli-traefik-usage/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>\"Command not found\" error</li> </ol> <pre><code># Make CLI executable\nchmod +x ./cli/ichrisbirch\n\n# Use absolute path\n./cli/ichrisbirch dev start\n</code></pre> <ol> <li>Port conflicts</li> </ol> <pre><code># Stop conflicting services\nichrisbirch dev stop\ndocker stop $(docker ps -q --filter \"name=ichrisbirch\")\n\n# Restart\nichrisbirch dev start\n</code></pre> <ol> <li>DNS resolution issues</li> </ol> <pre><code># Check /etc/hosts\ngrep docker.localhost /etc/hosts\n\n# Add missing entries (for development)\necho \"127.0.0.1 api.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 app.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 chat.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 dashboard.docker.localhost\" | sudo tee -a /etc/hosts\n</code></pre> <ol> <li>Certificate issues in browsers</li> </ol> <pre><code># Install mkcert if not already installed\nbrew install mkcert\nmkcert -install\n\n# Regenerate certificates with mkcert\nichrisbirch ssl-manager generate dev\n\n# Restart to pick up new certificates\nichrisbirch dev restart\n</code></pre> <ol> <li>Browser still shows certificate warnings</li> </ol> <pre><code># Verify mkcert is working\nmkcert -install\n\n# Check certificate details\nichrisbirch ssl-manager info dev\n\n# Clear browser cache and restart browser\n# Chrome: Settings &gt; Privacy &gt; Clear browsing data\n</code></pre>"},{"location":"cli-traefik-usage/#getting-help","title":"Getting Help","text":"<pre><code># Show general help\nichrisbirch help\n\n# Show environment-specific help\nichrisbirch dev help\n\n# Show SSL manager help\nichrisbirch ssl-manager help\n</code></pre>"},{"location":"cli-traefik-usage/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"cli-traefik-usage/#optimizing-startup-time","title":"Optimizing Startup Time","text":"<ol> <li>Keep images up to date: Regular <code>docker pull</code> for base images</li> <li>Use Docker BuildKit: Enable for faster builds</li> <li>Persist volumes: Avoid unnecessary data recreation</li> </ol>"},{"location":"cli-traefik-usage/#monitoring-performance","title":"Monitoring Performance","text":"<pre><code># Check container resource usage\ndocker stats\n\n# Monitor service response times (with trusted certificates)\ntime curl -I https://api.docker.localhost/health\n\n# View Traefik metrics\ncurl https://dashboard.docker.localhost/api/overview\n</code></pre>"},{"location":"cli-traefik-usage/#benefits-of-the-simplified-cli","title":"\ud83c\udf1f Benefits of the Simplified CLI","text":""},{"location":"cli-traefik-usage/#before-confusing-duplication","title":"Before (Confusing Duplication)","text":"<ul> <li><code>ichrisbirch traefik start dev</code> vs <code>ichrisbirch dev start</code> (both did the same thing)</li> <li>Users had to understand Traefik implementation details</li> <li>Inconsistent command patterns</li> <li>Implementation details exposed in user interface</li> </ul>"},{"location":"cli-traefik-usage/#after-clean-simple","title":"After (Clean &amp; Simple)","text":"<ul> <li>Single command per operation: <code>ichrisbirch dev start</code></li> <li>Implementation details hidden: Users don't need to know about Traefik</li> <li>Consistent patterns: All environments work the same way</li> <li>Modern HTTPS by default: No separate \"traefik\" commands needed</li> </ul>"},{"location":"cli-traefik-usage/#user-experience-improvements","title":"User Experience Improvements","text":"<ul> <li>Faster onboarding: New developers don't need to understand reverse proxy details</li> <li>Reduced cognitive load: Fewer commands to remember</li> <li>Better discoverability: <code>ichrisbirch help</code> shows all available commands clearly</li> <li>Professional CLI patterns: Follows industry-standard CLI design principles</li> </ul> <p>The simplified CLI provides a clean, professional interface that hides implementation complexity while providing powerful functionality for managing the modern Traefik-based deployment architecture.</p> <p>````text</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#ichrisbirchichrisbirchconfigpy","title":"<code>ichrisbirch/ichrisbirch/config.py</code>","text":"<p>In the Config class, we're setting the env_file based on the ENVIRONMENT variable. If ENVIRONMENT is not recognized, env_file will be None. When env_file is set, pydantic will automatically try to load the variables from the specified file.</p> <p>Also note that since pydantic automatically converts environment variables to their corresponding data types, we don't need to use Optional or Union in our field definitions anymore.</p>"},{"location":"configuration/#flake-8","title":"Flake 8","text":"<p><code>.flake8</code> cannot be loaded from <code>pyproject.toml</code></p>"},{"location":"css/","title":"CSS Notes","text":""},{"location":"css/#flex","title":"Flex","text":"<p>The display: flex and display: inline-flex properties in CSS are used to create a flex container and make its children flex items. The difference between them lies in how the flex container behaves in relation to other elements.</p> <p>display: flex: This makes the container a block-level flex container. A block-level element takes up the full width of its parent element, and it starts and ends with a new line. So, a flex container with display: flex will take up the full width of its parent and will not allow other elements to sit next to it on the same line.</p> <p>display: inline-flex: This makes the container an inline-level flex container. An inline-level element only takes up as much width as it needs, and it does not start or end with a new line. So, a flex container with display: inline-flex will only be as wide as necessary to contain its items, and it will allow other elements to sit next to it on the same line.</p>"},{"location":"css_bem/","title":"CSS BEM","text":"<p>HTML5 semantic elements help structure the content of web pages in a way that is meaningful for both browsers and developers. BEM (Block, Element, Modifier) is a methodology that aims to create reusable components and code sharing in front-end development. It stands for Block, Element, Modifier and provides a way for developers to name their CSS classes in a strict, understandable, and informative way, significantly improving code maintainability and readability.</p> <p>Here\u2019s an example of a medium complexity page using HTML5 semantic tags combined with BEM CSS naming conventions. This example includes a basic layout with a header, navigation, a main content area with an article and sidebar, and a footer.</p>"},{"location":"css_bem/#example-html-structure","title":"Example HTML Structure","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Example Page with HTML5 and BEM&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;header class=\"header\"&gt;\n    &lt;h1 class=\"header__title\"&gt;My Website&lt;/h1&gt;\n    &lt;nav class=\"nav\"&gt;\n        &lt;ul class=\"nav__list\"&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#home\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/nav&gt;\n&lt;/header&gt;\n\n&lt;main class=\"main\"&gt;\n    &lt;article class=\"article\"&gt;\n        &lt;h2 class=\"article__title\"&gt;Blog Post Title&lt;/h2&gt;\n        &lt;p class=\"article__meta\"&gt;Posted on &lt;time datetime=\"2023-04-01\"&gt;April 1, 2023&lt;/time&gt;&lt;/p&gt;\n        &lt;div class=\"article__content\"&gt;\n            &lt;p&gt;This is a blog post. It describes something interesting.&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/article&gt;\n\n    &lt;aside class=\"sidebar\"&gt;\n        &lt;div class=\"sidebar__section\"&gt;\n            &lt;h2 class=\"sidebar__title\"&gt;About Me&lt;/h2&gt;\n            &lt;p&gt;I am a web developer...&lt;/p&gt;\n        &lt;/div&gt;\n        &lt;div class=\"sidebar__section\"&gt;\n            &lt;h2 class=\"sidebar__title\"&gt;Archives&lt;/h2&gt;\n            &lt;ul class=\"sidebar__list\"&gt;\n                &lt;li class=\"sidebar__item\"&gt;March 2023&lt;/li&gt;\n                &lt;li class=\"sidebar__item\"&gt;February 2023&lt;/li&gt;\n                &lt;li class=\"sidebar__item\"&gt;January 2023&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/aside&gt;\n&lt;/main&gt;\n\n&lt;footer class=\"footer\"&gt;\n    &lt;p class=\"footer__text\"&gt;\u00a9 2023 My Website&lt;/p&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"css_bem/#example-css-using-bem","title":"Example CSS Using BEM","text":"<pre><code>.header {\n    background-color: #f0f0f0;\n    padding: 20px 0;\n}\n\n.header__title {\n    margin: 0;\n    padding: 0 20px;\n}\n\n.nav {\n    background-color: #333;\n}\n\n.nav__list {\n    list-style: none;\n    display: flex;\n    justify-content: center;\n    padding: 0;\n}\n\n.nav__item {\n    margin: 0 10px;\n}\n\n.nav__link {\n    color: white;\n    text-decoration: none;\n}\n\n.main {\n    display: flex;\n    margin: 20px;\n}\n\n.article {\n    flex: 3;\n}\n\n.article__title {\n    color: #333;\n}\n\n.article__meta {\n    font-style: italic;\n}\n\n.sidebar {\n    flex: 1;\n    padding-left: 20px;\n}\n\n.sidebar__title {\n    font-size: 20px;\n}\n\n.footer {\n    background-color: #333;\n    color: white;\n    text-align: center;\n    padding: 10px 0;\n}\n</code></pre>"},{"location":"css_bem/#bem-explanation","title":"BEM Explanation","text":"<ul> <li> <p>Block: Standalone entity that is meaningful on its own (e.g., <code>header</code>, <code>nav</code>, <code>article</code>, <code>sidebar</code>, <code>footer</code>). Blocks can be nested inside each other but should remain independent.</p> </li> <li> <p>Element: A part of a block that has no standalone meaning and is semantically tied to its block (e.g., <code>header__title</code>, <code>nav__link</code>, <code>article__title</code>). Elements are always part of a block, not another element.</p> </li> <li> <p>Modifier: Flags on blocks or elements used to change appearance, behavior, or state (e.g., <code>nav__link--active</code>, although not shown in the example above, would represent an active state of the navigation link).</p> </li> </ul> <p>BEM's naming convention makes the structure of HTML/CSS clear and understandable at a glance, provides a strong contract for developers on a project, and helps avoid CSS naming conflicts by using unique names based on the block-element hierarchy.</p>"},{"location":"developer_setup/","title":"Developer Setup","text":"<ul> <li>1. Programs to install</li> <li>2. Set up git-secret</li> <li>3. Setup the project</li> <li>4. Run the project</li> <li>5. Connecting to the Running Project</li> <li>5.1. App</li> <li>5.2. API</li> <li>6. Links and Notes</li> </ul>"},{"location":"developer_setup/#1-programs-to-install","title":"1. Programs to install","text":"<p>poetry git-secret Docker Desktop</p>"},{"location":"developer_setup/#2-set-up-git-secret","title":"2. Set up git-secret","text":""},{"location":"developer_setup/#3-setup-the-project","title":"3. Setup the project","text":"<pre><code>git clone https://github.com/datapointchris/ichrisbirch.git\n\ncd ichrisbirch/\n\ngit secret reveal\n\npoetry install\n\nsource .venv/bin/activate\n\nexport ENVIRONMENT=development\n\npre-commit install\n\n# Make sure Docker is running\n\npytest\n</code></pre>"},{"location":"developer_setup/#4-run-the-project","title":"4. Run the project","text":"<p>TODO</p> <p>This doesn't work!  I need to figure out another way to run it locally.   Right now it is relying on using local NGINX and Supervisor.</p> <pre><code># App and API are separate applications.\n# App is a flask app that runs the frontend\n# API is FastAPI running the API backend that the frontend connects to\n\n# Run these in separate shells for log separation\n# poetry run python ichrisbirch/runapidev.py\n# poetry run python ichrisbirch/runappdev.py\n</code></pre>"},{"location":"developer_setup/#5-connecting-to-the-running-project","title":"5. Connecting to the Running Project","text":""},{"location":"developer_setup/#51-app","title":"5.1. App","text":"<p>http://127.0.0.1:6000</p>"},{"location":"developer_setup/#52-api","title":"5.2. API","text":"<p>http://127.0.0.1:6200</p>"},{"location":"developer_setup/#6-links-and-notes","title":"6. Links and Notes","text":"<p>GitHub - github/scripts-to-rule-them-all: Set of boilerplate scripts describing the normalized script pattern that GitHub uses in its projects.</p>"},{"location":"docker-development/","title":"Docker Development Environment","text":""},{"location":"docker-development/#overview","title":"Overview","text":"<p>The iChrisBirch project uses Docker Compose for containerized development with separate configurations for development, testing, and production environments.</p>"},{"location":"docker-development/#architecture","title":"Architecture","text":""},{"location":"docker-development/#service-configuration","title":"Service Configuration","text":"<p>Development Environment (<code>docker-compose.dev.yml</code>):</p> <ul> <li>Nginx: Port 80/443 (reverse proxy)</li> <li>Flask App: Port 8000 (internal), auto-reload enabled</li> <li>FastAPI Backend: Port 8000 (internal), auto-reload enabled  </li> <li>PostgreSQL: Port 5432 (internal/external), persistent volumes</li> <li>Redis: Port 6379 (internal/external), persistent volumes</li> <li>Chat Service: Port 8505 (internal), auto-reload enabled</li> </ul> <p>Test Environment (<code>docker-compose.test.yml</code>):</p> <ul> <li>PostgreSQL: Port 5434 (external) \u2192 5432 (internal), tmpfs for speed</li> <li>Redis: Port 6380 (external) \u2192 6379 (internal), tmpfs for speed  </li> <li>FastAPI Backend: Port 8001 (external) \u2192 8000 (internal), isolated test database</li> <li>Nginx: Disabled for testing</li> </ul> <p>Production Environment (<code>docker-compose.yml</code>):</p> <ul> <li>All services: Standard internal ports with production optimizations</li> <li>SSL/TLS: Ready for certificate mounting</li> <li>Health checks: Comprehensive monitoring</li> <li>Resource limits: Production-ready constraints</li> </ul>"},{"location":"docker-development/#cli-command-reference","title":"CLI Command Reference","text":""},{"location":"docker-development/#development-commands","title":"Development Commands","text":"<pre><code>ichrisbirch dev start     # Start all development services\nichrisbirch dev stop      # Stop and remove containers  \nichrisbirch dev restart   # Restart existing containers\nichrisbirch dev rebuild   # Rebuild images and restart\nichrisbirch dev logs      # View live container logs\nichrisbirch dev status    # Show container status\n</code></pre> <p>Technical Details:</p> <ul> <li><code>restart</code>: Fast recovery from service crashes (no image rebuild)</li> <li><code>rebuild</code>: Full image rebuild after code/dependency changes</li> <li><code>logs</code>: Shows Docker infrastructure logs, not application logs</li> </ul>"},{"location":"docker-development/#test-commands","title":"Test Commands","text":"<pre><code>ichrisbirch test          # Run pytest in containerized environment\nichrisbirch test logs     # Run tests with timestamped log output\n</code></pre> <p>Test Infrastructure:</p> <ul> <li>Isolated database on port 5434 with tmpfs for performance</li> <li>Redis on port 6380 with tmpfs storage</li> <li>Services terminate automatically when tests complete</li> <li>Cleanup via <code>docker-compose down -v</code> removes test containers/volumes</li> </ul>"},{"location":"docker-development/#production-commands","title":"Production Commands","text":"<pre><code>ichrisbirch prod status    # Check production service status\nichrisbirch prod apihealth # HTTP health check for API service\nichrisbirch prod logs      # View production application logs\n</code></pre>"},{"location":"docker-development/#docker-compose-configuration","title":"Docker Compose Configuration","text":""},{"location":"docker-development/#file-structure","title":"File Structure","text":"<ul> <li><code>docker-compose.yml</code>: Base production configuration</li> <li><code>docker-compose.dev.yml</code>: Development overrides with debugging</li> <li><code>docker-compose.test.yml</code>: Test environment with performance optimizations</li> </ul>"},{"location":"docker-development/#environment-variables","title":"Environment Variables","text":"<p>Each environment uses corresponding environment files:</p> <ul> <li>Development: <code>.dev.env.secret</code> (Git Secret encrypted)</li> <li>Testing: <code>.test.env.secret</code> (Git Secret encrypted)  </li> <li>Production: <code>.prod.env.secret</code> (Git Secret encrypted)</li> </ul>"},{"location":"docker-development/#volume-mounts","title":"Volume Mounts","text":"<p>Development:</p> <ul> <li>Source code mounted for live editing</li> <li>Persistent database and Redis volumes</li> <li>Nginx configuration from <code>deploy/dev/nginx/</code></li> </ul> <p>Testing:</p> <ul> <li>tmpfs mounts for database and Redis (maximum speed)</li> <li>Isolated test data, discarded after tests</li> <li>No source code mounts (clean container environment)</li> </ul> <p>Production:</p> <ul> <li>Named volumes for data persistence</li> <li>SSL certificate mounting ready</li> <li>Optimized configurations from <code>deploy/prod/</code></li> </ul>"},{"location":"docker-development/#logging-architecture","title":"Logging Architecture","text":""},{"location":"docker-development/#application-logging","title":"Application Logging","text":"<ul> <li>Python loggers: Write directly to application log files</li> <li>Log locations: <code>$LOG_DIR</code> environment variable (defaults to <code>./logs</code>)</li> <li>Log files: <code>ichrisbirch.log</code>, <code>app.log</code>, <code>api.log</code>, <code>scheduler.log</code></li> <li>Colored output: CLI provides colored log viewing with <code>ichrisbirch logs</code></li> </ul>"},{"location":"docker-development/#container-logging","title":"Container Logging","text":"<ul> <li>Docker logs: Infrastructure and startup information only</li> <li>JSON file driver: Rotation and size limits configured</li> <li>Service tags: Each service tagged for log identification</li> <li>Access via: <code>docker-compose logs</code> for container-level debugging</li> </ul>"},{"location":"docker-development/#port-configuration","title":"Port Configuration","text":""},{"location":"docker-development/#external-port-mapping","title":"External Port Mapping","text":"<p>Development (accessible from host):</p> <ul> <li>Nginx: localhost:80, localhost:443</li> <li>PostgreSQL: localhost:5432</li> <li>Redis: localhost:6379</li> <li>API/App: Through Nginx reverse proxy</li> </ul> <p>Testing (isolated ports):</p> <ul> <li>PostgreSQL: localhost:5434</li> <li>Redis: localhost:6380</li> <li>FastAPI: localhost:8001</li> </ul> <p>Production:</p> <ul> <li>Nginx: Port 80/443 only (reverse proxy handles internal routing)</li> <li>All other services: Internal container network only</li> </ul>"},{"location":"docker-development/#service-communication","title":"Service Communication","text":"<ul> <li>Internal network: All services communicate via container names</li> <li>Health checks: Services wait for dependencies before starting</li> <li>Service discovery: Automatic via Docker Compose networking</li> </ul>"},{"location":"docker-development/#development-workflow","title":"Development Workflow","text":""},{"location":"docker-development/#starting-development","title":"Starting Development","text":"<pre><code># Clone and setup\ngit clone &lt;repository&gt;\ncd ichrisbirch\ngit secret reveal  # Decrypt environment files\n\n# Start development environment  \nichrisbirch dev start\n\n# View logs\nichrisbirch dev logs\n</code></pre>"},{"location":"docker-development/#running-tests","title":"Running Tests","text":"<pre><code># Run full test suite\nichrisbirch test\n\n# Run with log output\nichrisbirch test logs\n\n# Check test infrastructure\nichrisbirch dev status\n</code></pre>"},{"location":"docker-development/#debugging-services","title":"Debugging Services","text":"<pre><code># Check service status\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml ps\n\n# View specific service logs  \ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml logs api\n\n# Execute commands in running containers\ndocker exec -it icb-dev-api /bin/bash\n</code></pre>"},{"location":"docker-development/#production-deployment","title":"Production Deployment","text":""},{"location":"docker-development/#container-optimization","title":"Container Optimization","text":"<ul> <li>Multi-stage builds: Development vs production images</li> <li>Security: Non-root user (<code>appuser</code>) for all services</li> <li>Resource limits: Memory and CPU constraints configured</li> <li>Health monitoring: Comprehensive health check endpoints</li> </ul>"},{"location":"docker-development/#ssltls-configuration","title":"SSL/TLS Configuration","text":"<p>Ready for certificate mounting:</p> <pre><code>volumes:\n  - ./nginx/ssl:/etc/nginx/ssl:ro  # Uncomment for SSL certificates\n</code></pre>"},{"location":"docker-development/#scaling-considerations","title":"Scaling Considerations","text":"<ul> <li>Database: PostgreSQL optimized for production workloads</li> <li>Redis: Configured with appropriate memory limits and eviction policies</li> <li>Application services: Ready for horizontal scaling with load balancer</li> <li>Static files: Nginx optimized for efficient static file serving</li> </ul>"},{"location":"docker-development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker-development/#common-issues","title":"Common Issues","text":"<p>Port conflicts: Development uses standard ports; test uses offset ports to avoid conflicts</p> <p>Container permissions: Services run as <code>appuser</code> (UID 1000) for security</p> <p>Database connections: Services include health checks and retry logic for reliable startup</p> <p>Volume permissions: Ensure <code>$LOG_DIR</code> is writable by user running Docker Compose</p>"},{"location":"docker-development/#debug-commands","title":"Debug Commands","text":"<pre><code># Check all container status\nichrisbirch dev status\n\n# View recent infrastructure logs\nichrisbirch dev logs\n\n# Clean up unused containers/images  \ndocker system prune -f\n\n# Reset development environment\nichrisbirch dev stop &amp;&amp; ichrisbirch dev rebuild\n</code></pre>"},{"location":"docker-development/#performance-optimization","title":"Performance Optimization","text":"<p>Development: Persistent volumes with live code reloading</p> <p>Testing: tmpfs mounts eliminate I/O bottlenecks for test database/Redis</p> <p>Production: Optimized PostgreSQL and Redis configurations for production workloads</p>"},{"location":"docker-development/#migration-from-legacy-setup","title":"Migration from Legacy Setup","text":""},{"location":"docker-development/#key-changes","title":"Key Changes","text":"<ul> <li>Containerized services: All services now run in Docker containers</li> <li>Environment isolation: Separate configurations for dev/test/prod</li> <li>Modern authentication: API key-based internal service authentication</li> <li>Health monitoring: Comprehensive health checks and monitoring</li> <li>Simplified deployment: Single command deployment with Docker Compose</li> </ul>"},{"location":"docker-development/#compatibility","title":"Compatibility","text":"<ul> <li>Existing workflows: CLI commands maintain same interface</li> <li>Log viewing: Same colored log output with enhanced container support</li> <li>Development experience: Hot reloading and debugging capabilities preserved</li> <li>Production deployment: Enhanced reliability and monitoring capabilities</li> </ul>"},{"location":"documentation/","title":"Documentation","text":"<p>Built with Material for MkDocs</p> <p>Config file: <code>mkdocs.yml</code> Directory: <code>docs/</code> Docs build pipeline: <code>.github/workflows/deploy-docs.yml</code></p> <p>Docs are built using mkdocs automatically on push with the above pipeline, which triggers the <code>pages-build-deployment</code> Github workflow that publishes them to a <code>gh-pages</code> branch and publishes them to <code>datapointchris.github.io/ichrisbirch</code> from that branch.</p> <p>Refer to CICD for a description of the pipeline.</p>"},{"location":"documentation/#github-settings","title":"Github Settings","text":""},{"location":"documentation/#build-and-deployment","title":"Build and deployment","text":"<p>Source: Deploy from a branch</p> <p>Branch: gh-pages / (root)</p> <p>Note</p> <p>Even though this project is using Github Actions to publish the branch, the action is actually using <code>gh-deploy</code> so the source is NOT Github Actions, but rather \"Deploy from a branch\" that <code>gh-deploy</code> sets up.</p> <p>You might have to push the build once the first time to get the <code>gh-pages</code> branch to show up. This is the branch to use, not master, since part of the deploy script used <code>gh-deploy</code> which builds the <code>gh-pages</code> branch.</p> <p>In this branch, the root of the folder is the built docs, NOT /docs, because we are not building from the master branch where the docs live in /docs.</p>"},{"location":"documentation/#custom-domain","title":"Custom Domain","text":"<p>Custom domain: docs.ichrisbirch.com</p> <p>Refer to Domain Names for setting up the subdomain.</p> <p>Note</p> <p>CNAME record needs to be set up before adding the custom domain, or the lookup will fail. CNAME record goes in the <code>/docs</code> folder because that folder is built as the root on the <code>gh-pages</code> branch that is set up with that file when hosting.</p>"},{"location":"documentation/#diagrams","title":"Diagrams","text":"<p>Online FlowChart &amp; Diagrams Editor - Mermaid Live Editor</p> <p>GitHub - mingrammer/diagrams: Diagram as Code for prototyping cloud system architectures</p>"},{"location":"documentation/#todo-read-these-things","title":"TODO: Read these things","text":"<p>Vale.sh - A linter for prose</p> <p>Python's doctest: Document and Test Your Code at Once \u2013 Real Python</p> <p>Awesome documentation example for small project: Documentation \u2014 pypdf 3.5.1 documentation</p> <p>A Guide to Writing Your First Software Documentation \u2014 SitePointSitePoint</p> <p>How to Write Documentation For Your Next Software Development Project</p> <p>Software Documentation Best Practices [With Examples] helpjuice-logo-0307896d1acd18c6a7f52c4256467fb6ca1007315c373af21357496e9ceb49e2</p> <p>Software Documentation Types and Best Practices | by AltexSoft Inc | Prototypr</p> <p>Prepare the documentation for successful software project development</p> <p>How to Write Technical Documentation With Empathy | by Edward Huang | Jan, 2023 | Better Programming</p>"},{"location":"documentation_tools/","title":"Documentation Tools","text":"<p>This project includes tools for generating and maintaining documentation that stays in sync with the codebase.</p>"},{"location":"documentation_tools/#code-sync-tool","title":"Code Sync Tool","text":"<p>The code sync tool (<code>mkdocs_plugins/code_sync/</code>) automatically synchronizes code snippets in documentation with actual source code. This ensures documentation examples are always up-to-date.</p>"},{"location":"documentation_tools/#features","title":"Features","text":"<ul> <li>AST-based extraction: References Python functions, classes, and methods by name (survives refactoring)</li> <li>Line-based extraction: References any file type by line numbers (for configs, shell scripts, etc.)</li> <li>Decorator support: Automatically includes decorators when extracting functions/classes</li> <li>MkDocs integration: Plugin syncs code during <code>mkdocs serve</code> and <code>mkdocs build</code></li> <li>Pre-commit hook: Syncs code before each commit</li> <li>CI verification: GitHub Action fails if docs are out of sync</li> </ul>"},{"location":"documentation_tools/#markdown-syntax","title":"Markdown Syntax","text":""},{"location":"documentation_tools/#ast-based-references-recommended-for-python","title":"AST-Based References (Recommended for Python)","text":"<p>Reference Python functions, classes, or methods by name. Use this format in your markdown:</p> <pre><code>```python file=path/to/file.py element=function_name\n# This content is auto-replaced with the actual function code\n```\n</code></pre> <p>For example, to reference a function:</p> <pre><code>```python file=path/to/module.py element=function_name\n```\n</code></pre> <p>To reference a class method:</p> <pre><code>```python file=path/to/module.py element=ClassName.method_name\n```\n</code></pre> <p>To reference an entire class:</p> <pre><code>```python file=path/to/module.py element=ClassName\n```\n</code></pre>"},{"location":"documentation_tools/#line-based-references-for-non-python-files","title":"Line-Based References (For Non-Python Files)","text":"<p>Reference specific line ranges from any file:</p> <pre><code>```bash file=scripts/deploy.sh lines=10-25\n```\n</code></pre> <p>Examples:</p> <pre><code>```yaml file=config/settings.yml lines=1-20\n```\n</code></pre> <pre><code>```dockerfile file=Dockerfile lines=100-120\n```\n</code></pre>"},{"location":"documentation_tools/#optional-attributes","title":"Optional Attributes","text":"<p>Add a label/title to the code block:</p> <pre><code>```python file=path/to/module.py element=MyClass label=\"My Example Class\"\n```\n</code></pre>"},{"location":"documentation_tools/#cli-usage","title":"CLI Usage","text":"<p>The tool can be run directly from the command line:</p> <pre><code># Sync all docs with source code\npython -m mkdocs_plugins.code_sync docs --base-path .\n\n# Check if docs are in sync (CI mode - exits 1 if out of sync)\npython -m mkdocs_plugins.code_sync docs --base-path . --check\n\n# Verbose output\npython -m mkdocs_plugins.code_sync docs --base-path . -v\n</code></pre>"},{"location":"documentation_tools/#how-it-works","title":"How It Works","text":"<ol> <li>Scans markdown files for code blocks with <code>file=</code> and <code>element=</code> or <code>lines=</code> attributes</li> <li>For AST-based references:</li> <li>Parses the Python file using Python's <code>ast</code> module</li> <li>Finds the named element (function, class, method)</li> <li>Extracts the complete code including decorators and docstrings</li> <li>For line-based references:</li> <li>Reads the specified line range from the file</li> <li>Replaces the code block content with the extracted code</li> <li>Writes the updated markdown file</li> </ol>"},{"location":"documentation_tools/#automation","title":"Automation","text":""},{"location":"documentation_tools/#pre-commit-hook","title":"Pre-commit Hook","text":"<p>The pre-commit hook runs automatically before each commit:</p> <pre><code># In .pre-commit-config.yaml\n- id: code-sync\n  name: Code Sync\n  entry: python -m mkdocs_plugins.code_sync\n  args: [\"docs\", \"--base-path\", \".\"]\n</code></pre>"},{"location":"documentation_tools/#mkdocs-plugin","title":"MkDocs Plugin","text":"<p>The MkDocs plugin syncs code during documentation builds:</p> <pre><code># In mkdocs.yml\nplugins:\n  - code_sync:\n      enabled: true\n</code></pre>"},{"location":"documentation_tools/#github-actions","title":"GitHub Actions","text":"<p>The CI workflow verifies docs are in sync:</p> <pre><code># In .github/workflows/sync-docs-code.yml\n- name: Run Code Sync Check\n  run: python -m mkdocs_plugins.code_sync docs --base-path . --check\n</code></pre>"},{"location":"documentation_tools/#best-practices","title":"Best Practices","text":"<ol> <li>Prefer AST-based references for Python code - they survive refactoring</li> <li>Use line-based references only for non-Python files or when you need a specific section</li> <li>Keep referenced elements focused - reference specific functions rather than entire modules</li> <li>Run locally before pushing - use <code>python -m mkdocs_plugins.code_sync docs --base-path . --check</code> to verify</li> <li>Use meaningful labels - helps readers understand what the code shows</li> </ol>"},{"location":"documentation_tools/#diagram-generator","title":"Diagram Generator","text":"<p>The diagram generator (<code>mkdocs_plugins/diagrams/</code>) creates visual diagrams from code analysis.</p>"},{"location":"documentation_tools/#directory-structure","title":"Directory Structure","text":"<pre><code>mkdocs_plugins/diagrams/\n\u251c\u2500\u2500 analyzers/           # Code that analyzes project structure\n\u2502   \u2514\u2500\u2500 fixture_analyzer.py\n\u251c\u2500\u2500 renderers/           # Code that renders diagrams\n\u2502   \u251c\u2500\u2500 aws_diagram_renderer.py\n\u2502   \u251c\u2500\u2500 fixture_diagram_renderer.py\n\u2502   \u2514\u2500\u2500 testing_diagram_renderer.py\n\u2514\u2500\u2500 generate.py          # Main generation script\n</code></pre>"},{"location":"documentation_tools/#usage","title":"Usage","text":"<p>Generate all diagrams:</p> <pre><code>python -m mkdocs_plugins.diagrams\n\n# Force regeneration (ignore cache)\npython -m mkdocs_plugins.diagrams --force\n</code></pre> <p>Generated diagrams are saved to <code>docs/images/generated/</code>.</p>"},{"location":"documentation_tools/#mkdocs-integration","title":"MkDocs Integration","text":"<p>The diagrams plugin regenerates diagrams during builds:</p> <pre><code># In mkdocs.yml\nplugins:\n  - diagrams:\n      generate_diagrams: true\n</code></pre>"},{"location":"documentation_tools/#caching","title":"Caching","text":"<p>The generator caches content hashes to avoid unnecessary regeneration. The cache is stored in <code>.diagram_hash_cache.json</code> at the project root.</p>"},{"location":"documentation_tools/#adding-new-diagrams","title":"Adding New Diagrams","text":"<ol> <li>Create a new renderer in <code>mkdocs_plugins/diagrams/renderers/</code></li> <li>Add the renderer call to <code>generate.py</code></li> <li>Reference the generated diagram in your documentation using standard markdown image syntax</li> </ol>"},{"location":"domain_names/","title":"Domain Names","text":"<p>Hosted in <code>AWS Route 53</code> There are 3 hosted zones, one for the top level domain and one for each subdomain.</p>"},{"location":"domain_names/#ichrisbirchcom-hosted-zone","title":"ichrisbirch.com Hosted Zone","text":"<p>This is referred to as the <code>Apex</code> domain, or top level domain.</p> <p>There are 5 records in this hosted zone:</p> Record Name Type Description Value ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Apex Nameservers ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS api.ichrisbirch.com <code>NS</code> Nameservers from hosted zone <code>NS</code> record AWS API Nameservers docs.ichrisbirch.com <code>CNAME</code> Re-direct from Github Pages to docs subdomain datapointchris.github.io \\www.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver EC2 IP (Elastic IP)"},{"location":"domain_names/#apiichrisbirchcom-hosted-zone","title":"api.ichrisbirch.com Hosted Zone","text":"<p>There are 3 records in this hosted zone:</p> Record Name Type Description Value api.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver EC2 IP (Elastic IP) api.ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Api Nameservers api.ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS"},{"location":"domain_names/#docsichrisbirchcom-hosted-zone","title":"docs.ichrisbirch.com Hosted Zone","text":"<p>There are 3 records in this hosted zone:</p> Record Name Type Description Value docs.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver Github Servers docs.ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Docs Nameservers docs.ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS <p>Refer to the Documentation and CICD pages for setting up Github Pages with this subdomain.</p> <p>Use <code>dig {address}</code> to see if the domain looks set up correctly.</p> <p>Danger</p> <p>If the DNS seems to not be updating and the records are not working, be wary of the <code>Browser Cache</code> and history!! <code>Safari</code> kept the old <code>IP</code> in the cache until restart, the only way to see the update was to use a <code>Private Window</code> Check the cache before troubleshooting.</p>"},{"location":"domain_names/#apex-ichrisbirchcom-wwwichrisbirchcom","title":"Apex - <code>ichrisbirch.com ( www.ichrisbirch.com )</code>","text":"<p>There should be a <code>nameserver</code> record created with the <code>hosted zone</code>, and the <code>soa</code> is created automatically. The <code>A</code> record should point to the elastic IP if assigned, or public ip of the instance or load balancer and the name should have subdomain <code>www</code>.  I believe because the domain <code>ichrisbirch.com</code> is the apex, it doesn't need an <code>A</code> record.</p>"},{"location":"domain_names/#apiichrisbirchcom","title":"<code>api.ichrisbirch.com</code>","text":"<p>There should be a <code>nameserver</code> record created with the <code>hosted zone</code> and <code>soa</code>, same as the Apex. The <code>A</code> record should point to the elastic IP.</p> <p>There should be another <code>NS</code> record with the <code>API Nameservers</code> that is attached to the Apex zone. This allows the Apex to know how to discover the subdomain.</p>"},{"location":"domain_names/#docsichrisbirchcom","title":"<code>docs.ichrisbirch.com</code>","text":"<p>The docs are slightly different because they are hosted by <code>github</code>, being served with <code>mkdocs</code>, so they are not sitting on the server like the app (www) and api (api) are. There should be the similar <code>NS</code> and <code>soa</code> records created.  </p> <p>The <code>A</code> record points to the <code>github</code> (I think) hosts where the docs are hosted: <pre><code>\"185.199.108.153\",\n\"185.199.109.153\",\n\"185.199.110.153\",\n\"185.199.111.153\"\n</code></pre> The <code>CNAME</code> record lives at the <code>Apex</code> level, in place of the <code>NS</code> records like the <code>api</code> uses. Since the <code>CNAME</code> is an alias, it is used to alias <code>docs.ichrisbirch.com</code> =&gt; <code>datapointchris.github.io</code></p>"},{"location":"domain_names/#reference","title":"Reference","text":""},{"location":"domain_names/#1-ns-name-server-records","title":"1. NS (Name Server) Records","text":"<p>Purpose: NS records specify the authoritative name servers for a domain. These servers hold the DNS records for the domain. Function: Direct traffic by telling DNS resolvers which nameserver(s) to ask for the specific domain\u2019s information. Usage: Found at the domain's DNS zone and commonly points to multiple nameservers to provide redundancy. Example:</p> <pre><code>example.com. IN NS ns1.example.com.\nexample.com. IN NS ns2.example.com.\n</code></pre>"},{"location":"domain_names/#2-soa-start-of-authority-records","title":"2. SOA (Start of Authority) Records","text":"<p>Purpose: SOA records provide essential information about the DNS zone of a domain, including the primary nameserver, the admin\u2019s contact email, and timing information for zone transfers. Function: Defines the authoritative server and sets the rules for DNS caching, zone transfers, and DNS record updates. Usage: Should be the first record in a DNS zone file, as it contains critical operational data. Example:</p> <pre><code>example.com. IN SOA ns1.example.com. hostmaster.example.com. (\n              2023010101 ; Serial\n              7200       ; Refresh\n              3600       ; Retry\n              1209600    ; Expire\n              3600       ; Minimum TTL\n              )\n</code></pre>"},{"location":"domain_names/#3-a-address-records","title":"3. A (Address) Records","text":"<p>Purpose: A records map a domain or subdomain to an IPv4 address. Function: Translates the human-readable domain names to numerical IP addresses that computers use. Usage: Essential for pointing a domain or subdomain to a web server\u2019s IP address. Example:</p> <p><code>www.example.com. IN A 192.0.2.1</code></p>"},{"location":"domain_names/#4-cname-canonical-name-records","title":"4. CNAME (Canonical Name) Records","text":"<p>Purpose: CNAME records alias one domain name to another. Function: Points one domain/subdomain to another domain/subdomain, allowing management of multiple addresses by changing a single target address. Usage: Useful for pointing multiple subdomains to a single canonical name and to reduce redundancy in DNS management. Example:</p> <p><code>blog.example.com. IN CNAME www.example.com.</code></p>"},{"location":"domain_names/#how-they-relate-to-domains-and-subdomains","title":"How They Relate to Domains and Subdomains","text":"<p>NS Records: Define which servers are authoritative for the domain's DNS records. If you have subdomains, the NS records for the main domain affects them unless specifically overridden.</p> <p>SOA Records: Hold administrative information and control parameters for the DNS zone; they are vital for overall DNS zone health and updates.</p> <p>A Records: Directly tie domain names and subdomains to specific IP addresses. Different subdomains can be mapped to different IPs using A records.</p> <p>CNAME Records: Allow you to point subdomains (or even the root domain if needed) to other domain names, simplifying DNS management. For example, blog.example.com can point to <code>www.example.com</code>, which has an A record, thereby inheriting its IP indirectly.</p> <p>Example Application: For a domain example.com:</p> <ul> <li>NS Records: Point to ns1.example.com and ns2.example.com.</li> <li>SOA Record: Contains administrative details for the DNS zone, like contact info and timing settings.</li> <li>A Record: Points <code>www.example.com</code> to 192.0.2.1.</li> <li>CNAME Record: Points blog.example.com to <code>www.example.com</code>, which has the A record for the actual IP.</li> </ul>"},{"location":"domain_names/#aws-documentation","title":"AWS Documentation","text":"<p>Routing traffic for subdomains - Amazon Route 53Routing traffic for subdomains - Amazon Route 53</p>"},{"location":"git_secret/","title":"git-secret","text":"<p>Danger</p> <p>It seems that if <code>gpg</code> is updated then it causes some or all of the keys to need to be re-imported with <code>git secret</code> Also if there is a mismatch between <code>gpg</code> versions between computers or cicd and macos then it can create an issue. Overall, this is not the best way of handling secrets :sadface:</p>"},{"location":"git_secret/#making-a-secret","title":"Making a Secret","text":"<pre><code># for new repository\ngit secret init\n\n# This user has to have a public GPG key on THIS computer\ngit secret tell ichrisbirch@gmail.com\n\ngit secret add .env\ngit secret hide\n\ngit commit -am 'build: add secret .env file'\n</code></pre>"},{"location":"git_secret/#using-git-secret-with-ec2-instance","title":"Using git-secret with EC2 instance","text":""},{"location":"git_secret/#make-gpg-key-for-ec2-instance-on-local-machine","title":"Make gpg key for EC2 instance on local machine","text":"<pre><code>gpg --gen-key\n# Real name: iChrisBirch EC2\n# Email address: ec2@ichrisbirch.com\n\n# Export and upload keys to EC2 Instance\ngpg --export --armor ec2@ichrisbirch.com &gt; ec2-public.key\ngpg --export-secret-key --armor ec2@ichrisbirch.com &gt; ec2-private.key\nscp -i ~/.ssh/ichrisbirch-webserver.pem ec2-public.key ubuntu@ichrisbirch:~\nscp -i ~/.ssh/ichrisbirch-webserver.pem ec2-private.key ubuntu@ichrisbirch:~\n\n# Project Directory\ngit secret tell ec2@ichrisbirch.com\n# to re-encrypt them with the new authorized user\ngit secret reveal\ngit secret hide\ngit add .\ngit commit -m 'ops: Update secrets with new authorized user'\ngit push\n</code></pre>"},{"location":"git_secret/#import-gpg-key-on-ec2-instance","title":"Import gpg key on EC2 Instance","text":"<pre><code># Import keys\ngpg --import ec2-public.key\ngpg --import ec2-private.key\n\n# Project Directory\ngit pull\ngit secret reveal\n</code></pre>"},{"location":"git_secret/#make-a-gpg-key-for-cicd","title":"Make a gpg key for CICD","text":""},{"location":"git_secret/#make-a-new-key-locally","title":"Make a new key locally","text":"<pre><code># Generate new key, no passphrase\ngpg --gen-key\n# Export the secret key as one line, multiline not allowed\ngpg --armor --export-secret-key datapointchris@github.com | tr '\\n' ',' &gt; cicd-gpg-key.gpg\n# In the repository, make sure to add the new identity to allowed:\ngit secret tell datapointchris@github.com\ngit secret hide\n</code></pre>"},{"location":"git_secret/#add-the-key-to-the-cicd-environment-secrets","title":"Add the key to the CICD environment secrets","text":""},{"location":"git_secret/#add-run-step-to-cicd-workflow","title":"Add Run Step to CICD workflow","text":"<pre><code>- name: \"git-secret Reveal .env files\"\n  run: |\n    # Import private key and avoid the \"Inappropriate ioctl for device\" error\n    echo {% raw %}${{ secrets.CICD_GPG_KEY }}{% endraw %} | tr ',' '\\n' | gpg --batch --yes --pinentry-mode loopback --import\n    git secret reveal\n</code></pre>"},{"location":"git_secret/#expired-gpg-key","title":"Expired GPG key","text":"<p><code>git-secret: warning: at least one key for email(s) is revoked, expired, or otherwise invalid: ichrisbirch@gmail.com</code></p> <p>Expired keys need to have their expiry date extended, which requires the following steps:</p> <pre><code># List keys and subkey(s)\ngpg --list-secret-keys --verbose --with-subkey-fingerprints\n\n&gt;&gt;&gt; sec   ed25519 2022-04-19 [SC] [expired: 2024-04-18]\n&gt;&gt;&gt;       B98C7D8073BB87...\n&gt;&gt;&gt; uid           [ultimate] Chris Birch &lt;ichrisbirch@gmail.com&gt;\n&gt;&gt;&gt; ssb   cv25519 2022-04-19 [E] [expired: 2024-04-18]\n&gt;&gt;&gt;       2E418AB946A0ECA...\n\n# Set new expiry date for primary key and subkey(s)\n# NOTE: MUST put the primary key first, expire date, subkeys after in the same command\ngpg --quick-set-expire B98C7D8073BB87... 1y 2E418AB946A0ECA...\n\n# Check that the keys are no longer expired\ngpg --list-secret-keys --verbose --with-subkey-fingerprints\n\n&gt;&gt;&gt; sec   ed25519 2022-04-19 [SC] [expires: 2025-04-19]\n&gt;&gt;&gt;       B98C7D8073BB87...\n&gt;&gt;&gt; uid           [ultimate] Chris Birch &lt;ichrisbirch@gmail.com&gt;\n&gt;&gt;&gt; ssb   cv25519 2022-04-19 [E] [expires: 2025-04-19]\n&gt;&gt;&gt;       2E418AB946A0ECA...\n\n# Remove the expired email address for git-secret\ngit secret removeperson ichrisbirch@gmail.com\n\n&gt;&gt;&gt; git-secret: removed keys.\n&gt;&gt;&gt; git-secret: now [ichrisbirch@gmail.com] do not have an access to the repository.\n&gt;&gt;&gt; git-secret: make sure to hide the existing secrets again.\n\n# Add the email address as authorized viewer\ngit secret tell ichrisbirch@gmail.com\n\ngit-secret: done. ichrisbirch@gmail.com added as user(s) who know the secret.\n\n# Hide the secrets again\ngit secret hide\n\n&gt;&gt;&gt; git-secret: done. 3 of 3 files are hidden.\n\n# Check status to see that they are hidden\ngit status\n\n&gt;&gt;&gt;        modified:   .dev.env.secret\n&gt;&gt;&gt;        modified:   .gitsecret/keys/pubring.kbx\n&gt;&gt;&gt;        modified:   .gitsecret/keys/pubring.kbx~\n&gt;&gt;&gt;        modified:   .prod.env.secret\n&gt;&gt;&gt;        modified:   .test.env.secret\n</code></pre>"},{"location":"homelab-deployment/","title":"Homelab Production Deployment","text":"<p>This document covers deploying ichrisbirch to a Proxmox homelab environment with Docker and Cloudflare Tunnel.</p>"},{"location":"homelab-deployment/#architecture","title":"Architecture","text":"<pre><code>Internet \u2192 Cloudflare (SSL/CDN) \u2192 Cloudflare Tunnel \u2192 Traefik (port 80) \u2192 Docker Services\n</code></pre> <ul> <li>SSL/TLS: Handled by Cloudflare (no certificates needed locally)</li> <li>Reverse Proxy: Traefik handles routing, CORS, security headers, rate limiting</li> <li>No port forwarding: Tunnel connects outbound, no open ports on router</li> <li>Security: Home IP hidden behind Cloudflare</li> </ul>"},{"location":"homelab-deployment/#current-status","title":"Current Status","text":"<ul> <li>Infrastructure: Proxmox LXC container at <code>10.0.20.11</code> with Docker</li> <li>Database: PostgreSQL restored from AWS backup, working</li> <li>Containers: All services running (postgres, redis, api, app, chat, scheduler, traefik)</li> <li>External Access: Via Cloudflare Tunnel</li> </ul>"},{"location":"homelab-deployment/#quick-start-bootstrap-script","title":"Quick Start (Bootstrap Script)","text":"<p>For a fresh LXC container, use the bootstrap script:</p> <pre><code># As root on the LXC container\ncurl -fsSL https://raw.githubusercontent.com/datapointchris/ichrisbirch/master/scripts/bootstrap-homelab.sh | bash\n</code></pre> <p>Or if you've already cloned the repo:</p> <pre><code>sudo ./scripts/bootstrap-homelab.sh\n</code></pre> <p>The script handles Docker, AWS CLI, cloudflared, repository setup, and database initialization interactively.</p>"},{"location":"homelab-deployment/#manual-setup","title":"Manual Setup","text":""},{"location":"homelab-deployment/#1-proxmox-lxc-configuration","title":"1. Proxmox LXC Configuration","text":"<p>Ensure LXC has Docker-compatible settings in <code>/etc/pve/lxc/&lt;id&gt;.conf</code>:</p> <pre><code>lxc.apparmor.profile: unconfined\nlxc.cgroup2.devices.allow: a\nlxc.cap.drop:\nlxc.mount.auto: proc:rw sys:rw\n</code></pre>"},{"location":"homelab-deployment/#2-container-setup","title":"2. Container Setup","text":"<pre><code># Install prerequisites\napt update &amp;&amp; apt install -y curl git\n\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n\n# Install AWS CLI (for SSM parameter access)\napt install -y awscli\n\n# Clone repository\ncd /srv\ngit clone https://github.com/datapointchris/ichrisbirch.git\ncd ichrisbirch\n\n# Install CLI\nsudo ln -sf /srv/ichrisbirch/cli/ichrisbirch /usr/local/bin/ichrisbirch\n\n# Set up AWS credentials\nmkdir -p ~/.aws\n# Copy credentials from local machine or configure\n</code></pre>"},{"location":"homelab-deployment/#cloudflare-tunnel-setup","title":"Cloudflare Tunnel Setup","text":""},{"location":"homelab-deployment/#step-1-add-domain-to-cloudflare","title":"Step 1: Add Domain to Cloudflare","text":"<ol> <li>Create account at dash.cloudflare.com</li> <li>Click \"Add a site\" \u2192 enter <code>ichrisbirch.com</code></li> <li>Select Free plan</li> <li>Note the nameservers Cloudflare provides (e.g., <code>aria.ns.cloudflare.com</code>)</li> </ol>"},{"location":"homelab-deployment/#step-2-update-namecheap-dns","title":"Step 2: Update Namecheap DNS","text":"<ol> <li>Log into Namecheap</li> <li>Domain List \u2192 Manage \u2192 find Nameservers</li> <li>Change to \"Custom DNS\"</li> <li>Enter both Cloudflare nameservers</li> <li>Wait for activation email (10 min - 48 hours, usually ~30 min)</li> </ol>"},{"location":"homelab-deployment/#step-3-create-tunnel","title":"Step 3: Create Tunnel","text":"<ol> <li>In Cloudflare: Zero Trust \u2192 Networks \u2192 Tunnels</li> <li>Click \"Create a tunnel\" \u2192 select \"Cloudflared\"</li> <li>Name it <code>ichrisbirch-homelab</code></li> <li>Copy the tunnel token (long string starting with <code>eyJ...</code>)</li> </ol>"},{"location":"homelab-deployment/#step-4-install-cloudflared","title":"Step 4: Install cloudflared","text":"<p>On the LXC container:</p> <pre><code># Add Cloudflare repository\ncurl -fsSL https://pkg.cloudflare.com/cloudflare-main.gpg | sudo tee /usr/share/keyrings/cloudflare-main.gpg &gt;/dev/null\necho \"deb [signed-by=/usr/share/keyrings/cloudflare-main.gpg] https://pkg.cloudflare.com/cloudflared $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/cloudflared.list\n\n# Install\nsudo apt update &amp;&amp; sudo apt install -y cloudflared\n\n# Install as service with your token\nsudo cloudflared service install &lt;YOUR_TUNNEL_TOKEN&gt;\n\n# Verify it's running\nsudo systemctl status cloudflared\n</code></pre>"},{"location":"homelab-deployment/#step-5-configure-tunnel-routes","title":"Step 5: Configure Tunnel Routes","text":"<p>In Cloudflare dashboard, configure Public Hostnames for your tunnel.</p> <p>All routes point to Traefik on port 80, which handles internal routing:</p> Hostname Service Notes ichrisbirch.com http://localhost:80 Flask app (root domain) http://localhost:80 Flask app (www redirect) api.ichrisbirch.com http://localhost:80 FastAPI backend chat.ichrisbirch.com http://localhost:80 Streamlit chat (enable WebSockets!) <p>Traefik routes based on the <code>Host</code> header to the appropriate service.</p> <p>For chat (WebSocket support):</p> <ul> <li>Click the route \u2192 Additional settings</li> <li>Enable WebSockets</li> </ul>"},{"location":"homelab-deployment/#deployment","title":"Deployment","text":""},{"location":"homelab-deployment/#start-services","title":"Start Services","text":"<pre><code>cd /srv/ichrisbirch\n\n# Start production (fetches secrets from SSM, starts all services)\nichrisbirch prod start\n</code></pre> <p>This will:</p> <ul> <li>Fetch secrets from AWS SSM (POSTGRES_PASSWORD, REDIS_PASSWORD)</li> <li>Create the proxy network if needed</li> <li>Start all services via Docker Compose</li> </ul>"},{"location":"homelab-deployment/#verify-services","title":"Verify Services","text":"<pre><code># Check container status\nichrisbirch prod status\n\n# View logs\nichrisbirch prod logs\n\n# Test health endpoints\nichrisbirch prod health\n\n# Check tunnel status\nsudo systemctl status cloudflared\n</code></pre>"},{"location":"homelab-deployment/#access-urls","title":"Access URLs","text":"<p>Once tunnel is configured:</p> <ul> <li>App: https://ichrisbirch.com (also ) <li>API: https://api.ichrisbirch.com</li> <li>Chat: https://chat.ichrisbirch.com</li>"},{"location":"homelab-deployment/#ssm-parameters","title":"SSM Parameters","text":"<p>The following SSM parameters are configured for homelab Docker networking:</p> <pre><code># Already configured:\n/ichrisbirch/production/postgres/host = \"postgres\"\n/ichrisbirch/production/redis/host = \"redis\"\n\n# Add for cookie domain:\naws ssm put-parameter --region us-east-2 \\\n  --name \"/ichrisbirch/production/flask/cookie_domain\" \\\n  --value \"ichrisbirch.com\" --type String --overwrite\n</code></pre>"},{"location":"homelab-deployment/#cli-commands","title":"CLI Commands","text":"<pre><code>ichrisbirch prod start    # Fetch SSM secrets and start services\nichrisbirch prod stop     # Stop all services\nichrisbirch prod restart  # Restart without rebuilding\nichrisbirch prod rebuild  # Rebuild images and restart\nichrisbirch prod logs     # View container logs\nichrisbirch prod status   # Check service status\nichrisbirch prod health   # Run health checks\n</code></pre>"},{"location":"homelab-deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"homelab-deployment/#tunnel-not-connecting","title":"Tunnel Not Connecting","text":"<pre><code># Check cloudflared logs\nsudo journalctl -u cloudflared -f\n\n# Restart tunnel\nsudo systemctl restart cloudflared\n</code></pre>"},{"location":"homelab-deployment/#services-not-accessible-via-tunnel","title":"Services Not Accessible via Tunnel","text":"<pre><code># Verify Traefik is listening\ncurl -I http://localhost:80\n\n# Check container status\nichrisbirch prod status\n\n# View Traefik logs\ndocker logs icb-prod-traefik\n</code></pre>"},{"location":"homelab-deployment/#websocket-issues-chat","title":"WebSocket Issues (Chat)","text":"<p>Ensure WebSockets are enabled in the tunnel route settings for chat.ichrisbirch.com.</p>"},{"location":"homelab-deployment/#database-role-does-not-exist","title":"Database Role Does Not Exist","text":"<p>If you see <code>FATAL: role \"ichrisbirch\" does not exist</code>:</p> <pre><code># Create the role (get password from SSM first)\nPG_PASS=$(aws ssm get-parameter --region us-east-2 --name \"/ichrisbirch/production/postgres/password\" --with-decryption --query 'Parameter.Value' --output text)\n\ndocker exec icb-prod-postgres psql -U postgres -c \"CREATE ROLE ichrisbirch WITH LOGIN PASSWORD '$PG_PASS';\"\ndocker exec icb-prod-postgres psql -U postgres -c \"ALTER ROLE ichrisbirch CREATEDB;\"\ndocker exec icb-prod-postgres psql -U postgres -c \"ALTER DATABASE ichrisbirch OWNER TO ichrisbirch;\"\n</code></pre>"},{"location":"homelab-deployment/#database-restore-from-backup","title":"Database Restore from Backup","text":"<p>If you need to restore from a <code>.dump</code> file:</p> <pre><code># Stop app services first\ndocker stop icb-prod-api icb-prod-app icb-prod-chat icb-prod-scheduler\n\n# Terminate existing connections and recreate database\ndocker exec icb-prod-postgres psql -U postgres -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'ichrisbirch' AND pid &lt;&gt; pg_backend_pid();\"\ndocker exec icb-prod-postgres psql -U postgres -c \"DROP DATABASE ichrisbirch;\"\ndocker exec icb-prod-postgres psql -U postgres -c \"CREATE DATABASE ichrisbirch OWNER ichrisbirch;\"\n\n# Restore from dump\ncat /path/to/backup.dump | docker exec -i icb-prod-postgres pg_restore -U ichrisbirch -d ichrisbirch --no-owner\n\n# Start services\ndocker start icb-prod-api icb-prod-app icb-prod-chat icb-prod-scheduler\n</code></pre>"},{"location":"homelab-deployment/#sslprotocol-errors","title":"SSL/Protocol Errors","text":"<p>If you see <code>[SSL: WRONG_VERSION_NUMBER]</code> errors in logs:</p> <p>The internal services are trying to use HTTPS to communicate, but with Cloudflare Tunnel, internal communication should be HTTP (Cloudflare handles TLS externally).</p> <p>Check SSM parameter:</p> <pre><code>aws ssm get-parameter --region us-east-2 --name \"/ichrisbirch/production/protocol\" --query 'Parameter.Value' --output text\n</code></pre> <p>Should be <code>http</code>. If it's <code>https</code>, fix it:</p> <pre><code>aws ssm put-parameter --region us-east-2 --name \"/ichrisbirch/production/protocol\" --value \"http\" --type String --overwrite\n</code></pre> <p>Then restart services: <code>ichrisbirch prod restart</code></p>"},{"location":"homelab-deployment/#volume-naming-issues","title":"Volume Naming Issues","text":"<p>Docker Compose prefixes volumes with the project name. If you rebuild with a different project name, you may get new empty volumes.</p> <p>The CLI uses <code>--project-name icb-prod</code>, creating volumes like <code>icb-prod-postgres-data</code>.</p> <p>If your data is in differently-named volumes, check:</p> <pre><code>docker volume ls | grep postgres\n</code></pre> <p>You may need to copy data between volumes or update the project name.</p>"},{"location":"homelab-deployment/#container-info","title":"Container Info","text":"<ul> <li>LXC Host: ichrisbirch-lxc @ <code>10.0.20.11</code></li> <li>Services:</li> <li>traefik (port 80, receives tunnel traffic)</li> <li>postgres (5432, Docker internal)</li> <li>redis (6379, Docker internal)</li> <li>api (8000, Docker internal)</li> <li>app (5000, Docker internal)</li> <li>chat (8505, Docker internal)</li> <li>scheduler (no port)</li> <li>cloudflared (tunnel daemon, systemd service)</li> </ul>"},{"location":"homelab-deployment/#checklist","title":"Checklist","text":"<ul> <li> LXC container configured for Docker</li> <li> Docker and prerequisites installed</li> <li> Repository cloned</li> <li> CLI installed</li> <li> AWS credentials configured</li> <li> SSM parameters updated for Docker networking</li> <li> Flask cookie settings made configurable</li> <li> Traefik configured for Cloudflare Tunnel (HTTP-only)</li> <li> Domain added to Cloudflare</li> <li> Namecheap nameservers updated</li> <li> Cloudflare Tunnel created</li> <li> cloudflared installed and running</li> <li> Tunnel routes configured (pointing to localhost:80)</li> <li> SSM parameter added for cookie domain</li> <li> Full login flow tested</li> </ul>"},{"location":"html5_semantic/","title":"Semantic HTML","text":"<p>HTML5 introduced a set of semantic elements that provide meaningful information about the content they wrap, making web pages more readable for both developers and machines (like search engines or screen readers). Below is an example of a basic web page utilizing several common HTML5 semantic tags, along with explanations of when and why each tag is used.</p>"},{"location":"html5_semantic/#example-html5-page-with-semantic-tags","title":"Example HTML5 Page with Semantic Tags","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Example HTML5 Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;header&gt;\n    &lt;h1&gt;My Website&lt;/h1&gt;\n    &lt;nav&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;a href=\"#home\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/nav&gt;\n&lt;/header&gt;\n\n&lt;section id=\"home\"&gt;\n    &lt;h2&gt;Welcome to My Website&lt;/h2&gt;\n    &lt;p&gt;This is a paragraph explaining what my website is about.&lt;/p&gt;\n&lt;/section&gt;\n\n&lt;article&gt;\n    &lt;h2&gt;Blog Post Title&lt;/h2&gt;\n    &lt;p&gt;Posted on &lt;time datetime=\"2023-04-01\"&gt;April 1, 2023&lt;/time&gt;&lt;/p&gt;\n    &lt;p&gt;This is a blog post. It contains interesting content about a certain topic.&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;aside&gt;\n    &lt;h2&gt;About Me&lt;/h2&gt;\n    &lt;p&gt;This section provides information about the website owner or related links.&lt;/p&gt;\n&lt;/aside&gt;\n\n&lt;footer&gt;\n    &lt;p&gt;Contact information and copyright notice goes here.&lt;/p&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"html5_semantic/#explanations-of-semantic-tags","title":"Explanations of Semantic Tags","text":""},{"location":"html5_semantic/#header","title":"<code>&lt;header&gt;</code>","text":"<ul> <li>When to use: For introductory content or navigation links at the top of a section or page.</li> <li>Why: Helps identify the top part of a page or section, often containing the website's logo, navigation links, or titles.</li> </ul>"},{"location":"html5_semantic/#nav","title":"<code>&lt;nav&gt;</code>","text":"<ul> <li>When to use: For navigation links.</li> <li>Why: Indicates a section with navigation links to other pages or parts of the page. Search engines and screen readers can identify the navigation structure of a site more easily.</li> </ul>"},{"location":"html5_semantic/#section","title":"<code>&lt;section&gt;</code>","text":"<ul> <li>When to use: For a thematic grouping of content, typically with a heading.</li> <li>Why: Organizes the page content into thematic groups for easier understanding and navigation. Each <code>&lt;section&gt;</code> should ideally represent a standalone part of the page.</li> </ul>"},{"location":"html5_semantic/#article","title":"<code>&lt;article&gt;</code>","text":"<ul> <li>When to use: For self-contained, independent pieces of content that could be distributed and reused, like blog posts or news articles.</li> <li>Why: Marks content as being a complete, self-contained piece of the page's content. It's important for syndication and when separating document sections that could stand alone or be reused.</li> </ul>"},{"location":"html5_semantic/#aside","title":"<code>&lt;aside&gt;</code>","text":"<ul> <li>When to use: For tangentially related content to the main content, such as sidebars.</li> <li>Why: Differentiates side content from the main content, making it clear that it's supplementary. Good for sidebars, advertising, or content that complements the main content.</li> </ul>"},{"location":"html5_semantic/#footer","title":"<code>&lt;footer&gt;</code>","text":"<ul> <li>When to use: For footer content at the bottom of a section or page.</li> <li>Why: Contains information about the author, copyright notices, contact information, etc. It marks the end of a section or document.</li> </ul>"},{"location":"html5_semantic/#time","title":"<code>&lt;time&gt;</code>","text":"<ul> <li>When to use: To represent a specific period (a date or time).</li> <li>Why: Provides a standard way to encode dates and times in HTML, making it easier for machines to interpret datetime values in a human-readable format.</li> </ul> <p>These semantic elements are crucial for creating a well-structured and accessible webpage, improving both user experience and SEO.</p>"},{"location":"logging-configuration/","title":"Logging Configuration","text":""},{"location":"logging-configuration/#overview","title":"Overview","text":"<p>The ichrisbirch application uses structlog with a stdout-only architecture. All logs go to stdout, and Docker handles persistence via its logging driver. This is the industry-standard approach for containerized applications.</p> <p>Configuration: <code>ichrisbirch/logger.py</code></p> <pre><code>LOG_FORMAT = os.environ.get('LOG_FORMAT', 'console')\nLOG_LEVEL = os.environ.get('LOG_LEVEL', 'DEBUG')\nLOG_COLORS = os.environ.get('LOG_COLORS', 'auto')\nLOG_FILE = os.environ.get('LOG_FILE', '')\nLOG_DIR = os.environ.get('LOG_DIR', '/var/log/ichrisbirch')\n</code></pre>"},{"location":"logging-configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"logging-configuration/#log_format","title":"LOG_FORMAT","text":"<p>Controls the output format for log messages.</p> Value Description Use Case <code>console</code> (default) Human-readable colored output Development, debugging <code>json</code> Structured JSON output Production, log aggregation (ELK, Datadog) <p>Console format example:</p> <pre><code>2026-01-13T15:30:45Z [info     ] user_login_success    filename=auth.py func_name=login lineno=45 request_id=abc12345 user_id=123\n</code></pre> <p>JSON format example:</p> <pre><code>{\"timestamp\": \"2026-01-13T15:30:45Z\", \"level\": \"info\", \"event\": \"user_login_success\", \"filename\": \"auth.py\", \"func_name\": \"login\", \"lineno\": 45, \"request_id\": \"abc12345\", \"user_id\": 123}\n</code></pre>"},{"location":"logging-configuration/#log_level","title":"LOG_LEVEL","text":"<p>Controls which log messages are output.</p> Value Description <code>DEBUG</code> (default) All messages including detailed diagnostics <code>INFO</code> Normal operations (user login, task created, job completed) <code>WARNING</code> Recoverable issues (invalid input, retry attempts) <code>ERROR</code> Failures requiring attention (API errors, DB errors) <code>CRITICAL</code> System-level failures (can't connect to DB)"},{"location":"logging-configuration/#log_colors","title":"LOG_COLORS","text":"<p>Controls whether colored output is used in console format.</p> Value Description <code>auto</code> (default) Use TTY detection (colors if terminal, no colors if piped) <code>true</code> Force colors on (useful in Docker where TTY detection fails) <code>false</code> Force colors off"},{"location":"logging-configuration/#log_file","title":"LOG_FILE","text":"<p>Optional path to a log file for persistence. When set and the directory exists, logs are written to both stdout and the specified file.</p> Value Description Empty (default) No file logging, stdout only <code>/var/log/ichrisbirch/api.log</code> Write logs to file (and stdout) <p>File logging features:</p> <ul> <li>Uses <code>RotatingFileHandler</code> to prevent unbounded growth</li> <li>Max 25MB per file with 5 backup files (~150MB total per service)</li> <li>Plain text format (no ANSI color codes) for easy parsing</li> <li>Enables the Admin UI live logs feature</li> </ul>"},{"location":"logging-configuration/#log_dir","title":"LOG_DIR","text":"<p>Directory where log files are stored (used by admin UI for log aggregation).</p> Value Description <code>/var/log/ichrisbirch</code> (default) Standard log directory <p>Docker volume mount:</p> <pre><code>volumes:\n  - ichrisbirch_logs:/var/log/ichrisbirch\n</code></pre>"},{"location":"logging-configuration/#request-tracing","title":"Request Tracing","text":"<p>All services support request tracing via the <code>X-Request-ID</code> header. When a request arrives, middleware generates or extracts a request ID and binds it to structlog's context variables. All subsequent log messages include this ID, enabling correlation across services.</p>"},{"location":"logging-configuration/#request-id-flow","title":"Request ID Flow","text":"<pre><code>Flask Process                          FastAPI Process\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1. Request arrives\n2. Middleware generates/extracts:\n   X-Request-ID: abc12345\n3. bind_contextvars(request_id)\n4. All Flask logs include request_id\n5. API client adds header to request\n              \u2502\n              \u2514\u2500\u2500\u2500\u2500 HTTP Request \u2500\u2500\u2500\u2500\u25ba 6. Request arrives with header\n                                       7. Middleware extracts request_id\n                                       8. bind_contextvars(request_id)\n                                       9. All FastAPI logs include request_id\n</code></pre>"},{"location":"logging-configuration/#middleware-implementation","title":"Middleware Implementation","text":"<p>Flask: <code>ichrisbirch/app/middleware.py</code></p> <pre><code>class RequestTracingMiddleware:\n    def _before_request(self):\n        structlog.contextvars.clear_contextvars()\n        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4())[:8])\n        g.request_id = request_id\n        structlog.contextvars.bind_contextvars(request_id=request_id)\n</code></pre> <p>FastAPI: <code>ichrisbirch/api/middleware.py</code></p> <pre><code>class RequestTracingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        structlog.contextvars.clear_contextvars()\n        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4())[:8])\n        structlog.contextvars.bind_contextvars(request_id=request_id)\n        # ...\n</code></pre>"},{"location":"logging-configuration/#viewing-logs","title":"Viewing Logs","text":""},{"location":"logging-configuration/#development","title":"Development","text":"<p>Use the CLI to view colored logs that persist across container restarts:</p> <pre><code># All services\n./cli/ichrisbirch dev logs\n\n# Specific service\n./cli/ichrisbirch dev logs api\n./cli/ichrisbirch dev logs app\n</code></pre> <p>The logs command uses a watch loop that automatically reconnects when containers restart.</p>"},{"location":"logging-configuration/#testing","title":"Testing","text":"<pre><code># View test environment logs\n./cli/ichrisbirch testing logs\n\n# Specific service\n./cli/ichrisbirch testing logs api\n</code></pre>"},{"location":"logging-configuration/#production","title":"Production","text":"<pre><code># View production logs\n./cli/ichrisbirch prod logs\n\n# Specific service\n./cli/ichrisbirch prod logs api\n</code></pre>"},{"location":"logging-configuration/#direct-docker-commands","title":"Direct Docker Commands","text":"<pre><code># Follow logs for a specific container\ndocker logs -f icb-dev-api\n\n# View recent logs\ndocker logs --tail 100 icb-dev-api\n</code></pre>"},{"location":"logging-configuration/#log-persistence","title":"Log Persistence","text":"<p>Docker handles log persistence via its logging driver. By default, Docker uses the <code>json-file</code> driver which stores logs on disk. These logs persist across container restarts.</p>"},{"location":"logging-configuration/#accessing-docker-log-files","title":"Accessing Docker Log Files","text":"<pre><code># Find log file location\ndocker inspect icb-dev-api --format='{{.LogPath}}'\n\n# View raw log file (requires sudo)\nsudo cat /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log\n</code></pre>"},{"location":"logging-configuration/#log-rotation","title":"Log Rotation","text":"<p>Docker manages log rotation. Configure in Docker daemon settings if needed:</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n</code></pre>"},{"location":"logging-configuration/#structlog-configuration","title":"Structlog Configuration","text":"<p>Configuration file: <code>ichrisbirch/logger.py:42-75</code></p> <p>The structlog configuration includes:</p> <ol> <li>Context variable merging: Includes request_id and other bound context</li> <li>Log level filtering: Based on LOG_LEVEL environment variable</li> <li>Timestamp: ISO 8601 format (UTC)</li> <li>Call site tracking: Automatic filename, function name, and line number</li> <li>Stack info rendering: For exception tracebacks</li> <li>Format rendering: Console or JSON based on LOG_FORMAT</li> </ol>"},{"location":"logging-configuration/#processor-chain","title":"Processor Chain","text":"<pre><code>processors = [\n    structlog.contextvars.merge_contextvars,      # Include request_id, etc.\n    structlog.processors.add_log_level,           # Add level field\n    structlog.processors.TimeStamper(...),        # Add timestamp\n    CallsiteParameterAdder({...}),                # Add filename, func, line\n    structlog.processors.StackInfoRenderer(),     # Render stack traces\n    structlog.processors.format_exc_info,         # Format exceptions\n    # Final renderer: ConsoleRenderer or JSONRenderer\n]\n</code></pre>"},{"location":"logging-configuration/#third-party-logger-suppression","title":"Third-Party Logger Suppression","text":"<p>Noisy third-party loggers are suppressed to reduce log noise:</p> <p>Configuration: <code>ichrisbirch/logger.py:78-101</code></p> Logger Level Reason <code>apscheduler</code> WARNING Verbose job scheduling messages <code>httpx</code> WARNING HTTP client request details <code>werkzeug</code> WARNING Flask development server messages <code>boto3</code>, <code>botocore</code> INFO AWS SDK details <code>sqlalchemy_json</code> INFO JSON field operations"},{"location":"logging-configuration/#usage-pattern","title":"Usage Pattern","text":"<p>Every module uses the same simple pattern:</p> <pre><code>import structlog\n\nlogger = structlog.get_logger()\n\n# Log with structured data\nlogger.info('user_login_success', user_id=user.id, email=user.email)\nlogger.error('database_error', error=str(e), operation='create_user')\n</code></pre>"},{"location":"logging-configuration/#event-naming-convention","title":"Event Naming Convention","text":"<p>Use snake_case event names that describe what happened:</p> Good Bad <code>user_login_success</code> <code>logged in user</code> <code>task_created</code> <code>Created new task</code> <code>api_request_failed</code> <code>HTTP error</code> <code>job_started</code> <code>Starting job...</code>"},{"location":"logging-configuration/#migration-from-standard-logging","title":"Migration from Standard Logging","text":"<p>The project migrated from Python's standard logging module to structlog. Key differences:</p> Standard Logging Structlog <code>logger = logging.getLogger(__name__)</code> <code>logger = structlog.get_logger()</code> <code>logger.info(f'User {user_id} logged in')</code> <code>logger.info('user_login', user_id=user_id)</code> String interpolation Keyword arguments <code>__name__</code> for logger identification CallsiteParameterAdder for location"},{"location":"logging-configuration/#benefits","title":"Benefits","text":"<ol> <li>Structured data: All log data is structured, enabling filtering and analysis</li> <li>Request tracing: Correlate logs across services with request_id</li> <li>Consistent format: Same output format across all services</li> <li>Environment flexibility: Switch between console and JSON with one variable</li> <li>No file management: Docker handles persistence and rotation</li> <li>Industry standard: Follows 12-factor app logging principles</li> </ol>"},{"location":"logging-configuration/#admin-ui-integration","title":"Admin UI Integration","text":"<p>When <code>LOG_FILE</code> is configured, the admin dashboard provides live log viewing capabilities.</p>"},{"location":"logging-configuration/#live-logs-feature","title":"Live Logs Feature","text":"<p>The admin UI at <code>/admin/logs/</code> provides real-time log streaming via WebSocket:</p> <ul> <li>WebSocket endpoint: <code>wss://api.docker.localhost/admin/log-stream/</code></li> <li>Authentication: JWT cookie-based (set automatically on Flask login)</li> <li>Admin-only: Requires admin user privileges</li> <li>ANSI stripping: Color codes removed for clean browser display</li> <li>Client-side colorization: JavaScript re-applies colors based on log level</li> </ul>"},{"location":"logging-configuration/#log-graphs-feature","title":"Log Graphs Feature","text":"<p>The admin UI at <code>/admin/log-graphs/</code> provides log analytics:</p> <ul> <li>Reads all <code>*.log</code> files from <code>LOG_DIR</code></li> <li>Parses structlog format into structured data</li> <li>Generates charts for log levels, timestamps, and sources</li> <li>Useful for identifying patterns and issues</li> </ul>"},{"location":"logging-configuration/#docker-configuration","title":"Docker Configuration","text":"<p>To enable file logging in Docker Compose:</p> <pre><code>api:\n  environment:\n    - LOG_FILE=/var/log/ichrisbirch/api.log\n  volumes:\n    - ichrisbirch_logs:/var/log/ichrisbirch\n\napp:\n  environment:\n    - LOG_FILE=/var/log/ichrisbirch/app.log\n  volumes:\n    - ichrisbirch_logs:/var/log/ichrisbirch\n</code></pre> <p>All services share the same log volume, allowing the admin UI to aggregate logs from all services.</p>"},{"location":"nginx-to-traefik-migration-complete/","title":"Nginx to Traefik Migration Complete","text":"<p>This document summarizes the successful migration from nginx to Traefik reverse proxy, including the major CLI simplification that eliminated confusing command duplication.</p>"},{"location":"nginx-to-traefik-migration-complete/#migration-objectives-results","title":"\ud83c\udfaf Migration Objectives &amp; Results","text":""},{"location":"nginx-to-traefik-migration-complete/#primary-objectives-achieved","title":"\u2705 Primary Objectives Achieved","text":"<ol> <li>\u2705 Replace nginx with modern Traefik reverse proxy</li> <li>Modern dynamic service discovery via Docker labels</li> <li>Automatic HTTPS termination with certificate management</li> <li> <p>Built-in health checks and load balancing</p> </li> <li> <p>\u2705 Eliminate confusing CLI command duplication </p> </li> <li>SOLVED: Removed all <code>traefik-*</code> commands that duplicated regular environment commands</li> <li> <p>RESULT: Clean, professional CLI interface with single commands per operation</p> </li> <li> <p>\u2705 Implement browser-trusted HTTPS for local development</p> </li> <li>SOLVED: Integrated mkcert for browser-trusted certificates</li> <li> <p>RESULT: No more browser security warnings during development</p> </li> <li> <p>\u2705 Maintain service isolation across environments</p> </li> <li>SOLVED: Environment-specific Docker Compose files and configurations</li> <li>RESULT: Development, testing, and production environments properly isolated</li> </ol>"},{"location":"nginx-to-traefik-migration-complete/#cli-simplification-success","title":"\ud83d\ude80 CLI Simplification Success","text":""},{"location":"nginx-to-traefik-migration-complete/#before-migration-confusing-duplication","title":"Before Migration (Confusing Duplication)","text":"<p>The CLI had confusing command duplication where multiple commands did the same thing:</p> <pre><code># CONFUSING - Multiple commands for the same operation\nichrisbirch traefik start dev    # Started dev environment with Traefik\nichrisbirch dev start            # Also started dev environment with Traefik\n\n# Users were confused about which command to use\n# Implementation details (Traefik) were exposed in user interface\n# Inconsistent command patterns\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#after-migration-clean-professional","title":"After Migration (Clean &amp; Professional)","text":"<p>Complete CLI refactoring eliminated all duplication:</p> <pre><code># CLEAN - Single command per operation\nichrisbirch dev start            # Starts dev with Traefik + HTTPS (automatic)\nichrisbirch testing start       # Starts testing with Traefik + HTTPS (automatic)  \nichrisbirch prod start          # Starts prod with Traefik + HTTPS (automatic)\n\n# Implementation details hidden from users\n# Consistent patterns across all environments\n# Professional CLI design following industry standards\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#removed-commands-eliminated-duplication","title":"\u274c Removed Commands (Eliminated Duplication)","text":"<p>All <code>traefik-*</code> commands have been completely removed:</p> <ul> <li><code>ichrisbirch traefik start &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; start</code>)</li> <li><code>ichrisbirch traefik stop &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; stop</code>)</li> <li><code>ichrisbirch traefik restart &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; restart</code>)</li> <li><code>ichrisbirch traefik status &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; status</code>)</li> <li><code>ichrisbirch traefik logs &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; logs</code>)</li> <li><code>ichrisbirch traefik health &lt;env&gt;</code> \u2192 REMOVED (use <code>ichrisbirch &lt;env&gt; health</code>)</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#current-simplified-commands","title":"\u2705 Current Simplified Commands","text":"<pre><code># Environment Management (with Traefik + HTTPS by default)\nichrisbirch dev start           # Start development environment\nichrisbirch dev stop            # Stop development environment\nichrisbirch dev restart         # Restart development environment\nichrisbirch dev status          # Show status + HTTPS URLs\nichrisbirch dev logs            # View logs\nichrisbirch dev health          # Run health checks\n\n# SSL Certificate Management (top-level command)\nichrisbirch ssl-manager generate dev    # Generate certificates (prefers mkcert)\nichrisbirch ssl-manager info dev        # Show certificate details\nichrisbirch ssl-manager validate dev    # Validate certificates\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#browser-trusted-https-implementation","title":"\ud83d\udd12 Browser-Trusted HTTPS Implementation","text":""},{"location":"nginx-to-traefik-migration-complete/#mkcert-integration-success","title":"mkcert Integration Success","text":"<p>Problem Solved: Browser security warnings for local development</p> <p>Solution Implemented:</p> <pre><code># Install mkcert (one-time setup)\nbrew install mkcert\nmkcert -install\n\n# Generate browser-trusted certificates\nichrisbirch ssl-manager generate dev\n</code></pre> <p>Results:</p> <ul> <li>\u2705 No browser warnings: Certificates trusted by Chrome, Safari, Firefox</li> <li>\u2705 Proper Subject Alternative Names: Wildcard + specific subdomains</li> <li>\u2705 Long validity: 2+ year certificate lifetime</li> <li>\u2705 Automatic fallback: OpenSSL used when mkcert unavailable</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#certificate-strategy","title":"Certificate Strategy","text":"<pre><code># mkcert generates these domains for development:\n- docker.localhost\n- *.docker.localhost  \n- api.docker.localhost\n- app.docker.localhost\n- chat.docker.localhost\n- dashboard.docker.localhost\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#architecture-transformation","title":"\ud83c\udfd7\ufe0f Architecture Transformation","text":""},{"location":"nginx-to-traefik-migration-complete/#before-nginx-static-configuration","title":"Before: nginx Static Configuration","text":"<pre><code>nginx.conf (static file)\n\u251c\u2500\u2500 server blocks (manual updates)\n\u251c\u2500\u2500 upstream blocks (manual service discovery)\n\u251c\u2500\u2500 SSL configuration (manual certificates)\n\u2514\u2500\u2500 location blocks (manual routing)\n</code></pre> <p>Problems:</p> <ul> <li>Manual configuration file updates for new services</li> <li>Static routing that required nginx reloads</li> <li>Manual SSL certificate management</li> <li>No built-in health checks</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#after-traefik-dynamic-discovery","title":"After: Traefik Dynamic Discovery","text":"<pre><code>Traefik Dynamic Configuration\n\u251c\u2500\u2500 Docker labels (automatic service discovery)\n\u251c\u2500\u2500 Environment-specific configs (dynamic-dev/, dynamic-testing/, dynamic-prod/)\n\u251c\u2500\u2500 Automatic SSL termination (mkcert + OpenSSL fallback)\n\u2514\u2500\u2500 Built-in health checks and load balancing\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 Automatic service discovery: New services appear immediately</li> <li>\u2705 Dynamic configuration: No restarts needed for changes</li> <li>\u2705 Modern HTTPS: Automatic certificate management</li> <li>\u2705 Built-in monitoring: Dashboard and health checks included</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#environment-configuration","title":"\ud83c\udf10 Environment Configuration","text":""},{"location":"nginx-to-traefik-migration-complete/#development-environment","title":"Development Environment","text":"<ul> <li>Domain: <code>*.docker.localhost</code></li> <li>Port: 443 (standard HTTPS)</li> <li>Certificates: mkcert browser-trusted</li> <li>Dashboard: <code>https://dashboard.docker.localhost/</code> (dev/devpass)</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#testing-environment","title":"Testing Environment","text":"<ul> <li>Domain: <code>*.test.localhost</code></li> <li>Port: 8443 (custom HTTPS port)</li> <li>Certificates: Environment-specific</li> <li>Dashboard: <code>https://dashboard.test.localhost:8443/</code> (test/testpass)</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#production-environment","title":"Production Environment","text":"<ul> <li>Domain: <code>*.yourdomain.local</code></li> <li>Port: 443 (standard HTTPS)</li> <li>Certificates: Production-grade</li> <li>Dashboard: Restricted access</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#file-structure-changes","title":"\ud83d\udcc1 File Structure Changes","text":""},{"location":"nginx-to-traefik-migration-complete/#new-traefik-structure","title":"New Traefik Structure","text":"<pre><code>deploy-containers/traefik/\n\u251c\u2500\u2500 certs/                    # SSL certificates\n\u2502   \u251c\u2500\u2500 dev.crt|key          # Development (mkcert generated)\n\u2502   \u251c\u2500\u2500 testing.crt|key      # Testing environment\n\u2502   \u2514\u2500\u2500 prod.crt|key         # Production environment\n\u2502\n\u251c\u2500\u2500 dynamic-dev/              # Development configuration\n\u2502   \u251c\u2500\u2500 tls.yml              # TLS configuration\n\u2502   \u2514\u2500\u2500 middlewares.yml      # Development middleware\n\u2502\n\u251c\u2500\u2500 dynamic-testing/          # Testing configuration  \n\u2502   \u251c\u2500\u2500 tls.yml\n\u2502   \u2514\u2500\u2500 middlewares.yml\n\u2502\n\u251c\u2500\u2500 dynamic-prod/             # Production configuration\n\u2502   \u251c\u2500\u2500 tls.yml\n\u2502   \u2514\u2500\u2500 middlewares.yml\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 ssl-manager.sh       # Certificate management (mkcert support)\n\u2502\n\u2514\u2500\u2500 traefik.yml              # Base Traefik configuration\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#removed-nginx-files","title":"Removed nginx Files","text":"<pre><code>deploy-containers/nginx/      # REMOVED ENTIRELY\n\u251c\u2500\u2500 nginx.conf               # REMOVED\n\u251c\u2500\u2500 sites-available/         # REMOVED\n\u251c\u2500\u2500 ssl/                     # REMOVED\n\u2514\u2500\u2500 scripts/                 # REMOVED\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#docker-compose-evolution","title":"\ud83d\udd27 Docker Compose Evolution","text":""},{"location":"nginx-to-traefik-migration-complete/#service-label-configuration","title":"Service Label Configuration","text":"<p>Services now use Docker labels for automatic Traefik configuration:</p> <pre><code># Example: API service configuration\napi:\n  labels:\n    - \"traefik.enable=true\"\n    - \"traefik.http.routers.api-dev.rule=Host(`api.docker.localhost`)\"\n    - \"traefik.http.routers.api-dev.entrypoints=websecure\"\n    - \"traefik.http.routers.api-dev.tls=true\"\n    - \"traefik.http.services.api-dev.loadbalancer.server.port=8000\"\n    - \"traefik.http.services.api-dev.loadbalancer.healthcheck.path=/health\"\n</code></pre> <p>Benefits over nginx:</p> <ul> <li>No manual configuration file updates</li> <li>Automatic service discovery</li> <li>Built-in health checks</li> <li>Dynamic updates without restarts</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#troubleshooting-migration-issues","title":"\ud83d\udea8 Troubleshooting Migration Issues","text":""},{"location":"nginx-to-traefik-migration-complete/#common-post-migration-issues","title":"Common Post-Migration Issues","text":""},{"location":"nginx-to-traefik-migration-complete/#1-browser-certificate-warnings","title":"1. Browser Certificate Warnings","text":"<p>Problem: Self-signed certificates show security warnings</p> <p>Solution:</p> <pre><code># Install and use mkcert for browser-trusted certificates\nbrew install mkcert\nmkcert -install\nichrisbirch ssl-manager generate dev\nichrisbirch dev restart\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#2-command-not-found-errors","title":"2. Command Not Found Errors","text":"<p>Problem: Trying to use removed <code>traefik-*</code> commands</p> <p>Solution: Use simplified commands instead:</p> <pre><code># WRONG (removed)\nichrisbirch traefik start dev\n\n# CORRECT (current)\nichrisbirch dev start\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#3-port-conflicts","title":"3. Port Conflicts","text":"<p>Problem: Previous nginx containers conflicting with Traefik</p> <p>Solution:</p> <pre><code># Stop all nginx containers  \ndocker stop $(docker ps -q --filter \"name=nginx\")\ndocker rm $(docker ps -aq --filter \"name=nginx\")\n\n# Start with Traefik\nichrisbirch dev start\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#4-dns-resolution-issues","title":"4. DNS Resolution Issues","text":"<p>Problem: Old nginx URLs no longer resolving</p> <p>Solution: Update <code>/etc/hosts</code> for new domains:</p> <pre><code># Add Traefik domains\necho \"127.0.0.1 api.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 app.docker.localhost\" | sudo tee -a /etc/hosts  \necho \"127.0.0.1 chat.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 dashboard.docker.localhost\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#performance-and-monitoring-improvements","title":"\ud83d\udcca Performance and Monitoring Improvements","text":""},{"location":"nginx-to-traefik-migration-complete/#built-in-traefik-dashboard","title":"Built-in Traefik Dashboard","text":"<p>Access: <code>https://dashboard.docker.localhost/</code> (dev/devpass)</p> <p>Features:</p> <ul> <li>Real-time service discovery status</li> <li>Request metrics and response times</li> <li>Health check results</li> <li>Certificate information</li> <li>Router configuration validation</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#health-check-integration","title":"Health Check Integration","text":"<pre><code># Comprehensive environment health check\nichrisbirch dev health\n\n# Example output:\n# [\u2713] Container: icb-dev-traefik (Up 2 hours)\n# [\u2713] Container: icb-dev-api (Up 2 hours (healthy))\n# [\u2713] DNS: api.docker.localhost found in /etc/hosts\n# [\u2713] API Health: HTTP 200 (OK)\n# [\u2713] App Frontend: HTTP 200 (OK)\n</code></pre>"},{"location":"nginx-to-traefik-migration-complete/#user-experience-improvements","title":"\ud83c\udf1f User Experience Improvements","text":""},{"location":"nginx-to-traefik-migration-complete/#developer-workflow-benefits","title":"Developer Workflow Benefits","text":"<p>Before (nginx + duplicated commands):</p> <ol> <li>Choose between confusing duplicate commands</li> <li>Manually update nginx configuration for new services</li> <li>Restart nginx for configuration changes</li> <li>Deal with browser certificate warnings</li> <li>Limited monitoring and debugging tools</li> </ol> <p>After (Traefik + simplified CLI):</p> <ol> <li>Single command per operation: <code>ichrisbirch dev start</code></li> <li>Automatic service discovery: New services appear immediately</li> <li>Dynamic updates: No restarts needed</li> <li>Browser-trusted HTTPS: No certificate warnings with mkcert</li> <li>Built-in monitoring: Dashboard and comprehensive health checks</li> </ol>"},{"location":"nginx-to-traefik-migration-complete/#onboarding-improvements","title":"Onboarding Improvements","text":"<p>New developers benefit from:</p> <ul> <li>Simplified CLI: No need to understand reverse proxy implementation details</li> <li>Consistent patterns: Same commands work across all environments</li> <li>Better documentation: Clear explanations without confusing alternatives</li> <li>Modern HTTPS: Works out-of-the-box without certificate warnings</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#migration-success-metrics","title":"\ud83c\udfaf Migration Success Metrics","text":""},{"location":"nginx-to-traefik-migration-complete/#cli-simplification-results","title":"CLI Simplification Results","text":"<ul> <li>\u2705 Eliminated 6 duplicate commands (all <code>traefik-*</code> variants)</li> <li>\u2705 Reduced cognitive load for new developers</li> <li>\u2705 Improved command discoverability through <code>ichrisbirch help</code></li> <li>\u2705 Professional CLI patterns following industry standards</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>\u2705 Browser-trusted HTTPS with mkcert (no security warnings)</li> <li>\u2705 Automatic service discovery (no manual configuration updates)</li> <li>\u2705 Built-in monitoring via Traefik dashboard</li> <li>\u2705 Dynamic configuration (no restarts for changes)</li> <li>\u2705 Environment isolation maintained across dev/testing/prod</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#developer-experience","title":"Developer Experience","text":"<ul> <li>\u2705 Faster onboarding: Simplified commands and clearer documentation</li> <li>\u2705 Reduced errors: Fewer command options reduce user confusion</li> <li>\u2705 Better debugging: Comprehensive health checks and monitoring</li> <li>\u2705 Modern tooling: Professional-grade reverse proxy with dashboard</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#future-considerations","title":"\ud83d\ude80 Future Considerations","text":""},{"location":"nginx-to-traefik-migration-complete/#potential-enhancements","title":"Potential Enhancements","text":"<ol> <li>Let's Encrypt Integration: For production environments with real domains</li> <li>Advanced Load Balancing: Weighted routing and circuit breakers</li> <li>Observability: Integration with Prometheus/Grafana for metrics</li> <li>Service Mesh: Consider Traefik Mesh for microservice communication</li> </ol>"},{"location":"nginx-to-traefik-migration-complete/#maintenance-notes","title":"Maintenance Notes","text":"<ul> <li>Certificate Renewal: mkcert certificates valid for 2+ years</li> <li>Configuration Updates: Use environment-specific dynamic configuration directories</li> <li>Security: Regular review of middleware and TLS configurations</li> <li>Performance: Monitor dashboard for service health and response times</li> </ul>"},{"location":"nginx-to-traefik-migration-complete/#summary","title":"\ud83d\udcdd Summary","text":"<p>The nginx to Traefik migration has been successfully completed with significant improvements:</p>"},{"location":"nginx-to-traefik-migration-complete/#major-achievements","title":"\u2705 Major Achievements","text":"<ol> <li>CLI Duplication Eliminated: Removed confusing <code>traefik-*</code> commands</li> <li>Browser-Trusted HTTPS: mkcert integration eliminates security warnings  </li> <li>Modern Architecture: Dynamic service discovery with automatic configuration</li> <li>Professional UX: Clean, consistent CLI interface following industry standards</li> </ol>"},{"location":"nginx-to-traefik-migration-complete/#key-benefits","title":"\ud83c\udfaf Key Benefits","text":"<ul> <li>Simplified Operations: Single command per operation (<code>ichrisbirch dev start</code>)</li> <li>Better Security: Browser-trusted certificates with proper Subject Alternative Names</li> <li>Improved Monitoring: Built-in dashboard and comprehensive health checks</li> <li>Faster Development: Automatic service discovery and dynamic updates</li> </ul> <p>The migration provides a modern, professional foundation for the iChrisBirch application with significantly improved developer experience and operational simplicity.</p>"},{"location":"project_layout/","title":"Project Layout","text":""},{"location":"project_layout/#environment-files","title":"Environment Files","text":"<p>Location: <code>/</code> <code>.dev.env</code> <code>.test.env</code> <code>.prod.env</code></p>"},{"location":"project_layout/#project-configuration","title":"Project Configuration","text":"<p>Location: <code>/</code> <code>config.py</code> - Config classes for environments</p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get the iChrisBirch application running locally with modern Traefik deployment and browser-trusted HTTPS in under 5 minutes.</p>"},{"location":"quick-start/#prerequisites","title":"\ud83d\ude80 Prerequisites","text":""},{"location":"quick-start/#required-software","title":"Required Software","text":"<ul> <li>Docker: Download from docker.com</li> <li>Docker Compose: Included with Docker Desktop</li> <li>mkcert: For browser-trusted SSL certificates</li> </ul>"},{"location":"quick-start/#install-mkcert-recommended","title":"Install mkcert (Recommended)","text":"<p>mkcert generates browser-trusted certificates that eliminate security warnings during development.</p> <pre><code># macOS\nbrew install mkcert\n\n# Linux\ncurl -JLO \"https://dl.filippo.io/mkcert/latest?for=linux/amd64\"\nchmod +x mkcert-v*-linux-amd64\nsudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\n# Install the local Certificate Authority\nmkcert -install\n</code></pre>"},{"location":"quick-start/#setup-local-dns","title":"Setup Local DNS","text":"<p>Add these entries to your <code>/etc/hosts</code> file:</p> <pre><code># Add to /etc/hosts (one-time setup)\necho \"127.0.0.1 api.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 app.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 chat.docker.localhost\" | sudo tee -a /etc/hosts\necho \"127.0.0.1 dashboard.docker.localhost\" | sudo tee -a /etc/hosts\n</code></pre>"},{"location":"quick-start/#quick-start-commands","title":"\u26a1 Quick Start Commands","text":""},{"location":"quick-start/#1-start-development-environment","title":"1. Start Development Environment","text":"<pre><code># Navigate to project directory\ncd ichrisbirch\n\n# Start development environment (includes HTTPS + Traefik automatically)\n./cli/ichrisbirch dev start\n</code></pre> <p>What this does:</p> <ul> <li>Generates SSL certificates (prefers mkcert for browser trust)</li> <li>Starts all services with Docker Compose</li> <li>Configures Traefik reverse proxy with HTTPS</li> <li>Displays access URLs when ready</li> </ul>"},{"location":"quick-start/#2-verify-everything-works","title":"2. Verify Everything Works","text":"<pre><code># Check service status and URLs\n./cli/ichrisbirch dev status\n\n# Run comprehensive health checks\n./cli/ichrisbirch dev health\n</code></pre>"},{"location":"quick-start/#3-access-your-applications","title":"3. Access Your Applications","text":"<p>Open these URLs in your browser (no security warnings with mkcert):</p> <ul> <li>API: https://api.docker.localhost/</li> <li>App: https://app.docker.localhost/</li> <li>Chat: https://chat.docker.localhost/</li> <li>Dashboard: https://dashboard.docker.localhost/ (dev/devpass)</li> </ul>"},{"location":"quick-start/#what-you-get","title":"\ud83c\udfaf What You Get","text":""},{"location":"quick-start/#modern-development-environment","title":"Modern Development Environment","text":"<ul> <li>\u2705 Browser-trusted HTTPS: No certificate warnings with mkcert</li> <li>\u2705 Automatic service discovery: Traefik detects services automatically  </li> <li>\u2705 Professional URLs: Clean subdomain-based routing</li> <li>\u2705 Built-in monitoring: Traefik dashboard with real-time metrics</li> <li>\u2705 Health checks: Comprehensive service health monitoring</li> </ul>"},{"location":"quick-start/#simplified-cli-interface","title":"Simplified CLI Interface","text":"<p>The CLI has been completely simplified to eliminate confusing command duplication:</p> <pre><code># Single commands for each operation\n./cli/ichrisbirch dev start       # Start development (Traefik + HTTPS automatic)\n./cli/ichrisbirch dev status      # Show status + URLs\n./cli/ichrisbirch dev health      # Run health checks  \n./cli/ichrisbirch dev logs        # View service logs\n./cli/ichrisbirch dev stop        # Stop development\n\n# SSL certificate management\n./cli/ichrisbirch ssl-manager generate dev    # Generate certificates\n./cli/ichrisbirch ssl-manager info dev        # Certificate information\n</code></pre> <p>No more confusing command duplication - each operation has exactly one command.</p>"},{"location":"quick-start/#environment-details","title":"\ud83c\udf10 Environment Details","text":""},{"location":"quick-start/#development-environment","title":"Development Environment","text":"<p>Domains: <code>*.docker.localhost</code> Port: 443 (standard HTTPS) Certificates: mkcert browser-trusted</p> <p>Services:</p> <ul> <li>FastAPI Backend: <code>https://api.docker.localhost/</code></li> <li>Flask Frontend: <code>https://app.docker.localhost/</code> </li> <li>Streamlit Chat: <code>https://chat.docker.localhost/</code></li> <li>Traefik Dashboard: <code>https://dashboard.docker.localhost/</code> (dev/devpass)</li> </ul>"},{"location":"quick-start/#whats-running","title":"What's Running","text":"<pre><code># Check what's running\n./cli/ichrisbirch dev status\n\n# Example output:\n# Container Status:\n# [\u2713] icb-dev-traefik    (Up 2 minutes)\n# [\u2713] icb-dev-postgres   (Up 2 minutes (healthy))\n# [\u2713] icb-dev-api        (Up 2 minutes (healthy))\n# [\u2713] icb-dev-app        (Up 2 minutes (healthy))\n# [\u2713] icb-dev-chat       (Up 2 minutes)\n#\n# Development environment URLs:\n#   API:       https://api.docker.localhost/\n#   APP:       https://app.docker.localhost/\n#   CHAT:      https://chat.docker.localhost/\n#   DASHBOARD: https://dashboard.docker.localhost/\n</code></pre>"},{"location":"quick-start/#common-commands","title":"\ud83d\udd27 Common Commands","text":""},{"location":"quick-start/#daily-development-workflow","title":"Daily Development Workflow","text":"<pre><code># Start development\n./cli/ichrisbirch dev start\n\n# Quick status check  \n./cli/ichrisbirch dev status\n\n# View API logs while developing\n./cli/ichrisbirch dev logs api\n\n# Stop when done\n./cli/ichrisbirch dev stop\n</code></pre>"},{"location":"quick-start/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Run comprehensive health check\n./cli/ichrisbirch dev health\n\n# View specific service logs\n./cli/ichrisbirch dev logs traefik    # Traefik proxy logs\n./cli/ichrisbirch dev logs api        # API backend logs\n./cli/ichrisbirch dev logs app        # Flask app logs\n\n# Restart if needed\n./cli/ichrisbirch dev restart\n</code></pre>"},{"location":"quick-start/#ssl-certificate-management","title":"SSL Certificate Management","text":"<pre><code># Generate certificates (automatic with mkcert)\n./cli/ichrisbirch ssl-manager generate dev\n\n# Check certificate information\n./cli/ichrisbirch ssl-manager info dev\n\n# Validate certificates\n./cli/ichrisbirch ssl-manager validate dev\n</code></pre>"},{"location":"quick-start/#quick-troubleshooting","title":"\ud83d\udea8 Quick Troubleshooting","text":""},{"location":"quick-start/#1-browser-shows-certificate-warning","title":"1. Browser Shows Certificate Warning","text":"<p>Problem: \"Not secure\" or certificate warning in browser</p> <p>Solution: Install mkcert for browser-trusted certificates</p> <pre><code>brew install mkcert\nmkcert -install\n./cli/ichrisbirch ssl-manager generate dev\n./cli/ichrisbirch dev restart\n</code></pre>"},{"location":"quick-start/#2-service-not-accessible","title":"2. Service Not Accessible","text":"<p>Problem: 404 or connection refused</p> <p>Solution: Check service status and health</p> <pre><code>./cli/ichrisbirch dev status\n./cli/ichrisbirch dev health\n./cli/ichrisbirch dev logs\n</code></pre>"},{"location":"quick-start/#3-port-conflicts","title":"3. Port Conflicts","text":"<p>Problem: \"Port already in use\" errors</p> <p>Solution: Stop conflicting services</p> <pre><code>./cli/ichrisbirch dev stop\ndocker stop $(docker ps -q --filter \"name=ichrisbirch\")\n./cli/ichrisbirch dev start\n</code></pre>"},{"location":"quick-start/#4-dns-resolution-issues","title":"4. DNS Resolution Issues","text":"<p>Problem: \"Server not found\" errors</p> <p>Solution: Check <code>/etc/hosts</code> entries</p> <pre><code>grep docker.localhost /etc/hosts\n# Should show: 127.0.0.1 api.docker.localhost (etc.)\n</code></pre>"},{"location":"quick-start/#next-steps","title":"\ud83d\udcda Next Steps","text":""},{"location":"quick-start/#learn-more","title":"Learn More","text":"<ul> <li>CLI Management Guide - Complete CLI reference</li> <li>Traefik Deployment Guide - Detailed Traefik configuration</li> <li>SSL Certificate Troubleshooting - Certificate issues and solutions</li> <li>CLI Command Troubleshooting - CLI usage and migration guide</li> </ul>"},{"location":"quick-start/#development-workflows","title":"Development Workflows","text":"<ul> <li>Developer Setup - Complete development environment setup</li> <li>Testing Guide - Running tests and test environments</li> <li>Docker Development - Docker workflows and debugging</li> </ul>"},{"location":"quick-start/#other-environments","title":"Other Environments","text":"<pre><code># Testing environment (different port and domain)\n./cli/ichrisbirch testing start\n# Access at: https://api.test.localhost:8443/\n\n# Production environment (requires Cloudflare Tunnel setup)\n./cli/ichrisbirch prod start\n# Fetches secrets from AWS SSM, starts services\n# Access at: https://api.ichrisbirch.com/ (via Cloudflare Tunnel)\n</code></pre> <p>Note: Production requires additional setup. See Homelab Deployment Guide</p>"},{"location":"quick-start/#benefits-of-modern-setup","title":"\ud83c\udf1f Benefits of Modern Setup","text":""},{"location":"quick-start/#compared-to-traditional-development","title":"Compared to Traditional Development","text":"<p>Traditional (nginx/Apache):</p> <ul> <li>Manual configuration files for each service</li> <li>Self-signed certificates with browser warnings</li> <li>Manual service discovery and routing updates</li> <li>Limited monitoring and debugging tools</li> </ul> <p>Modern (Traefik + mkcert):</p> <ul> <li>\u2705 Automatic service discovery: Services appear immediately when started</li> <li>\u2705 Browser-trusted HTTPS: No security warnings during development  </li> <li>\u2705 Professional URLs: Clean subdomain-based routing</li> <li>\u2705 Built-in monitoring: Traefik dashboard with real-time metrics</li> <li>\u2705 Simplified CLI: One command per operation, no confusing duplication</li> </ul>"},{"location":"quick-start/#developer-experience","title":"Developer Experience","text":"<ul> <li>\u26a1 Faster startup: Single command starts everything</li> <li>\ud83d\udd12 Secure by default: HTTPS with trusted certificates  </li> <li>\ud83c\udfaf Simple commands: No need to understand reverse proxy details</li> <li>\ud83d\udcca Better debugging: Comprehensive health checks and monitoring</li> <li>\ud83d\udc65 Team consistency: Same setup works for all developers</li> </ul> <p>The modern setup provides a professional development environment that matches production behavior while being simple enough for daily development use.</p> <p>\ud83c\udf89 You're ready to develop! The application should now be running with browser-trusted HTTPS at the URLs above. Check the CLI Management Guide for complete command reference.</p>"},{"location":"scheduler/","title":"Scheduler","text":""},{"location":"scheduler/#apscheduler","title":"APScheduler","text":"<p>The scheduler is run in it's own <code>wsgi</code> application managed by <code>supervisor</code>. The scheduler is using the standard blocking scheduler since it is in its own process. Workers need to be set to 1 for <code>gunicorn</code> in order to not start multiple instances of the scheduler. Technically the scheduler could be run as part of the API since the tasks are related to the API, but the API will be changing to async in the future which would require a different scheduler, and the jobs may not always be only related to the API.</p> <p>The jobs are located in the <code>jobs.py</code> file in the <code>/scheduler</code> directory.</p>"},{"location":"scheduler/#current-jobs","title":"Current Jobs","text":"<p><code>decrease_task_priority</code> - Decreases the priority of all tasks by 1 every 24 hours.</p> <p><code>check_and_run_autotasks</code> - Checks if any autotasks need to be run based on their schedule and runs them if so.</p> <p><code>backup_database</code> - Backs up the postgres database to S3 every 3 days.</p>"},{"location":"terraform/","title":"Terraform","text":""},{"location":"terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"terraform/#terraform-state-is-locked","title":"Terraform State is Locked","text":"<p>Github Runners sometimes lock the state.</p> <p>Locally: <code>terraform plan</code> -&gt; This will give you an ID of the lock. <code>terraform force-unlock $LOCK_ID</code></p>"},{"location":"terraform/#terraform-plan-backend-configuration-changed","title":"terraform plan -&gt; backend configuration changed","text":"<p>Reference: Confusing error message when terraform backend is changed - Terraform - HashiCorp Discuss</p> <pre><code>\u279c terraform plan\n\u2577\n\u2502 Error: Backend initialization required: please run \"terraform init\"\n\u2502\n\u2502 Reason: Backend configuration block has changed\n\u2502\n\u2502 The \"backend\" is the interface that Terraform uses to store state,\n\u2502 perform operations, etc. If this message is showing up, it means that the\n\u2502 Terraform configuration you're using is using a custom configuration for\n\u2502 the Terraform backend.\n\u2502\n\u2502 Changes to backend configurations require reinitialization. This allows\n\u2502 Terraform to set up the new configuration, copy existing state, etc. Please run\n\u2502 \"terraform init\" with either the \"-reconfigure\" or \"-migrate-state\" flags to\n\u2502 use the current configuration.\n\u2502\n\u2502 If the change reason above is incorrect, please verify your configuration\n\u2502 hasn't changed and try again. At this point, no changes to your existing\n\u2502 configuration or state have been made.\n\u2575\n\n\n\u279c terraform init\nInitializing the backend...\n\u2577\n\u2502 Error: Backend configuration changed\n\u2502\n\u2502 A change in the backend configuration has been detected, which may require migrating existing state.\n\u2502\n\u2502 If you wish to attempt automatic migration of the state, use \"terraform init -migrate-state\".\n\u2502 If you wish to store the current configuration with no changes to the state, use \"terraform init -reconfigure\".\n\n\n\u279c terraform init -migrate-state\nInitializing the backend...\nBackend configuration changed!\n\nTerraform has detected that the configuration specified for the backend\nhas changed. Terraform will now check for existing state in the backends.\n\n\u2577\n\u2502 Error: Failed to decode current backend config\n\u2502\n\u2502 The backend configuration created by the most recent run of \"terraform init\" could not be decoded: unsupported attribute \"assume_role_duration_seconds\". The configuration may have been initialized by an earlier version that\n\u2502 used an incompatible configuration structure. Run \"terraform init -reconfigure\" to force re-initialization of the backend.\n\u2575\n\n\n\u279c terraform init -reconfigure\nInitializing the backend...\n\nSuccessfully configured the backend \"s3\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing provider plugins...\n- Reusing previous version of hashicorp/aws from the dependency lock file\n- Reusing previous version of hashicorp/tls from the dependency lock file\n- Using previously-installed hashicorp/aws v5.67.0\n- Using previously-installed hashicorp/tls v4.0.6\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>This was caused by an upgrade to terraform, a new version had different configuration options.</p>"},{"location":"traefik-deployment/","title":"Traefik Reverse Proxy Deployment Guide","text":"<p>This guide covers the complete deployment of Traefik reverse proxy for the iChrisBirch application, providing modern HTTPS termination, dynamic load balancing, and browser-trusted certificates for local development.</p>"},{"location":"traefik-deployment/#overview","title":"\ud83c\udfaf Overview","text":"<p>Traefik v3.4 serves as the modern reverse proxy solution with a simplified, unified CLI interface that has eliminated confusing command duplication:</p> <ul> <li>One command per operation: <code>ichrisbirch dev start</code> (no more separate <code>traefik start dev</code>)</li> <li>Dynamic service discovery via Docker labels</li> <li>Browser-trusted HTTPS with mkcert for local development</li> <li>Automatic certificate management with fallback strategies</li> <li>Centralized routing for all application services</li> <li>Production-ready configuration with security best practices</li> </ul>"},{"location":"traefik-deployment/#cli-simplification-benefits","title":"\ud83d\ude80 CLI Simplification Benefits","text":"<p>The CLI has been completely refactored to provide a clean, professional interface:</p> <p>Before (Confusing):</p> <ul> <li><code>ichrisbirch traefik start dev</code> vs <code>ichrisbirch dev start</code> (did the same thing)</li> <li>Users needed to understand Traefik implementation details</li> <li>Inconsistent command patterns</li> </ul> <p>After (Clean &amp; Simple):</p> <ul> <li>Single command: <code>ichrisbirch dev start</code> uses Traefik + HTTPS by default</li> <li>Hidden implementation: Users don't need to know about reverse proxy details  </li> <li>Consistent patterns: All environments work the same way</li> <li>Modern HTTPS by default: No separate \"traefik\" commands needed</li> </ul>"},{"location":"traefik-deployment/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":""},{"location":"traefik-deployment/#traefik-vs-nginx","title":"Traefik vs. Nginx","text":"Feature Nginx (Legacy) Traefik (Modern) Configuration Static files Dynamic Docker labels SSL/TLS Manual setup Automatic certificate management Service Discovery Manual config updates Automatic via Docker WebSocket Support Manual proxy_pass Built-in with headers Load Balancing Manual upstream blocks Automatic service discovery Health Checks External scripts Built-in health monitoring Hot Reload Requires restart Automatic configuration reload"},{"location":"traefik-deployment/#environment-separation","title":"Environment Separation","text":"<ul> <li>Development: <code>*.docker.localhost</code> with mkcert browser-trusted certificates</li> <li>Testing: <code>*.test.localhost:8443</code> with isolated testing certificates</li> <li>Production: <code>*.ichrisbirch.com</code> via Cloudflare Tunnel (see Homelab Deployment)</li> </ul>"},{"location":"traefik-deployment/#proxy-network-isolation","title":"Proxy Network Isolation","text":"<p>Each environment uses its own proxy network to avoid routing conflicts:</p> Environment Proxy Network Traefik Constraint Development <code>proxy-dev</code> <code>traefik.environment=development</code> Testing <code>proxy-test</code> <code>traefik.environment=testing</code> Production <code>proxy</code> (no constraint needed) <p>This allows all environments to run simultaneously. Traefik only discovers containers with matching environment labels.</p>"},{"location":"traefik-deployment/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<pre><code>deploy-containers/traefik/\n\u251c\u2500\u2500 certs/                           # SSL certificates\n\u2502   \u251c\u2500\u2500 dev.crt, dev.key            # Development environment (mkcert generated)\n\u2502   \u251c\u2500\u2500 testing.crt, testing.key    # Testing environment\n\u2502   \u2514\u2500\u2500 prod.crt, prod.key          # Production environment\n\u251c\u2500\u2500 dynamic-dev/                     # Development-specific configuration\n\u2502   \u251c\u2500\u2500 tls.yml                     # Development TLS config\n\u2502   \u2514\u2500\u2500 middlewares.yml             # Development middleware\n\u251c\u2500\u2500 dynamic-testing/                 # Testing-specific configuration\n\u2502   \u251c\u2500\u2500 tls.yml                     # Testing TLS config\n\u2502   \u2514\u2500\u2500 middlewares.yml             # Testing middleware\n\u251c\u2500\u2500 dynamic-prod/                    # Production-specific configuration\n\u2502   \u251c\u2500\u2500 tls.yml                     # Production TLS config\n\u2502   \u2514\u2500\u2500 middlewares.yml             # Production middleware\n\u251c\u2500\u2500 scripts/                        # Management utilities\n\u2502   \u2514\u2500\u2500 ssl-manager.sh              # Certificate management (with mkcert support)\n\u2514\u2500\u2500 traefik.yml                     # Base Traefik configuration\n</code></pre>"},{"location":"traefik-deployment/#configuration-details","title":"\ud83d\udd27 Configuration Details","text":""},{"location":"traefik-deployment/#service-discovery","title":"Service Discovery","text":"<p>Traefik discovers services automatically via Docker labels:</p> <pre><code># Example service configuration\napi:\n  labels:\n    - \"traefik.enable=true\"\n    - \"traefik.http.routers.api-dev.rule=Host(`api.docker.localhost`)\"\n    - \"traefik.http.routers.api-dev.entrypoints=websecure\"\n    - \"traefik.http.routers.api-dev.tls=true\"\n    - \"traefik.http.routers.api-dev.tls.options=dev@file\"\n    - \"traefik.http.routers.api-dev.middlewares=cors-dev@file,security-headers-dev@file\"\n    - \"traefik.http.services.api-dev.loadbalancer.server.port=8000\"\n</code></pre>"},{"location":"traefik-deployment/#middleware-stack","title":"Middleware Stack","text":"<p>Each environment includes:</p> <ul> <li>CORS Headers: Cross-origin request handling</li> <li>Security Headers: XSS protection, content security policy</li> <li>Rate Limiting: Request throttling (environment-specific)</li> <li>WebSocket Support: For Streamlit chat service</li> <li>TLS Options: Cipher suites and protocol versions</li> </ul>"},{"location":"traefik-deployment/#network-architecture","title":"Network Architecture","text":"<pre><code>graph TB\n    Client[Client Browser] --&gt; Traefik[Traefik Reverse Proxy]\n    Traefik --&gt; API[FastAPI Backend]\n    Traefik --&gt; APP[Flask Frontend]\n    Traefik --&gt; CHAT[Streamlit Chat]\n    API --&gt; DB[(PostgreSQL)]\n    API --&gt; CACHE[(Redis)]\n    APP --&gt; API\n    CHAT --&gt; API\n\n    subgraph \"Docker Networks\"\n        Traefik -.-&gt; PROXY[proxy network]\n        API -.-&gt; DEFAULT[default network]\n        APP -.-&gt; DEFAULT\n        CHAT -.-&gt; DEFAULT\n        DB -.-&gt; DEFAULT\n        CACHE -.-&gt; DEFAULT\n    end</code></pre>"},{"location":"traefik-deployment/#environment-urls","title":"\ud83c\udf10 Environment URLs","text":""},{"location":"traefik-deployment/#development-environment","title":"Development Environment","text":"<ul> <li>API: https://api.docker.localhost/</li> <li>App: https://app.docker.localhost/</li> <li>Chat: https://chat.docker.localhost/</li> <li>Dashboard: https://dashboard.docker.localhost/ (dev/devpass)</li> </ul>"},{"location":"traefik-deployment/#testing-environment","title":"Testing Environment","text":"<ul> <li>API: https://api.test.localhost:8443/</li> <li>App: https://app.test.localhost:8443/</li> <li>Chat: https://chat.test.localhost:8443/</li> <li>Dashboard: https://dashboard.test.localhost:8443/ (test/testpass)</li> </ul>"},{"location":"traefik-deployment/#production-environment","title":"Production Environment","text":"<p>Production uses Cloudflare Tunnel for secure external access without exposing ports:</p> <ul> <li>API: https://api.ichrisbirch.com/</li> <li>App: https://app.ichrisbirch.com/</li> <li>Chat: https://chat.ichrisbirch.com/</li> </ul> <p>Cloudflare handles TLS termination; Traefik receives HTTP internally and provides routing, CORS, security headers, and rate limiting.</p> <p>Note: See Homelab Deployment Guide for complete production setup</p>"},{"location":"traefik-deployment/#simplified-deployment-commands","title":"\ud83d\ude80 Simplified Deployment Commands","text":""},{"location":"traefik-deployment/#modern-cli-interface-post-refactoring","title":"Modern CLI Interface (Post-Refactoring)","text":"<p>All environments now use simplified commands with Traefik + HTTPS by default:</p> <pre><code># Start any environment (uses Traefik automatically)\nichrisbirch dev start               # Development\nichrisbirch testing start          # Testing  \nichrisbirch prod start             # Production\n\n# Status and monitoring\nichrisbirch dev status             # Service status + URLs\nichrisbirch dev health             # Comprehensive health checks\nichrisbirch dev logs               # View service logs\n\n# SSL certificate management\nichrisbirch ssl-manager generate dev    # Generate certificates (prefers mkcert)\nichrisbirch ssl-manager info dev        # Certificate information\nichrisbirch ssl-manager validate dev    # Validate certificates\n</code></pre>"},{"location":"traefik-deployment/#removed-commands-no-longer-needed","title":"\u274c Removed Commands (No Longer Needed)","text":"<p>The following commands have been eliminated to reduce confusion:</p> <ul> <li><code>ichrisbirch traefik start &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; start</code></li> <li><code>ichrisbirch traefik stop &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; stop</code> </li> <li><code>ichrisbirch traefik status &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; status</code></li> <li><code>ichrisbirch traefik health &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; health</code></li> <li><code>ichrisbirch traefik logs &lt;env&gt;</code> \u2192 Use <code>ichrisbirch &lt;env&gt; logs</code></li> </ul>"},{"location":"traefik-deployment/#legacy-cli-commands-deprecated","title":"Legacy CLI Commands (Deprecated)","text":"<pre><code># DEPRECATED - DO NOT USE\nichrisbirch traefik start &lt;env&gt;      # REMOVED - Use ichrisbirch &lt;env&gt; start instead\nichrisbirch traefik stop &lt;env&gt;       # REMOVED - Use ichrisbirch &lt;env&gt; stop instead\nichrisbirch traefik restart &lt;env&gt;    # REMOVED - Use ichrisbirch &lt;env&gt; restart instead\nichrisbirch traefik status &lt;env&gt;     # REMOVED - Use ichrisbirch &lt;env&gt; status instead\nichrisbirch traefik logs &lt;env&gt;       # REMOVED - Use ichrisbirch &lt;env&gt; logs instead\nichrisbirch traefik health &lt;env&gt;     # REMOVED - Use ichrisbirch &lt;env&gt; health instead\n</code></pre>"},{"location":"traefik-deployment/#ssl-certificate-management","title":"SSL certificate management","text":"<p>ichrisbirch ssl-manager generate ENV   # Generate certificates ichrisbirch ssl-manager validate ENV   # Validate existing ichrisbirch ssl-manager info ENV       # Show certificate info</p>"},{"location":"traefik-deployment/#direct-script-usage","title":"Direct Script Usage","text":"<pre><code># Alternative: Use scripts directly\n./deploy-containers/traefik/scripts/deploy.sh dev up\n./deploy-containers/traefik/scripts/health-check.sh dev\n./deploy-containers/traefik/scripts/ssl-manager.sh info all\n</code></pre>"},{"location":"traefik-deployment/#docker-compose-commands","title":"Docker Compose Commands","text":"<pre><code># Development\ndocker-compose -f docker-compose.dev.yml up -d\n\n# Test  \ndocker-compose -f docker-compose.test.yml up -d\n\n# Production\ndocker-compose up -d\n</code></pre>"},{"location":"traefik-deployment/#ssl-certificate-management_1","title":"\ud83d\udd12 SSL Certificate Management","text":""},{"location":"traefik-deployment/#automatic-certificate-generation","title":"Automatic Certificate Generation","text":"<p>Certificates are generated automatically with appropriate Subject Alternative Names (SANs):</p> <pre><code># Generate certificates for all environments\nichrisbirch ssl-manager generate all\n\n# Generate specific environment\nichrisbirch ssl-manager generate dev\n</code></pre>"},{"location":"traefik-deployment/#certificate-details","title":"Certificate Details","text":"<ul> <li>Algorithm: RSA 2048-bit</li> <li>Validity: 365 days</li> <li>SANs: Wildcard domain + specific subdomains</li> <li>Storage: <code>deploy-containers/traefik/certs/</code></li> </ul>"},{"location":"traefik-deployment/#dns-configuration","title":"DNS Configuration","text":"<p>For local development, add entries to <code>/etc/hosts</code>:</p> <pre><code># Development environment\n127.0.0.1 api.docker.localhost\n127.0.0.1 app.docker.localhost  \n127.0.0.1 chat.docker.localhost\n127.0.0.1 dashboard.docker.localhost\n\n# Test environment\n127.0.0.1 api.test.localhost\n127.0.0.1 app.test.localhost\n127.0.0.1 chat.test.localhost\n127.0.0.1 dashboard.test.localhost\n</code></pre>"},{"location":"traefik-deployment/#health-monitoring","title":"\ud83d\udcca Health Monitoring","text":""},{"location":"traefik-deployment/#comprehensive-health-checks","title":"Comprehensive Health Checks","text":"<p>The health check system validates:</p> <ul> <li>Docker Containers: Status and health checks</li> <li>DNS Resolution: Local hosts and external domains</li> <li>HTTP Endpoints: API health, app frontend, chat service</li> <li>WebSocket Support: Streamlit WebSocket functionality</li> <li>Dashboard Access: Authentication and API availability</li> </ul>"},{"location":"traefik-deployment/#health-check-output","title":"Health Check Output","text":"<pre><code>$ ichrisbirch traefik health dev\n\nHealth Check for dev Environment\n========================================\n\n[\u2713] Container: icb-dev-traefik (Up 5 minutes)\n[\u2713] Container: icb-dev-api (Up 5 minutes (healthy))\n[\u2713] DNS: api.docker.localhost found in /etc/hosts (127.0.0.1)\n[\u2713] API Health: HTTP 200 (OK)\n[\u2713] Chat Service WebSocket: HTTP 426 (WebSocket upgrade supported)\n</code></pre>"},{"location":"traefik-deployment/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"traefik-deployment/#common-issues","title":"Common Issues","text":"<ol> <li>Port Conflicts</li> </ol> <pre><code># Stop conflicting services\ndocker stop $(docker ps -q --filter \"name=ichrisbirch\")\n</code></pre> <ol> <li>DNS Resolution</li> </ol> <pre><code># Add to /etc/hosts\necho \"127.0.0.1 api.docker.localhost\" | sudo tee -a /etc/hosts\n</code></pre> <ol> <li>Certificate Issues</li> </ol> <pre><code># Regenerate certificates\nichrisbirch ssl-manager generate all\n</code></pre> <ol> <li>Container Health</li> </ol> <pre><code># Check specific container logs\nichrisbirch traefik logs dev api\n</code></pre>"},{"location":"traefik-deployment/#verification-steps","title":"Verification Steps","text":"<ol> <li>Network Connectivity</li> </ol> <pre><code>curl -k -I https://api.docker.localhost/health\n</code></pre> <ol> <li>Container Status</li> </ol> <pre><code>ichrisbirch traefik status dev\n</code></pre> <ol> <li>Certificate Validation</li> </ol> <pre><code>ichrisbirch ssl-manager validate dev\n</code></pre>"},{"location":"traefik-deployment/#performance-considerations","title":"\ud83d\udcc8 Performance Considerations","text":""},{"location":"traefik-deployment/#development-vs-production","title":"Development vs. Production","text":"<ul> <li>Development: Optimized for fast iteration with file watching</li> <li>Test: Isolated with in-memory databases for speed</li> <li>Production: Optimized for performance with persistent storage</li> </ul>"},{"location":"traefik-deployment/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Traefik: Lightweight proxy with minimal overhead</li> <li>API: Multiple workers for production (4 workers)</li> <li>App: Gunicorn WSGI server for production</li> <li>Database: Production-tuned PostgreSQL settings</li> </ul>"},{"location":"traefik-deployment/#migration-from-nginx","title":"\ud83d\udd04 Migration from Nginx","text":""},{"location":"traefik-deployment/#advantages-of-migration","title":"Advantages of Migration","text":"<ol> <li>Dynamic Configuration: No manual config file updates</li> <li>Automatic Service Discovery: Docker label-based routing</li> <li>Built-in SSL: Automatic certificate management</li> <li>WebSocket Support: Native WebSocket proxying</li> <li>Health Monitoring: Integrated health checks</li> <li>Hot Reload: Configuration changes without restarts</li> </ol>"},{"location":"traefik-deployment/#legacy-support","title":"Legacy Support","text":"<p>The original nginx configuration remains available in <code>deploy-metal/</code> for compatibility and rollback scenarios.</p>"},{"location":"traefik-deployment/#advanced-configuration","title":"\ud83d\udee0\ufe0f Advanced Configuration","text":""},{"location":"traefik-deployment/#custom-middleware","title":"Custom Middleware","text":"<p>Add custom middleware in dynamic configuration files:</p> <pre><code># deploy-containers/traefik/dynamic/middlewares-dev.yml\nhttp:\n  middlewares:\n    custom-headers:\n      headers:\n        customRequestHeaders:\n          X-Custom-Header: \"development\"\n</code></pre>"},{"location":"traefik-deployment/#load-balancing","title":"Load Balancing","text":"<p>For production scaling:</p> <pre><code>labels:\n  - \"traefik.http.services.api.loadbalancer.server.port=8000\"\n  - \"traefik.http.services.api.loadbalancer.healthcheck.path=/health\"\n  - \"traefik.http.services.api.loadbalancer.healthcheck.interval=30s\"\n</code></pre>"},{"location":"traefik-deployment/#monitoring-integration","title":"Monitoring Integration","text":"<p>Traefik provides metrics endpoints for monitoring:</p> <ul> <li>Prometheus: <code>/metrics</code> endpoint</li> <li>Dashboard: Real-time service overview</li> <li>Access Logs: Structured request logging</li> </ul>"},{"location":"traefik-deployment/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Traefik Documentation</li> <li>Docker Compose Override</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#infrastructure-deployment","title":"\ud83d\udd27 Infrastructure &amp; Deployment","text":""},{"location":"troubleshooting/#cli-commands","title":"CLI Commands","text":"<ul> <li>CLI Command Troubleshooting - Issues with simplified CLI interface and eliminated command duplication</li> </ul>"},{"location":"troubleshooting/#ssl-certificates","title":"SSL Certificates","text":"<ul> <li>SSL Certificate Troubleshooting - Browser warnings, mkcert setup, and certificate validation issues</li> </ul>"},{"location":"troubleshooting/#application-issues","title":"\ud83d\udea8 Application Issues","text":""},{"location":"troubleshooting/#flask","title":"Flask","text":"<p>Error</p> <p>Blank pages loading, but no errors.</p> <p>Solution</p> <p>Try a different port Sometimes the port is busy or used, but does not give a 'port in use' error</p>"},{"location":"troubleshooting/#poetry","title":"Poetry","text":"<p>Error</p> <p>ModuleNotFoundError: No module named 'cachecontrol' when running poetry:</p> <p>Solution</p> <p><code>sudo apt install python3-cachecontrol</code></p>"},{"location":"troubleshooting/#supervisor","title":"Supervisor","text":"<p>Error</p> <p>supervisor.sock no such file</p> <p>Solution</p> <p>make sure directories and files for logs are created.</p> <p>Error</p> <p>BACKOFF can't find command... that is pointing to .venv</p> <p>Solution</p> <p>Prod: Check that the project is installed Dev: Check the symlink isn't broken</p> <p>Error</p> <pre><code>error: &lt;class 'FileNotFoundError'&gt;, [Errno 2] No such file or directory: file: /usr/local/Cellar/supervisor/4.2.5/libexec/lib/python3.11/site-packages/supervisor/xmlrpc.py line: 55\n</code></pre> <p>Solution</p> <p>Start and run supervisor with homebrew: <code>brew services start supervisor</code></p> <p>Error</p> <pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/var/www/ichrisbirch/ichrisbirch/NoneNone/pylogger.log'\n</code></pre> <p>Solution</p> <p>The environment file has not been loaded. Most likely you need to run <code>git secret reveal</code> This happens when the project has been cloned for the first time or directory has been deleted or the env files might have changed.</p>"},{"location":"troubleshooting/#nginx","title":"NGINX","text":"<p>Error</p> <p>bind() to 0.0.0.0:80 failed (98: Address already in use)</p> <p>Solution</p> <p><code>sudo pkill -f nginx &amp; wait $!</code> <code>sudo systemctl start nginx</code></p> <p>Error</p> <p>DEV bind() to 127.0.0.1:80 failed (13: Permission denied)</p> <p>Solution</p> <p>NGINX is not running as root.  It does not run reliably with homebrew. Use <code>sudo nginx -s reload</code> instead of homebrew.</p>"},{"location":"troubleshooting/#api-postgres","title":"API Postgres","text":"<p>Error</p> <p>[error] 94580#0: *18 kevent() reported that connect() failed (61: Connection refused) while connecting to upstream, client: 127.0.0.1, server: api.localhost, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:4200/\", host: \"api.macmini.local</p> <p>Solution</p> <p>DB cannot connect.  Postgres string was built wrong, corrected by adding a test to check config is loaded properly.</p> <p>Error</p> <p>Local changes were working but nothing that connected to prod postgres.</p> <p><code>api.ichrisbirch.com/tasks/</code> - 502 Bad Gateway <code>api.ichrisbirch.com</code> Success redirect to <code>/docs</code> <code>ichrisbirch.com</code> redirects to www in browser but error with requests <code>www.ichrisbirch.com/tasks/</code> - Internal Server Error Can connect to prod server with DBeaver Verified that the connection info is the same. Seems that the API is not connecting to postgres instance</p> <p>api.macmini.local WORKING api.macmini.local/ WORKING api.macmini.local/docs WORKING api.macmini.local/tasks WORKING api.macmini.local/tasks/1 WORKING api.macmini.local/tasks/completed</p> <p>ichrisbirch.com WORKING api.ichrisbirch.com/ WORKING api.ichrisbirch.com/docs ERROR api.ichrisbirch.com/tasks ERROR api.ichrisbirch.com/tasks/1 ERROR api.ichrisbirch.com/tasks/completed</p> <p>Solution</p> <p>The issue was resolved by modifying the security group of the postgres instance to allow the ec2 instance to connect by allowing it's security group.</p>"},{"location":"troubleshooting/#pytest","title":"Pytest","text":"<p>Error</p> <p>E       assert 307 == 200 E        +  where 307 = &lt;Response [307]&gt;.status_code</p> <p>Solution</p> <p>The trailing <code>/</code> is missing from the endpoint being called in the test, resulting in a 307 Temporary Redirect To fix: <code>/endpoint</code> --&gt; <code>/endpoint/</code></p>"},{"location":"troubleshooting/#alembic","title":"Alembic","text":"<p>Error</p> <p>Alembic is not able to upgrade to the latest because the revisions got out of sync.</p> <p>Solution</p> <p>Find the last revision that was successfully run (manually by inspecting the database) and then run: <code>alembic stamp &lt;revision&gt;</code> to set the current revision to the last successful one. Then run the upgrade again: <code>alembic upgrade head</code></p>"},{"location":"troubleshooting/#fastapi","title":"FastAPI","text":"<p>Error</p> <p>Request Error: Client error '405 Method Not Allowed' for url xxx</p> <p>Solution</p> <p>Make sure that the <code>id</code> is being passed correctly for routes like <code>/endpoint/{id}/</code> The error will not say <code>id</code> is not found, it will give a 405 error because the url is not correct</p> <p>Error</p> <p><code>PATCH</code> endpoint giving: 422 Unprocessable Entity: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"id\"],\"msg\":\"Field required\"</p> <p>Solution</p> <p><code>PATCH</code> endpoints require the ID in the endpoint, but also the ID must be passed in the payload for the model so it can update the record in the DB by ID.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#fastapi-crud-endpoints","title":"FastAPI Crud Endpoints","text":"<p>Order matters with endpoints, dynamic routes <code>route/endpoint/{id}</code> are last. They even have to be after other endpoints:</p> <p><code>route/</code> <code>route/endpoint/</code> <code>route/endpoint/extension</code> The two below don't matter the order, only that they are after all of the endpoints that do not take in a path variable. <code>route/{id}</code> <code>route/endpoint/{id}</code> </p>"},{"location":"api/#endpoint-structure","title":"Endpoint Structure","text":""},{"location":"api/#get-vs-post-vs-put-for-updating-resources","title":"GET vs POST vs PUT for Updating Resources","text":"<p>In RESTful API design, it's common to use a POST or PUT request when you want to update a resource.</p> <p>A <code>GET</code> request should be idempotent, meaning that making the same request multiple times should have the same effect as making it once. In your case, marking a task as complete changes the state of the task, so a <code>GET</code> request would not be appropriate.</p> <p>Between <code>POST</code> and <code>PUT</code>, the choice depends on whether you consider marking a task as complete to be a partial update of the task or a creation of a new state for the task.</p> <p>If you consider it to be a partial update, you should use a <code>PUT</code> request. If you consider it to be a creation of a new state, you should use a <code>POST</code> request.</p> <p>In your current implementation, you're using a <code>POST</code> request, which is perfectly fine. If you wanted to use a PUT request, you could change the decorator to <code>@router.put</code> and the route to something like <code>/task/{task_id}/complete/</code>.</p>"},{"location":"api/#endpoint-resource-vs-action-order","title":"Endpoint Resource vs Action Order","text":"<p>The choice between <code>/tasks/{task_id}/complete/</code> and <code>/tasks/complete/{task_id}/</code> is largely a matter of personal preference and the conventions you've established in your project. However, the most common and RESTful way to design the endpoint would be <code>/tasks/{task_id}/complete/</code>.</p> <p>This is because in REST, URLs are used to represent resources, and the components of the URL are used to form a hierarchy of resources. In this case, the task with a specific task_id is the resource, and complete is an action on that resource. So, it makes sense to structure the URL as <code>/tasks/{task_id}/complete/</code>, where complete is a sub-resource of the task.</p> <p>This structure also has the advantage of being consistent with the other endpoints in your code, which use the structure <code>/tasks/{task_id}/</code>.</p>"},{"location":"api/#put-vs-patch","title":"PUT vs PATCH","text":"<p>In RESTful API design, PUT and PATCH are both used to update a resource, but they are used in slightly different ways:</p> <p>PUT is used to update a resource with a complete new version. It's idempotent, meaning that making the same PUT request multiple times will have the same effect as making it once. If you PUT a resource and then PUT it again with the same data, the second request will have no effect.</p> <p>PATCH, on the other hand, is used to update a resource with a partial update. It's not idempotent by nature, meaning that making the same PATCH request multiple times may have different effects. For example, if you PATCH a resource to increment a counter, making the same PATCH request again will increment the counter again.</p> <p>In RESTful API design, a successful <code>PATCH</code> request typically returns a <code>200 OK</code> status code along with the updated resource. This allows the client to see the changes that were made, which might be different from the changes that were requested if some of the changes couldn't be applied.</p> <p>However, if the <code>PATCH</code> request doesn't return the updated resource, it should return a <code>204 No Content</code> status code to indicate that the request was successful but there's no representation to return (i.e., no body).</p>"},{"location":"api/#good-api-design","title":"Good API Design","text":""},{"location":"api/authentication_strategies/","title":"API Authentication Strategies","text":"<p>This document outlines the different authentication methods for various types of API access.</p>"},{"location":"api/authentication_strategies/#authentication-hierarchy","title":"Authentication Hierarchy","text":""},{"location":"api/authentication_strategies/#1-internal-service-authentication","title":"1. Internal Service Authentication","text":"<p>Who: Your Flask app, background jobs, internal services Purpose: Trusted components of your application ecosystem Implementation: Shared service key</p> <pre><code>from ichrisbirch.api.client import internal_service_client\n\n# Your Flask app calling your API\nclient = internal_service_client(\"flask-frontend\")\nusers = client.resource('users', UserModel)\nuser = users.list(username=\"john_doe\")[0]  # Check username for login\n</code></pre>"},{"location":"api/authentication_strategies/#2-developer-api-keys","title":"2. Developer API Keys","text":"<p>Who: External developers building their own frontends Purpose: Third-party application access with controlled permissions Implementation: Individual API keys with scoping</p> <pre><code># Custom provider for external developers\nclass DeveloperAPIKeyProvider(CredentialProvider):\n    def __init__(self, api_key: str, app_name: str = \"external-app\"):\n        self.api_key = api_key\n        self.app_name = app_name\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\n            \"X-API-Key\": self.api_key,\n            \"X-App-Name\": self.app_name\n        }\n\n    def is_available(self) -&gt; bool:\n        return bool(self.api_key)\n\n# External developer usage\nclient = APIClient(credential_provider=DeveloperAPIKeyProvider(\"dev_abc123\"))\n</code></pre>"},{"location":"api/authentication_strategies/#3-user-token-authentication","title":"3. User Token Authentication","text":"<p>Who: End users of external applications Purpose: User-scoped access through external frontends Implementation: OAuth 2.0 or JWT tokens</p> <pre><code># For user-specific operations\nclass UserBearerTokenProvider(CredentialProvider):\n    def __init__(self, user_token: str):\n        self.user_token = user_token\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\"Authorization\": f\"Bearer {self.user_token}\"}\n\n    def is_available(self) -&gt; bool:\n        return bool(self.user_token)\n\n# External app on behalf of user\nclient = APIClient(credential_provider=UserBearerTokenProvider(\"user_jwt_token\"))\n</code></pre>"},{"location":"api/authentication_strategies/#api-access-scenarios","title":"API Access Scenarios","text":""},{"location":"api/authentication_strategies/#scenario-1-your-flask-app-current","title":"Scenario 1: Your Flask App (Current)","text":"<pre><code>@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form['username']\n    password = request.form['password']\n\n    # Check credentials via API\n    with internal_service_client(\"flask-frontend\") as client:\n        users = client.resource('users', UserModel)\n\n        # Find user by username\n        user_list = users.list(username=username)\n        if not user_list:\n            return \"Invalid username\", 401\n\n        user = user_list[0]\n\n        # Verify password via API action\n        auth_result = users.action('verify_password', {\n            'user_id': user.id,\n            'password': password\n        })\n\n        if auth_result['valid']:\n            session['user_id'] = user.id\n            return redirect('/dashboard')\n        else:\n            return \"Invalid password\", 401\n</code></pre>"},{"location":"api/authentication_strategies/#scenario-2-external-developer-building-mobile-app","title":"Scenario 2: External Developer Building Mobile App","text":"<pre><code># Mobile app developer gets API key: \"dev_mobile_app_xyz789\"\n# They build a React Native app\n\n// In their mobile app\nconst apiKey = \"dev_mobile_app_xyz789\";\n\n// Login endpoint for their users\nasync function loginUser(username, password) {\n    const response = await fetch('/api/auth/login', {\n        method: 'POST',\n        headers: {\n            'X-API-Key': apiKey,\n            'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ username, password })\n    });\n\n    if (response.ok) {\n        const { user_token } = await response.json();\n        // Store user token for subsequent requests\n        return user_token;\n    }\n}\n\n// Accessing user data\nasync function getUserTasks(userToken) {\n    const response = await fetch('/api/tasks', {\n        headers: {\n            'Authorization': `Bearer ${userToken}`,\n            'X-API-Key': apiKey\n        }\n    });\n    return response.json();\n}\n</code></pre>"},{"location":"api/authentication_strategies/#scenario-3-external-web-app-developer","title":"Scenario 3: External Web App Developer","text":"<pre><code># Python web developer gets API key: \"dev_web_portal_abc456\"\n# They build a Django app that integrates with your API\n\nfrom your_api_client import APIClient, DeveloperAPIKeyProvider, UserBearerTokenProvider\n\nclass ExternalTaskService:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def authenticate_user(self, username: str, password: str) -&gt; Optional[str]:\n        \"\"\"Authenticate user and return JWT token\"\"\"\n        provider = DeveloperAPIKeyProvider(self.api_key, \"external-web-app\")\n\n        with APIClient(credential_provider=provider) as client:\n            auth = client.resource('auth', AuthModel)\n            result = auth.action('login', {\n                'username': username,\n                'password': password\n            })\n\n            return result.get('token') if result['success'] else None\n\n    def get_user_tasks(self, user_token: str) -&gt; List[dict]:\n        \"\"\"Get tasks for authenticated user\"\"\"\n        user_provider = UserBearerTokenProvider(user_token)\n\n        with APIClient(credential_provider=user_provider) as client:\n            tasks = client.resource('tasks', TaskModel)\n            return tasks.list()\n</code></pre>"},{"location":"api/authentication_strategies/#backend-api-implementation","title":"Backend API Implementation","text":"<p>You'd need to add authentication middleware to your FastAPI backend:</p> <pre><code># In your FastAPI app\nfrom fastapi import HTTPException, Depends, Header\nfrom typing import Optional\n\nasync def get_api_key(x_api_key: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract API key from headers\"\"\"\n    return x_api_key\n\nasync def get_user_token(authorization: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract user token from Authorization header\"\"\"\n    if authorization and authorization.startswith(\"Bearer \"):\n        return authorization[7:]  # Remove \"Bearer \" prefix\n    return None\n\nasync def get_internal_service(x_internal_service: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract internal service auth\"\"\"\n    return x_internal_service\n\n@app.middleware(\"http\")\nasync def auth_middleware(request: Request, call_next):\n    \"\"\"Authentication middleware to validate requests\"\"\"\n\n    # Internal service requests\n    if request.headers.get(\"X-Internal-Service\"):\n        # Validate service key\n        service_key = request.headers.get(\"Authorization\", \"\").replace(\"Service \", \"\")\n        if not validate_service_key(service_key):\n            raise HTTPException(401, \"Invalid service key\")\n\n    # Developer API key requests\n    elif request.headers.get(\"X-API-Key\"):\n        api_key = request.headers.get(\"X-API-Key\")\n        if not validate_developer_key(api_key):\n            raise HTTPException(401, \"Invalid API key\")\n\n    # User token requests\n    elif request.headers.get(\"Authorization\", \"\").startswith(\"Bearer \"):\n        token = request.headers.get(\"Authorization\")[7:]\n        if not validate_user_token(token):\n            raise HTTPException(401, \"Invalid user token\")\n\n    else:\n        raise HTTPException(401, \"Authentication required\")\n\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"api/authentication_strategies/#developer-onboarding-process","title":"Developer Onboarding Process","text":""},{"location":"api/authentication_strategies/#for-external-developers","title":"For External Developers","text":"<ol> <li>Registration: Developer signs up on your developer portal</li> <li>API Key Generation: System generates unique API key</li> <li>Documentation: Provide API docs and client library</li> <li>Rate Limiting: Apply limits based on their plan</li> <li>Monitoring: Track usage and provide analytics</li> </ol> <pre><code># Developer management endpoints\n@router.post(\"/developer/register\")\nasync def register_developer(developer_info: DeveloperRegistration):\n    \"\"\"Register new developer and generate API key\"\"\"\n    api_key = generate_api_key()\n\n    # Store in database\n    dev_record = DeveloperAccount(\n        name=developer_info.name,\n        email=developer_info.email,\n        api_key=api_key,\n        rate_limit=1000,  # requests per hour\n        scopes=[\"read:tasks\", \"read:users\"]  # limited permissions\n    )\n\n    return {\"api_key\": api_key, \"documentation\": \"/docs/api\"}\n\n@router.get(\"/developer/usage\")\nasync def get_usage_stats(api_key: str = Depends(get_api_key)):\n    \"\"\"Get API usage statistics for developer\"\"\"\n    return get_developer_usage(api_key)\n</code></pre>"},{"location":"api/authentication_strategies/#summary","title":"Summary","text":"<ul> <li>Internal Service Auth: Your Flask app and internal services (current implementation)</li> <li>Developer API Keys: External developers building applications</li> <li>User Tokens: End users of those external applications</li> </ul> <p>The key insight is that you have three layers of authentication:</p> <ol> <li>Service-level (your internal components)</li> <li>Application-level (external developers)  </li> <li>User-level (end users through external apps)</li> </ol> <p>Would you like me to help implement any of these authentication providers or show you how to set up the developer registration system?</p>"},{"location":"api/client/","title":"API Client Architecture","text":"<p>The ichrisbirch API Client provides a modern, session-based architecture for interacting with the FastAPI backend. This architecture follows industry patterns from libraries like boto3 and Stripe, providing flexible authentication, resource management, and request handling.</p>"},{"location":"api/client/#overview","title":"Overview","text":"<p>The API client consists of four main components:</p> <ol> <li>Credential Providers (<code>auth.py</code>) - Pluggable authentication strategies</li> <li>Session Management (<code>session.py</code>) - Persistent configuration and HTTP client management</li> <li>Resource Clients (<code>resource.py</code>) - Generic CRUD operations for API resources</li> <li>Main API Client (<code>api.py</code>) - High-level interface with factory methods</li> </ol>"},{"location":"api/client/#quick-start","title":"Quick Start","text":""},{"location":"api/client/#basic-usage","title":"Basic Usage","text":"<pre><code>from ichrisbirch.api.client import APIClient\n\n# Context-aware client (automatically detects Flask context)\nclient = APIClient()\n\n# Get a resource client for tasks\ntasks = client.resource('tasks', TaskModel)\n\n# CRUD operations\ntask = tasks.get(123)\nall_tasks = tasks.list()\nnew_task = tasks.create({'title': 'New Task', 'description': 'Task description'})\nupdated_task = tasks.update(123, {'status': 'completed'})\ntasks.delete(123)\n\n# Custom actions\nresult = tasks.action('bulk_complete', {'task_ids': [1, 2, 3]})\n\n# Direct API requests\nresponse = client.request('GET', '/custom/endpoint')\n</code></pre>"},{"location":"api/client/#authentication-patterns","title":"Authentication Patterns","text":"<pre><code>from ichrisbirch.api.client import (\n    internal_service_client,\n    user_client,\n    flask_session_client,\n    default_client\n)\n\n# Internal service authentication\nclient = internal_service_client('flask-frontend')\n\n# User authentication\nclient = user_client('user123')\n\n# Flask session authentication\nclient = flask_session_client()\n\n# Default context-aware authentication\nclient = default_client()\n</code></pre>"},{"location":"api/client/#architecture-principles","title":"Architecture Principles","text":""},{"location":"api/client/#session-based-design","title":"Session-Based Design","text":"<p>Following the boto3 pattern, the client uses sessions to manage:</p> <ul> <li>HTTP client lifecycle</li> <li>Authentication state</li> <li>Base URL and default headers</li> <li>Request/response handling</li> </ul>"},{"location":"api/client/#pluggable-authentication","title":"Pluggable Authentication","text":"<p>Credential providers allow different authentication strategies:</p> <ul> <li>InternalServiceProvider: Service-to-service authentication</li> <li>UserTokenProvider: User-based token authentication  </li> <li>FlaskSessionProvider: Flask session-based authentication</li> </ul>"},{"location":"api/client/#generic-resource-pattern","title":"Generic Resource Pattern","text":"<p>Instead of specific factory methods for each resource type, the client uses a generic <code>resource()</code> method that works with any Pydantic model:</p> <pre><code># Generic pattern (preferred)\ntasks = client.resource('tasks', TaskModel)\nusers = client.resource('users', UserModel)\n\n# Avoids specific factories like:\n# tasks = client.tasks()  # Not implemented\n# users = client.users()  # Not implemented\n</code></pre>"},{"location":"api/client/#context-aware-defaults","title":"Context-Aware Defaults","text":"<p>The client automatically detects context and chooses appropriate authentication:</p> <ul> <li>Inside Flask request context: Uses Flask session</li> <li>Outside Flask context: Uses internal service authentication</li> <li>No defensive defaults - fails fast on misconfiguration</li> </ul>"},{"location":"api/client/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Credential Providers - Authentication strategies and implementation</li> <li>Session Management - Session lifecycle and configuration</li> <li>Resource Clients - CRUD operations and custom actions</li> <li>Migration Guide - Migrating from QueryAPI to the new client</li> <li>Usage Examples - Common patterns and use cases</li> </ul>"},{"location":"api/client/#design-benefits","title":"Design Benefits","text":"<ol> <li>Industry Standard Pattern: Follows established patterns from boto3, Stripe, etc.</li> <li>Flexible Authentication: Pluggable providers support various auth strategies</li> <li>Type Safety: Full Pydantic model integration with proper typing</li> <li>Resource Agnostic: Generic client works with any API endpoint</li> <li>Context Awareness: Automatically adapts to Flask vs non-Flask environments</li> <li>Session Management: Proper HTTP client lifecycle management</li> <li>Extensible: Easy to add new credential providers or custom endpoints</li> </ol>"},{"location":"api/client/#future-considerations","title":"Future Considerations","text":"<ul> <li>Async Support: Could add async versions of all methods</li> <li>Caching: Could add response caching at the session level</li> <li>Retry Logic: Could add automatic retry with exponential backoff</li> <li>Rate Limiting: Could add client-side rate limiting</li> <li>Response Streaming: Could add support for streaming responses</li> </ul>"},{"location":"api/client/auth/","title":"Credential Providers","text":"<p>Credential providers are pluggable authentication strategies that supply credentials for API requests. They follow a simple interface and can be easily extended for different authentication scenarios.</p>"},{"location":"api/client/auth/#credentialprovider-interface","title":"CredentialProvider Interface","text":"<p>All credential providers implement the abstract <code>CredentialProvider</code> class:</p> <pre><code>class CredentialProvider(ABC):\n    \"\"\"Abstract base for credential providers.\"\"\"\n\n    @abstractmethod\n    def get_credentials(self) -&gt; Dict[str, str]:\n        \"\"\"Return headers to be added to requests.\"\"\"\n        pass\n\n    @abstractmethod\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if credentials are available.\"\"\"\n        pass\n</code></pre>"},{"location":"api/client/auth/#built-in-providers","title":"Built-in Providers","text":""},{"location":"api/client/auth/#internalserviceprovider","title":"InternalServiceProvider","text":"<p>Used for service-to-service authentication within the ichrisbirch ecosystem.</p> <pre><code>from ichrisbirch.api.client.auth import InternalServiceProvider\n\nprovider = InternalServiceProvider(service_name=\"flask-frontend\")\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"Service flask-frontend &lt;service_key&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>Flask app calling FastAPI endpoints</li> <li>Background jobs accessing the API</li> <li>Internal microservice communication</li> </ul> <p>Configuration:</p> <ul> <li>Requires <code>AUTH_INTERNAL_SERVICE_KEY</code> environment variable</li> <li>Service name identifies the calling service</li> </ul>"},{"location":"api/client/auth/#usertokenprovider","title":"UserTokenProvider","text":"<p>Used for user-based authentication with JWT tokens or similar.</p> <pre><code>from ichrisbirch.api.client.auth import UserTokenProvider\n\nprovider = UserTokenProvider(user_id=\"user123\", app_id=\"web-app\")\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"Bearer &lt;user_token&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>API calls on behalf of a specific user</li> <li>Background tasks with user context</li> <li>Service calls that need user permissions</li> </ul> <p>Implementation Notes:</p> <ul> <li>Token retrieval should be implemented based on your token storage</li> <li>Could integrate with JWT libraries, database lookups, or external auth services</li> <li>May include token refresh logic</li> </ul>"},{"location":"api/client/auth/#flasksessionprovider","title":"FlaskSessionProvider","text":"<p>Uses Flask session data for authentication in web request contexts.</p> <pre><code>from ichrisbirch.api.client.auth import FlaskSessionProvider\n\nprovider = FlaskSessionProvider()\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"User &lt;user_id&gt;\", \"X-App-ID\": \"&lt;app_id&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>Web requests where user is already authenticated</li> <li>Form submissions and AJAX calls</li> <li>Any Flask route that needs to call the API</li> </ul> <p>Behavior:</p> <ul> <li>Only available within Flask request context</li> <li>Extracts user_id and app_id from session</li> <li>Returns empty dict if session data is missing</li> </ul>"},{"location":"api/client/auth/#default-provider-selection","title":"Default Provider Selection","text":"<p>The <code>APISession</code> automatically selects an appropriate provider when none is explicitly provided:</p> <pre><code>def _default_provider(self) -&gt; CredentialProvider:\n    \"\"\"Select appropriate provider based on context.\"\"\"\n    if has_request_context():\n        return FlaskSessionProvider()\n    else:\n        return InternalServiceProvider()\n</code></pre> <p>This provides context-aware authentication:</p> <ul> <li>In Flask requests: Uses session-based auth</li> <li>Outside Flask: Uses internal service auth</li> </ul>"},{"location":"api/client/auth/#custom-providers","title":"Custom Providers","text":"<p>You can create custom credential providers for specific authentication needs:</p> <pre><code>class APIKeyProvider(CredentialProvider):\n    \"\"\"API key based authentication.\"\"\"\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\"X-API-Key\": self.api_key}\n\n    def is_available(self) -&gt; bool:\n        return bool(self.api_key)\n\n# Usage\nprovider = APIKeyProvider(\"your-api-key\")\nclient = APIClient(credential_provider=provider)\n</code></pre>"},{"location":"api/client/auth/#provider-selection-guide","title":"Provider Selection Guide","text":"Context Recommended Provider Use Case Flask request with user session <code>FlaskSessionProvider</code> Web app user actions Flask request without session <code>InternalServiceProvider</code> Internal API calls Background job with user context <code>UserTokenProvider</code> User-specific tasks Background job system task <code>InternalServiceProvider</code> System maintenance External script/tool <code>APIKeyProvider</code> (custom) CLI tools, scripts Testing <code>InternalServiceProvider</code> Test automation"},{"location":"api/client/auth/#error-handling","title":"Error Handling","text":"<p>Credential providers follow a fail-fast approach:</p> <ul> <li><code>is_available()</code> returns <code>False</code> when credentials cannot be obtained</li> <li><code>get_credentials()</code> may raise exceptions for configuration errors</li> <li>No defensive defaults - missing configuration should fail clearly</li> </ul> <p>This ensures authentication problems are detected early rather than silently failing with unclear errors.</p>"},{"location":"api/client/auth/#security-considerations","title":"Security Considerations","text":"<ol> <li>Service Keys: Store in environment variables, not code</li> <li>User Tokens: Implement secure token storage and refresh logic</li> <li>Session Security: Ensure Flask sessions are properly secured</li> <li>Credential Rotation: Design providers to support key/token rotation</li> <li>Logging: Avoid logging credential values in plaintext</li> </ol>"},{"location":"api/client/auth/#testing","title":"Testing","text":"<p>Mock credential providers for testing:</p> <pre><code>class MockCredentialProvider(CredentialProvider):\n    def __init__(self, credentials: Dict[str, str]):\n        self._credentials = credentials\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return self._credentials\n\n    def is_available(self) -&gt; bool:\n        return True\n\n# Test usage\nmock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\nclient = APIClient(credential_provider=mock_provider)\n</code></pre>"},{"location":"api/client/examples/","title":"API Client Usage Examples","text":"<p>This document provides practical examples of using the new API client for common scenarios in the ichrisbirch application.</p>"},{"location":"api/client/examples/#basic-setup","title":"Basic Setup","text":"<pre><code>from ichrisbirch.api.client import APIClient, internal_service_client, user_client\nfrom ichrisbirch.models import TaskModel, UserModel, ProjectModel\n</code></pre>"},{"location":"api/client/examples/#flask-web-application-examples","title":"Flask Web Application Examples","text":""},{"location":"api/client/examples/#user-dashboard","title":"User Dashboard","text":"<pre><code>@app.route('/dashboard')\n@login_required\ndef dashboard():\n    \"\"\"User dashboard showing their tasks and projects.\"\"\"\n    # Uses Flask session automatically\n    with APIClient() as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        projects_client = client.resource('projects', ProjectModel)\n\n        # Get user's active tasks\n        active_tasks = tasks_client.list(\n            assigned_to=session['user_id'],\n            status='active',\n            limit=10\n        )\n\n        # Get user's projects\n        projects = projects_client.list(\n            owner_id=session['user_id'],\n            include_task_counts=True\n        )\n\n        return render_template('dashboard.html',\n                             tasks=active_tasks,\n                             projects=projects)\n</code></pre>"},{"location":"api/client/examples/#task-management","title":"Task Management","text":"<pre><code>@app.route('/tasks/&lt;int:task_id&gt;/complete', methods=['POST'])\n@login_required\ndef complete_task(task_id):\n    \"\"\"Mark a task as completed.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        try:\n            # Use custom action for completion\n            result = tasks.action('complete', {\n                'task_id': task_id,\n                'completion_note': request.form.get('note', '')\n            })\n\n            flash('Task completed successfully', 'success')\n            return redirect(url_for('dashboard'))\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                flash('Task not found', 'error')\n            elif e.response.status_code == 403:\n                flash('Not authorized to complete this task', 'error')\n            else:\n                flash('Error completing task', 'error')\n            return redirect(url_for('dashboard'))\n</code></pre>"},{"location":"api/client/examples/#crud-operations","title":"CRUD Operations","text":"<pre><code>@app.route('/projects', methods=['GET', 'POST'])\n@login_required\ndef projects():\n    \"\"\"List projects or create new project.\"\"\"\n    with APIClient() as client:\n        projects_client = client.resource('projects', ProjectModel)\n\n        if request.method == 'POST':\n            # Create new project\n            project_data = {\n                'name': request.form['name'],\n                'description': request.form['description'],\n                'owner_id': session['user_id']\n            }\n\n            try:\n                new_project = projects_client.create(project_data)\n                flash(f'Project \"{new_project.name}\" created', 'success')\n                return redirect(url_for('projects'))\n\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == 422:\n                    errors = e.response.json()\n                    flash(f'Validation errors: {errors}', 'error')\n                else:\n                    flash('Error creating project', 'error')\n\n        # List projects\n        user_projects = projects_client.list(owner_id=session['user_id'])\n        return render_template('projects.html', projects=user_projects)\n</code></pre>"},{"location":"api/client/examples/#background-job-examples","title":"Background Job Examples","text":""},{"location":"api/client/examples/#data-processing-job","title":"Data Processing Job","text":"<pre><code>def process_daily_reports():\n    \"\"\"Background job to generate daily reports.\"\"\"\n    # Use internal service authentication\n    with internal_service_client('report-processor') as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        reports_client = client.resource('reports', ReportModel)\n\n        # Get completed tasks from yesterday\n        yesterday = datetime.now() - timedelta(days=1)\n        completed_tasks = tasks_client.list(\n            status='completed',\n            completed_after=yesterday.isoformat(),\n            limit=1000\n        )\n\n        # Generate report\n        report_data = {\n            'date': yesterday.date().isoformat(),\n            'total_completed': len(completed_tasks),\n            'completion_rate': calculate_completion_rate(completed_tasks),\n            'top_performers': get_top_performers(completed_tasks)\n        }\n\n        # Create report\n        report = reports_client.create(report_data)\n        logger.info(f\"Daily report created: {report.id}\")\n\n        # Send notifications\n        notify_managers(report)\n</code></pre>"},{"location":"api/client/examples/#bulk-operations","title":"Bulk Operations","text":"<pre><code>def archive_old_tasks():\n    \"\"\"Archive tasks older than 90 days.\"\"\"\n    with internal_service_client('maintenance') as client:\n        tasks_client = client.resource('tasks', TaskModel)\n\n        # Find old completed tasks\n        cutoff_date = datetime.now() - timedelta(days=90)\n        old_tasks = tasks_client.list(\n            status='completed',\n            completed_before=cutoff_date.isoformat(),\n            limit=500  # Process in batches\n        )\n\n        if old_tasks:\n            # Use bulk action\n            task_ids = [task.id for task in old_tasks]\n            result = tasks_client.action('bulk_archive', {\n                'task_ids': task_ids,\n                'archive_reason': 'automatic_cleanup'\n            })\n\n            logger.info(f\"Archived {result['archived_count']} old tasks\")\n</code></pre>"},{"location":"api/client/examples/#service-integration-examples","title":"Service Integration Examples","text":""},{"location":"api/client/examples/#external-api-integration","title":"External API Integration","text":"<pre><code>class ExternalSyncService:\n    \"\"\"Service to sync data with external API.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('external-sync')\n        self.tasks = self.client.resource('tasks', TaskModel)\n        self.users = self.client.resource('users', UserModel)\n\n    def sync_user_tasks(self, external_user_id: str):\n        \"\"\"Sync tasks for a user from external system.\"\"\"\n        try:\n            # Get external tasks\n            external_tasks = self.get_external_tasks(external_user_id)\n\n            # Find corresponding internal user\n            user = self.users.list(external_id=external_user_id)[0]\n\n            for ext_task in external_tasks:\n                # Check if task already exists\n                existing = self.tasks.list(external_id=ext_task['id'])\n\n                task_data = {\n                    'title': ext_task['title'],\n                    'description': ext_task['description'],\n                    'assigned_to': user.id,\n                    'external_id': ext_task['id'],\n                    'external_updated_at': ext_task['updated_at']\n                }\n\n                if existing:\n                    # Update existing task\n                    self.tasks.update(existing[0].id, task_data)\n                else:\n                    # Create new task\n                    self.tasks.create(task_data)\n\n        except Exception as e:\n            logger.error(f\"Error syncing tasks for user {external_user_id}: {e}\")\n            raise\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#notification-service","title":"Notification Service","text":"<pre><code>class NotificationService:\n    \"\"\"Service for sending notifications based on API events.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('notification-service')\n        self.notifications = self.client.resource('notifications', NotificationModel)\n        self.users = self.client.resource('users', UserModel)\n\n    def notify_task_assignment(self, task_id: int, assigned_user_id: str):\n        \"\"\"Send notification when task is assigned.\"\"\"\n        try:\n            # Get task details\n            tasks = self.client.resource('tasks', TaskModel)\n            task = tasks.get(task_id)\n\n            if not task:\n                logger.warning(f\"Task {task_id} not found for notification\")\n                return\n\n            # Get user details\n            user = self.users.get(assigned_user_id)\n\n            # Create notification\n            notification_data = {\n                'user_id': assigned_user_id,\n                'type': 'task_assignment',\n                'title': 'New Task Assigned',\n                'message': f'You have been assigned task: {task.title}',\n                'related_object_type': 'task',\n                'related_object_id': task_id,\n                'priority': task.priority\n            }\n\n            notification = self.notifications.create(notification_data)\n\n            # Send via email/SMS if needed\n            if user.notification_preferences.get('email_enabled'):\n                self.send_email_notification(user, notification)\n\n        except Exception as e:\n            logger.error(f\"Error sending task assignment notification: {e}\")\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#testing-examples","title":"Testing Examples","text":""},{"location":"api/client/examples/#unit-testing-with-mocks","title":"Unit Testing with Mocks","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom ichrisbirch.api.client import APIClient\nfrom ichrisbirch.api.client.auth import MockCredentialProvider\n\nclass TestTaskService:\n\n    @pytest.fixture\n    def mock_api_client(self):\n        \"\"\"Mock API client for testing.\"\"\"\n        provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\n        return APIClient(\n            base_url=\"http://test-api:8000\",\n            credential_provider=provider\n        )\n\n    def test_get_user_tasks(self, mock_api_client):\n        \"\"\"Test getting user tasks.\"\"\"\n        # Mock the HTTP response\n        with patch.object(mock_api_client.session.client, 'request') as mock_request:\n            mock_response = Mock()\n            mock_response.json.return_value = [\n                {'id': 1, 'title': 'Test Task', 'assigned_to': 'user123'},\n                {'id': 2, 'title': 'Another Task', 'assigned_to': 'user123'}\n            ]\n            mock_request.return_value = mock_response\n\n            # Test the service\n            tasks = mock_api_client.resource('tasks', TaskModel)\n            user_tasks = tasks.list(assigned_to='user123')\n\n            # Verify the request\n            mock_request.assert_called_once()\n            args, kwargs = mock_request.call_args\n            assert kwargs['method'] == 'GET'\n            assert 'tasks' in kwargs['url']\n            assert kwargs['params'] == {'assigned_to': 'user123'}\n</code></pre>"},{"location":"api/client/examples/#integration-testing","title":"Integration Testing","text":"<pre><code>@pytest.mark.integration\nclass TestAPIClientIntegration:\n    \"\"\"Integration tests with actual API.\"\"\"\n\n    @pytest.fixture\n    def api_client(self):\n        \"\"\"Real API client for integration tests.\"\"\"\n        return APIClient(base_url=os.environ['TEST_API_URL'])\n\n    def test_crud_operations(self, api_client):\n        \"\"\"Test full CRUD cycle.\"\"\"\n        with api_client:\n            tasks = api_client.resource('tasks', TaskModel)\n\n            # Create\n            task_data = {\n                'title': 'Integration Test Task',\n                'description': 'Created by integration test',\n                'priority': 'low'\n            }\n            created_task = tasks.create(task_data)\n            assert created_task.id is not None\n            assert created_task.title == task_data['title']\n\n            # Read\n            fetched_task = tasks.get(created_task.id)\n            assert fetched_task.title == created_task.title\n\n            # Update\n            updated_data = {'status': 'in_progress'}\n            updated_task = tasks.update(created_task.id, updated_data)\n            assert updated_task.status == 'in_progress'\n\n            # Delete\n            tasks.delete(created_task.id)\n\n            # Verify deletion\n            deleted_task = tasks.get(created_task.id)\n            assert deleted_task is None\n</code></pre>"},{"location":"api/client/examples/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/client/examples/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>def robust_task_processing(task_id: int):\n    \"\"\"Process task with comprehensive error handling.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        try:\n            # Get task\n            task = tasks.get(task_id)\n            if not task:\n                logger.warning(f\"Task {task_id} not found\")\n                return {'success': False, 'error': 'Task not found'}\n\n            # Process task\n            result = tasks.action('process', {'task_id': task_id})\n\n            return {'success': True, 'result': result}\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                error_msg = f\"Task {task_id} not found\"\n            elif e.response.status_code == 422:\n                validation_errors = e.response.json()\n                error_msg = f\"Validation errors: {validation_errors}\"\n            elif e.response.status_code == 403:\n                error_msg = \"Not authorized to process this task\"\n            else:\n                error_msg = f\"HTTP error {e.response.status_code}: {e.response.text}\"\n\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except httpx.ConnectError:\n            error_msg = \"Could not connect to API server\"\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except httpx.TimeoutException:\n            error_msg = \"Request timed out\"\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except Exception as e:\n            error_msg = f\"Unexpected error: {str(e)}\"\n            logger.exception(error_msg)\n            return {'success': False, 'error': error_msg}\n</code></pre>"},{"location":"api/client/examples/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\nfrom typing import Callable, Any\n\ndef with_retry(func: Callable, max_attempts: int = 3, delay: float = 1.0) -&gt; Any:\n    \"\"\"Execute function with retry logic.\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            return func()\n        except (httpx.ConnectError, httpx.TimeoutException) as e:\n            if attempt == max_attempts - 1:\n                raise e\n\n            logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay}s...\")\n            time.sleep(delay)\n            delay *= 2  # Exponential backoff\n\ndef get_tasks_with_retry():\n    \"\"\"Get tasks with automatic retry.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        return with_retry(lambda: tasks.list(status='active'))\n</code></pre>"},{"location":"api/client/examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/client/examples/#connection-reuse","title":"Connection Reuse","text":"<pre><code>class TaskProcessor:\n    \"\"\"Long-running task processor that reuses connections.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('task-processor')\n        self.tasks = self.client.resource('tasks', TaskModel)\n        self.notifications = self.client.resource('notifications', NotificationModel)\n\n    def process_batch(self, batch_size: int = 50):\n        \"\"\"Process tasks in batches.\"\"\"\n        offset = 0\n\n        while True:\n            # Get batch of pending tasks\n            pending_tasks = self.tasks.list(\n                status='pending',\n                limit=batch_size,\n                offset=offset\n            )\n\n            if not pending_tasks:\n                break\n\n            # Process each task\n            for task in pending_tasks:\n                try:\n                    self.process_single_task(task)\n                except Exception as e:\n                    logger.error(f\"Error processing task {task.id}: {e}\")\n\n            offset += batch_size\n\n    def process_single_task(self, task: TaskModel):\n        \"\"\"Process a single task.\"\"\"\n        # Update status\n        self.tasks.update(task.id, {'status': 'processing'})\n\n        # Do processing work\n        result = self.do_work(task)\n\n        # Update with result\n        self.tasks.update(task.id, {\n            'status': 'completed',\n            'result': result\n        })\n\n        # Send notification\n        self.notifications.create({\n            'user_id': task.assigned_to,\n            'type': 'task_completed',\n            'message': f'Task \"{task.title}\" completed'\n        })\n\n    def close(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#pagination-handling","title":"Pagination Handling","text":"<pre><code>def get_all_tasks() -&gt; List[TaskModel]:\n    \"\"\"Get all tasks using pagination.\"\"\"\n    all_tasks = []\n\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        page_size = 100\n        offset = 0\n\n        while True:\n            batch = tasks.list(limit=page_size, offset=offset)\n            if not batch:\n                break\n\n            all_tasks.extend(batch)\n            offset += page_size\n\n            # Avoid infinite loops\n            if len(batch) &lt; page_size:\n                break\n\n    return all_tasks\n</code></pre> <p>These examples demonstrate practical usage patterns for the new API client in various scenarios throughout the ichrisbirch application.</p>"},{"location":"api/client/migration/","title":"Migration Guide: QueryAPI to API Client","text":"<p>\u2705 MIGRATION COMPLETE (2026-01-01): All 12 files have been migrated from <code>QueryAPI</code> to <code>LoggingAPIClient</code>. The <code>QueryAPI</code> class has been deleted. This document is preserved for historical reference.</p> <p>This guide helped migrate from the legacy <code>QueryAPI</code> class to the new session-based API client architecture.</p>"},{"location":"api/client/migration/#overview-of-changes","title":"Overview of Changes","text":"<p>The new API client provides:</p> <ul> <li>Session-based architecture following industry patterns (boto3, Stripe)</li> <li>Pluggable authentication with credential providers</li> <li>Generic resource pattern instead of specific methods</li> <li>Better type safety with full Pydantic integration</li> <li>Context-aware defaults for Flask vs non-Flask environments</li> </ul>"},{"location":"api/client/migration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"api/client/migration/#phase-1-coexistence","title":"Phase 1: Coexistence","text":"<p>Both systems can coexist during migration:</p> <pre><code># Old QueryAPI (still available)\nfrom ichrisbirch.app.query_api import QueryAPI\nquery_api = QueryAPI(use_internal_auth=True)\n\n# New API Client\nfrom ichrisbirch.api.client import APIClient\nclient = APIClient()\n</code></pre>"},{"location":"api/client/migration/#phase-2-gradual-migration","title":"Phase 2: Gradual Migration","text":"<p>Migrate one endpoint at a time:</p> <pre><code># Before: QueryAPI\ndef get_tasks():\n    query_api = QueryAPI()\n    return query_api.get_generic(\"tasks\")\n\n# After: API Client\ndef get_tasks():\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list()\n</code></pre>"},{"location":"api/client/migration/#phase-3-complete-migration","title":"Phase 3: Complete Migration","text":"<p>Remove QueryAPI usage and update all code to use the new client.</p>"},{"location":"api/client/migration/#common-migration-patterns","title":"Common Migration Patterns","text":""},{"location":"api/client/migration/#basic-get-requests","title":"Basic GET Requests","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\ntasks = query_api.get_generic(\"tasks\")\ntask = query_api.get_generic(\"tasks\", resource_id=123)\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    tasks_client = client.resource('tasks', TaskModel)\n    tasks = tasks_client.list()\n    task = tasks_client.get(123)\n</code></pre>"},{"location":"api/client/migration/#post-actions","title":"POST Actions","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\nresult = query_api.post_action(\"tasks/bulk_complete\", {\n    'task_ids': [1, 2, 3]\n})\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    result = tasks.action('bulk_complete', {\n        'task_ids': [1, 2, 3]\n    })\n</code></pre>"},{"location":"api/client/migration/#custom-endpoints","title":"Custom Endpoints","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\ndata = query_api.get_generic(\"custom/endpoint\")\nresult = query_api.post_action(\"custom/action\", {'data': 'value'})\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    # Direct requests for truly custom endpoints\n    data = client.request('GET', '/custom/endpoint')\n    result = client.request('POST', '/custom/action', json={'data': 'value'})\n\n    # Or if it belongs to a resource\n    resource = client.resource('custom', CustomModel)\n    data = resource.custom_endpoint('GET', '/endpoint')\n    result = resource.custom_endpoint('POST', '/action', {'data': 'value'})\n</code></pre>"},{"location":"api/client/migration/#authentication-patterns","title":"Authentication Patterns","text":"<p>Before (QueryAPI):</p> <pre><code># Internal service\nquery_api = QueryAPI(use_internal_auth=True)\n\n# User context (automatic)\nquery_api = QueryAPI()  # Uses Flask session\n</code></pre> <p>After (API Client):</p> <pre><code># Internal service\nfrom ichrisbirch.api.client import internal_service_client\nclient = internal_service_client()\n\n# User context (automatic)\nfrom ichrisbirch.api.client import default_client\nclient = default_client()  # Context-aware\n\n# Or explicit Flask session\nfrom ichrisbirch.api.client import flask_session_client\nclient = flask_session_client()\n</code></pre>"},{"location":"api/client/migration/#specific-queryapi-method-migrations","title":"Specific QueryAPI Method Migrations","text":""},{"location":"api/client/migration/#get_generic","title":"get_generic()","text":"<p>QueryAPI Pattern:</p> <pre><code>def get_generic(self, endpoint: str, resource_id: int = None, **params) -&gt; Union[List[Dict], Dict, None]:\n    # Internal implementation\n    return response_data\n</code></pre> <p>API Client Equivalent:</p> <pre><code># For resource operations\ntasks = client.resource('tasks', TaskModel)\nall_tasks = tasks.list(**params)  # GET /tasks\nsingle_task = tasks.get(resource_id, **params)  # GET /tasks/{id}\n\n# For custom endpoints\nresponse = client.request('GET', f'/{endpoint}', params=params)\n</code></pre>"},{"location":"api/client/migration/#post_action","title":"post_action()","text":"<p>QueryAPI Pattern:</p> <pre><code>def post_action(self, endpoint: str, data: Dict) -&gt; Dict:\n    # Internal implementation\n    return response_data\n</code></pre> <p>API Client Equivalent:</p> <pre><code># For resource actions\nresource = client.resource('tasks', TaskModel)\nresult = resource.action('action_name', data)\n\n# For custom endpoints\nresult = client.request('POST', f'/{endpoint}', json=data)\n</code></pre>"},{"location":"api/client/migration/#internal-authentication","title":"Internal Authentication","text":"<p>QueryAPI Pattern:</p> <pre><code>query_api = QueryAPI(use_internal_auth=True)\n</code></pre> <p>API Client Equivalent:</p> <pre><code>from ichrisbirch.api.client import internal_service_client\nclient = internal_service_client(\"flask-frontend\")\n</code></pre>"},{"location":"api/client/migration/#file-by-file-migration-examples","title":"File-by-File Migration Examples","text":""},{"location":"api/client/migration/#flask-route-migration","title":"Flask Route Migration","text":"<p>Before:</p> <pre><code>@app.route('/tasks')\ndef get_tasks():\n    query_api = QueryAPI()\n    tasks = query_api.get_generic(\"tasks\", status=\"active\")\n    return render_template('tasks.html', tasks=tasks)\n</code></pre> <p>After:</p> <pre><code>@app.route('/tasks')\ndef get_tasks():\n    with APIClient() as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        tasks = tasks_client.list(status=\"active\")\n        return render_template('tasks.html', tasks=tasks)\n</code></pre>"},{"location":"api/client/migration/#background-job-migration","title":"Background Job Migration","text":"<p>Before:</p> <pre><code>def process_pending_tasks():\n    query_api = QueryAPI(use_internal_auth=True)\n    pending = query_api.get_generic(\"tasks\", status=\"pending\")\n\n    for task in pending:\n        result = query_api.post_action(f\"tasks/{task['id']}/process\", {})\n        if result['success']:\n            query_api.post_action(f\"tasks/{task['id']}/complete\", {})\n</code></pre> <p>After:</p> <pre><code>def process_pending_tasks():\n    with internal_service_client(\"background-processor\") as client:\n        tasks = client.resource('tasks', TaskModel)\n        pending = tasks.list(status=\"pending\")\n\n        for task in pending:\n            result = tasks.action('process', task_id=task.id)\n            if result['success']:\n                tasks.action('complete', task_id=task.id)\n</code></pre>"},{"location":"api/client/migration/#service-class-migration","title":"Service Class Migration","text":"<p>Before:</p> <pre><code>class TaskService:\n    def __init__(self):\n        self.query_api = QueryAPI()\n\n    def get_user_tasks(self, user_id: str):\n        return self.query_api.get_generic(\"tasks\", assigned_to=user_id)\n\n    def complete_task(self, task_id: int):\n        return self.query_api.post_action(f\"tasks/{task_id}/complete\", {})\n</code></pre> <p>After:</p> <pre><code>class TaskService:\n    def __init__(self):\n        self.client = APIClient()\n        self.tasks = self.client.resource('tasks', TaskModel)\n\n    def get_user_tasks(self, user_id: str):\n        return self.tasks.list(assigned_to=user_id)\n\n    def complete_task(self, task_id: int):\n        return self.tasks.action('complete', task_id=task_id)\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/migration/#testing-migration","title":"Testing Migration","text":""},{"location":"api/client/migration/#mock-queryapi","title":"Mock QueryAPI","text":"<p>Before:</p> <pre><code>def test_get_tasks(monkeypatch):\n    def mock_get_generic(endpoint, **params):\n        return [{'id': 1, 'title': 'Test Task'}]\n\n    monkeypatch.setattr(QueryAPI, 'get_generic', mock_get_generic)\n    # Test code...\n</code></pre> <p>After:</p> <pre><code>def test_get_tasks():\n    mock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test\"})\n    with APIClient(credential_provider=mock_provider) as client:\n        # Use actual client with mock auth\n        pass\n\n# Or mock at HTTP level\n@responses.activate\ndef test_get_tasks():\n    responses.add(responses.GET,\n                 \"http://api.test/tasks\",\n                 json=[{'id': 1, 'title': 'Test Task'}])\n\n    with APIClient(base_url=\"http://api.test\") as client:\n        tasks = client.resource('tasks', TaskModel)\n        result = tasks.list()\n</code></pre>"},{"location":"api/client/migration/#breaking-changes","title":"Breaking Changes","text":""},{"location":"api/client/migration/#method-names","title":"Method Names","text":"QueryAPI API Client <code>get_generic(endpoint)</code> <code>resource(name, model).list()</code> <code>get_generic(endpoint, resource_id=123)</code> <code>resource(name, model).get(123)</code> <code>post_action(endpoint, data)</code> <code>resource(name, model).action(name, data)</code> Custom methods <code>request(method, endpoint, **kwargs)</code>"},{"location":"api/client/migration/#return-types","title":"Return Types","text":"<ul> <li>QueryAPI: Returns raw dictionaries and lists</li> <li>API Client: Returns Pydantic model instances</li> </ul>"},{"location":"api/client/migration/#authentication","title":"Authentication","text":"<ul> <li>QueryAPI: Boolean flag <code>use_internal_auth</code></li> <li>API Client: Pluggable credential providers</li> </ul>"},{"location":"api/client/migration/#configuration","title":"Configuration","text":"<ul> <li>QueryAPI: Hardcoded settings and Flask coupling</li> <li>API Client: Configurable sessions with explicit dependencies</li> </ul>"},{"location":"api/client/migration/#rollback-strategy","title":"Rollback Strategy","text":"<p>If issues arise during migration:</p> <ol> <li>Keep QueryAPI Available: Don't remove QueryAPI until migration is complete</li> <li>Incremental Rollback: Roll back one endpoint at a time if needed</li> <li>Feature Flags: Use feature flags to toggle between old and new implementations</li> </ol> <pre><code>def get_tasks():\n    if settings.use_new_api_client:\n        with APIClient() as client:\n            return client.resource('tasks', TaskModel).list()\n    else:\n        query_api = QueryAPI()\n        return query_api.get_generic(\"tasks\")\n</code></pre>"},{"location":"api/client/migration/#migration-checklist","title":"Migration Checklist","text":""},{"location":"api/client/migration/#before-migration","title":"Before Migration","text":"<ul> <li> Understand current QueryAPI usage patterns</li> <li> Identify all endpoints and authentication requirements</li> <li> Set up Pydantic models for all resources</li> <li> Plan migration order (start with low-risk endpoints)</li> </ul>"},{"location":"api/client/migration/#during-migration","title":"During Migration","text":"<ul> <li> Migrate one endpoint at a time</li> <li> Test each migration thoroughly</li> <li> Update all related tests</li> <li> Update documentation and examples</li> <li> Monitor for performance issues</li> </ul>"},{"location":"api/client/migration/#after-migration","title":"After Migration","text":"<ul> <li> Remove QueryAPI imports and usage</li> <li> Clean up old authentication code</li> <li> Update CI/CD pipelines</li> <li> Train team on new patterns</li> <li> Update development guidelines</li> </ul>"},{"location":"api/client/migration/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Session Management: Remember to close clients or use context managers</li> <li>Authentication Context: Understand when Flask session vs internal auth is used</li> <li>Type Safety: Use proper Pydantic models for type checking</li> <li>Error Handling: New client may raise different exceptions</li> <li>URL Building: New client uses existing <code>utils.url_builder()</code> patterns</li> </ol>"},{"location":"api/client/migration/#support","title":"Support","text":"<p>For migration assistance:</p> <ol> <li>Documentation: Reference the full API client documentation</li> <li>Examples: See <code>examples.md</code> for common patterns</li> <li>Testing: Use mock credential providers for unit tests</li> <li>Performance: Monitor response times and connection usage</li> </ol>"},{"location":"api/client/resources/","title":"Resource Clients","text":"<p>Resource clients provide a generic interface for CRUD operations on API endpoints. They work with any Pydantic model and follow consistent patterns for data access.</p>"},{"location":"api/client/resources/#overview","title":"Overview","text":"<p>The <code>ResourceClient</code> class provides standardized operations for any API resource:</p> <pre><code>from ichrisbirch.api.client import APIClient\nfrom ichrisbirch.models import TaskModel\n\n# Get a resource client\nclient = APIClient()\ntasks = client.resource('tasks', TaskModel)\n\n# All resources support the same operations\ntask = tasks.get(123)\nall_tasks = tasks.list()\nnew_task = tasks.create({'title': 'New Task'})\nupdated_task = tasks.update(123, {'status': 'completed'})\ntasks.delete(123)\n</code></pre>"},{"location":"api/client/resources/#crud-operations","title":"CRUD Operations","text":""},{"location":"api/client/resources/#get-single-resource","title":"Get Single Resource","text":"<p>Retrieve a specific resource by ID:</p> <pre><code># Get task with ID 123\ntask = tasks.get(123)\n# Returns: TaskModel instance or None\n\n# Get with query parameters\ntask = tasks.get(123, include_details=True)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>GET</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Returns: Pydantic model instance or <code>None</code></li> </ul>"},{"location":"api/client/resources/#list-resources","title":"List Resources","text":"<p>Retrieve multiple resources with optional filtering:</p> <pre><code># Get all tasks\nall_tasks = tasks.list()\n# Returns: List[TaskModel]\n\n# Get with filters\nactive_tasks = tasks.list(status='active', limit=10)\nurgent_tasks = tasks.list(priority='high', assigned_to='user123')\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>GET</code></li> <li>URL: <code>/{resource_name}</code></li> <li>Query params: All keyword arguments become query parameters</li> <li>Returns: <code>List[ModelType]</code></li> </ul>"},{"location":"api/client/resources/#create-resource","title":"Create Resource","text":"<p>Create a new resource:</p> <pre><code># Create from dictionary\nnew_task = tasks.create({\n    'title': 'Complete documentation',\n    'description': 'Write comprehensive API docs',\n    'priority': 'high'\n})\n# Returns: TaskModel instance\n\n# Create from Pydantic model\ntask_data = TaskModel(title='Another task', priority='low')\nnew_task = tasks.create(task_data.model_dump())\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>POST</code></li> <li>URL: <code>/{resource_name}</code></li> <li>Body: JSON data</li> <li>Returns: Created model instance</li> </ul>"},{"location":"api/client/resources/#update-resource","title":"Update Resource","text":"<p>Update an existing resource:</p> <pre><code># Partial update\nupdated_task = tasks.update(123, {'status': 'completed'})\n# Returns: TaskModel instance\n\n# Full update\nupdated_task = tasks.update(123, {\n    'title': 'Updated title',\n    'description': 'Updated description',\n    'status': 'in_progress'\n})\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>PUT</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Body: JSON data (partial or full)</li> <li>Returns: Updated model instance</li> </ul>"},{"location":"api/client/resources/#delete-resource","title":"Delete Resource","text":"<p>Remove a resource:</p> <pre><code># Delete by ID\ntasks.delete(123)\n# Returns: None (or raises exception on error)\n\n# Delete with confirmation\ntasks.delete(123, confirm=True)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>DELETE</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Returns: <code>None</code></li> </ul>"},{"location":"api/client/resources/#custom-actions","title":"Custom Actions","text":"<p>Resources support custom actions beyond CRUD operations:</p> <pre><code># POST action with data\nresult = tasks.action('bulk_complete', {\n    'task_ids': [1, 2, 3, 4],\n    'completion_note': 'Batch completed'\n})\n\n# GET action (no data)\nstats = tasks.action('statistics', method='GET')\n\n# Action with query parameters\nreport = tasks.action('export', method='GET', format='csv', date_range='last_week')\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>POST</code> (default) or specified method</li> <li>URL: <code>/{resource_name}/{action}</code></li> <li>Body: JSON data (for POST actions)</li> <li>Query params: Additional keyword arguments</li> <li>Returns: Action-specific response</li> </ul>"},{"location":"api/client/resources/#custom-endpoints","title":"Custom Endpoints","text":"<p>For endpoints that don't follow standard resource patterns:</p> <pre><code># Custom endpoint relative to resource\nresult = tasks.custom_endpoint('GET', '/special-report')\n\n# Custom endpoint with data\nresult = tasks.custom_endpoint('POST', '/batch-operations', {\n    'operation': 'archive',\n    'filter': {'status': 'completed'}\n})\n\n# Custom endpoint with query parameters\nresult = tasks.custom_endpoint('GET', '/export', format='json', limit=100)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: As specified</li> <li>URL: <code>/{resource_name}{endpoint}</code></li> <li>Body: JSON data (if provided)</li> <li>Query params: Additional keyword arguments</li> </ul>"},{"location":"api/client/resources/#type-safety","title":"Type Safety","text":"<p>Resource clients are fully typed with Pydantic models:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ichrisbirch.models import TaskModel, UserModel\n\n# Type hints work correctly\ntasks: ResourceClient[TaskModel] = client.resource('tasks', TaskModel)\nusers: ResourceClient[UserModel] = client.resource('users', UserModel)\n\n# Return types are properly inferred\ntask: TaskModel = tasks.get(123)  # Type: TaskModel | None\ntask_list: List[TaskModel] = tasks.list()  # Type: List[TaskModel]\n</code></pre>"},{"location":"api/client/resources/#error-handling","title":"Error Handling","text":"<p>Resource clients handle errors consistently:</p>"},{"location":"api/client/resources/#http-errors","title":"HTTP Errors","text":"<pre><code>try:\n    task = tasks.get(999)  # Non-existent ID\nexcept httpx.HTTPStatusError as e:\n    if e.response.status_code == 404:\n        print(\"Task not found\")\n    else:\n        print(f\"HTTP error: {e.response.status_code}\")\n</code></pre>"},{"location":"api/client/resources/#validation-errors","title":"Validation Errors","text":"<pre><code>try:\n    new_task = tasks.create({'invalid_field': 'value'})\nexcept httpx.HTTPStatusError as e:\n    if e.response.status_code == 422:\n        errors = e.response.json()\n        print(f\"Validation errors: {errors}\")\n</code></pre>"},{"location":"api/client/resources/#network-errors","title":"Network Errors","text":"<pre><code>try:\n    tasks.list()\nexcept httpx.ConnectError:\n    print(\"Could not connect to API\")\nexcept httpx.TimeoutException:\n    print(\"Request timed out\")\n</code></pre>"},{"location":"api/client/resources/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/client/resources/#resource-factory-pattern","title":"Resource Factory Pattern","text":"<p>Create resource clients as needed:</p> <pre><code>def get_user_tasks(user_id: str) -&gt; List[TaskModel]:\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list(assigned_to=user_id)\n</code></pre>"},{"location":"api/client/resources/#shared-resource-clients","title":"Shared Resource Clients","text":"<p>Reuse resource clients for multiple operations:</p> <pre><code>def process_tasks():\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        # Multiple operations with same client\n        pending = tasks.list(status='pending')\n        for task in pending:\n            result = tasks.action('process', {'task_id': task.id})\n            if result['success']:\n                tasks.update(task.id, {'status': 'completed'})\n</code></pre>"},{"location":"api/client/resources/#cross-resource-operations","title":"Cross-Resource Operations","text":"<p>Work with multiple resource types:</p> <pre><code>def assign_tasks_to_user(user_id: str, task_ids: List[int]):\n    with APIClient() as client:\n        users = client.resource('users', UserModel)\n        tasks = client.resource('tasks', TaskModel)\n\n        # Verify user exists\n        user = users.get(user_id)\n        if not user:\n            raise ValueError(f\"User {user_id} not found\")\n\n        # Assign tasks\n        for task_id in task_ids:\n            tasks.update(task_id, {'assigned_to': user_id})\n</code></pre>"},{"location":"api/client/resources/#model-integration","title":"Model Integration","text":"<p>Resource clients work seamlessly with Pydantic models:</p>"},{"location":"api/client/resources/#input-validation","title":"Input Validation","text":"<pre><code># Models validate input data\ntask_data = TaskModel(\n    title=\"New task\",\n    priority=\"high\",\n    due_date=datetime.now() + timedelta(days=7)\n)\n\n# Create using model data\nnew_task = tasks.create(task_data.model_dump())\n</code></pre>"},{"location":"api/client/resources/#output-parsing","title":"Output Parsing","text":"<pre><code># Responses are parsed into model instances\ntask = tasks.get(123)\n\n# Access typed attributes\nprint(task.title)  # String\nprint(task.created_at)  # datetime\nprint(task.priority)  # Enum value\n</code></pre>"},{"location":"api/client/resources/#relationship-loading","title":"Relationship Loading","text":"<pre><code># Load related data\ntask = tasks.get(123, include_assignee=True)\nprint(task.assignee.name)  # Related model data\n\n# Batch operations\ncompleted_tasks = tasks.list(status='completed', include_subtasks=True)\n</code></pre>"},{"location":"api/client/resources/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Session Reuse: Use the same APIClient for multiple operations</li> <li>Batch Operations: Use custom actions for bulk operations</li> <li>Filtering: Use query parameters to reduce data transfer</li> <li>Pagination: Handle large result sets appropriately</li> </ol> <pre><code># Efficient batch processing\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n\n    # Process in batches\n    offset = 0\n    batch_size = 100\n\n    while True:\n        batch = tasks.list(limit=batch_size, offset=offset)\n        if not batch:\n            break\n\n        # Process batch\n        for task in batch:\n            process_task(task)\n\n        offset += batch_size\n</code></pre>"},{"location":"api/client/resources/#best-practices","title":"Best Practices","text":"<ol> <li>Use Type Hints: Always specify the Pydantic model type</li> <li>Handle Errors: Catch and handle HTTP and network errors appropriately</li> <li>Session Management: Use context managers for proper cleanup</li> <li>Resource Naming: Use consistent naming that matches API endpoints</li> <li>Data Validation: Let Pydantic models handle input validation</li> <li>Custom Actions: Use actions for operations that don't fit CRUD patterns</li> </ol>"},{"location":"api/client/session/","title":"Session Management","text":"<p>The <code>APISession</code> class manages HTTP client lifecycle, authentication state, and request/response handling. It provides a persistent, configured connection to the API server.</p>"},{"location":"api/client/session/#core-concepts","title":"Core Concepts","text":""},{"location":"api/client/session/#session-lifecycle","title":"Session Lifecycle","text":"<p>Sessions manage the underlying HTTP client and provide consistent configuration across multiple requests:</p> <pre><code>from ichrisbirch.api.client import APIClient\n\n# Session is created automatically\nclient = APIClient()\n\n# Session is reused for all requests\ntasks = client.resource('tasks', TaskModel)\nusers = client.resource('users', UserModel)\n\n# Properly close when done\nclient.close()\n\n# Or use context manager\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    # Session automatically closed\n</code></pre>"},{"location":"api/client/session/#configuration","title":"Configuration","text":"<p>Sessions are configured with:</p> <ul> <li>Base URL: Default API server endpoint</li> <li>Credential Provider: Authentication strategy</li> <li>Default Headers: Headers added to all requests</li> <li>HTTP Client: Persistent connection pool</li> </ul> <pre><code># Custom configuration\nclient = APIClient(\n    base_url=\"https://api.example.com\",\n    credential_provider=custom_provider,\n    default_headers={\"User-Agent\": \"MyApp/1.0\"}\n)\n</code></pre>"},{"location":"api/client/session/#default-behavior","title":"Default Behavior","text":""},{"location":"api/client/session/#automatic-base-url-detection","title":"Automatic Base URL Detection","text":"<p>When no base URL is provided, the session uses <code>settings.api_url</code>:</p> <pre><code># Uses settings.api_url automatically\nclient = APIClient()\n\n# Explicit base URL override\nclient = APIClient(base_url=\"https://custom-api.com\")\n</code></pre>"},{"location":"api/client/session/#context-aware-authentication","title":"Context-Aware Authentication","text":"<p>Sessions automatically select appropriate authentication based on context:</p> <pre><code>def _default_provider(self) -&gt; CredentialProvider:\n    \"\"\"Select appropriate provider based on context.\"\"\"\n    if has_request_context():\n        return FlaskSessionProvider()\n    else:\n        return InternalServiceProvider()\n</code></pre> <p>In Flask Request Context:</p> <ul> <li>Uses <code>FlaskSessionProvider</code></li> <li>Extracts user credentials from Flask session</li> <li>Suitable for web requests</li> </ul> <p>Outside Flask Context:</p> <ul> <li>Uses <code>InternalServiceProvider</code></li> <li>Uses service-to-service authentication</li> <li>Suitable for background jobs, scripts</li> </ul>"},{"location":"api/client/session/#request-handling","title":"Request Handling","text":""},{"location":"api/client/session/#url-building","title":"URL Building","text":"<p>Sessions handle URL construction using the existing <code>utils.url_builder()</code> function:</p> <pre><code># Input: endpoint = \"/tasks/123\"\n# Base URL: \"https://api.example.com\"\n# Result: \"https://api.example.com/tasks/123/\"\n\nurl = utils.url_builder(self.base_url, endpoint)\n</code></pre> <p>This preserves the existing URL building patterns and ensures consistent path handling.</p>"},{"location":"api/client/session/#header-management","title":"Header Management","text":"<p>Sessions merge headers from multiple sources:</p> <ol> <li>Default headers (from session configuration)</li> <li>Authentication headers (from credential provider)</li> <li>Request-specific headers (from method call)</li> </ol> <pre><code># Session default headers\nsession = APISession(default_headers={\"User-Agent\": \"MyApp/1.0\"})\n\n# Credential provider headers\n# {\"Authorization\": \"Bearer token\"}\n\n# Request headers\nsession.request(\"GET\", \"/tasks\", headers={\"Accept\": \"application/json\"})\n\n# Final merged headers:\n# {\n#   \"User-Agent\": \"MyApp/1.0\",\n#   \"Authorization\": \"Bearer token\",\n#   \"Accept\": \"application/json\"\n# }\n</code></pre>"},{"location":"api/client/session/#http-client-management","title":"HTTP Client Management","text":"<p>Sessions use httpx for HTTP requests with these features:</p> <ul> <li>Connection pooling: Reuses connections for better performance</li> <li>Timeout handling: Configurable request timeouts</li> <li>Error handling: Proper HTTP error responses</li> <li>JSON serialization: Automatic JSON encoding/decoding</li> </ul> <pre><code># The session manages httpx.Client lifecycle\nclass APISession:\n    def __init__(self, ...):\n        self.client = httpx.Client()\n\n    def request(self, method: str, endpoint: str, **kwargs) -&gt; Any:\n        response = self.client.request(method, url, **merged_kwargs)\n        return response.json()\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/session/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/client/session/#long-running-sessions","title":"Long-Running Sessions","text":"<p>For applications that make many API calls, reuse the session:</p> <pre><code># Good: Reuse session\nclient = APIClient()\nfor item in items:\n    result = client.resource('tasks', TaskModel).create(item)\nclient.close()\n\n# Better: Use context manager\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    for item in items:\n        result = tasks.create(item)\n</code></pre>"},{"location":"api/client/session/#short-lived-sessions","title":"Short-Lived Sessions","text":"<p>For one-off requests, sessions can be created and discarded:</p> <pre><code># Quick operations\ndef get_user(user_id: str) -&gt; UserModel:\n    with APIClient() as client:\n        return client.resource('users', UserModel).get(user_id)\n</code></pre>"},{"location":"api/client/session/#custom-authentication","title":"Custom Authentication","text":"<p>Override default authentication for specific needs:</p> <pre><code># Service authentication\nclient = APIClient(credential_provider=InternalServiceProvider(\"background-job\"))\n\n# User authentication  \nclient = APIClient(credential_provider=UserTokenProvider(\"user123\"))\n\n# Custom authentication\nclient = APIClient(credential_provider=CustomProvider(...))\n</code></pre>"},{"location":"api/client/session/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api/client/session/#development-environment","title":"Development Environment","text":"<pre><code># Local development with debug headers\nclient = APIClient(\n    base_url=\"http://localhost:8000\",\n    default_headers={\n        \"X-Debug\": \"true\",\n        \"User-Agent\": \"Development-Client\"\n    }\n)\n</code></pre>"},{"location":"api/client/session/#production-environment","title":"Production Environment","text":"<pre><code># Production with proper timeouts and headers\nclient = APIClient(\n    default_headers={\n        \"User-Agent\": \"ichrisbirch-frontend/1.0\",\n        \"X-Request-Source\": \"flask-app\"\n    }\n)\n</code></pre>"},{"location":"api/client/session/#testing-environment","title":"Testing Environment","text":"<pre><code># Testing with mock authentication\nmock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\nclient = APIClient(\n    base_url=\"http://test-api:8000\",\n    credential_provider=mock_provider\n)\n</code></pre>"},{"location":"api/client/session/#error-handling","title":"Error Handling","text":"<p>Sessions handle errors at multiple levels:</p>"},{"location":"api/client/session/#http-errors","title":"HTTP Errors","text":"<pre><code>try:\n    response = session.request(\"GET\", \"/invalid-endpoint\")\nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP {e.response.status_code}: {e.response.text}\")\n</code></pre>"},{"location":"api/client/session/#connection-errors","title":"Connection Errors","text":"<pre><code>try:\n    response = session.request(\"GET\", \"/tasks\")\nexcept httpx.ConnectError:\n    print(\"Could not connect to API server\")\n</code></pre>"},{"location":"api/client/session/#authentication-errors","title":"Authentication Errors","text":"<pre><code># Credential provider errors\nif not session.credential_provider.is_available():\n    raise AuthenticationError(\"No credentials available\")\n</code></pre>"},{"location":"api/client/session/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Connection Reuse: Sessions maintain persistent connections</li> <li>Request Pooling: httpx handles connection pooling automatically</li> <li>Memory Management: Close sessions when done to free resources</li> <li>Concurrent Requests: Sessions are not thread-safe; use separate sessions per thread</li> </ol>"},{"location":"api/client/session/#best-practices","title":"Best Practices","text":"<ol> <li>Use Context Managers: Always use <code>with APIClient() as client:</code> when possible</li> <li>Session Reuse: Don't create new clients for each request</li> <li>Proper Cleanup: Call <code>close()</code> or use context managers</li> <li>Error Handling: Handle both HTTP and connection errors</li> <li>Configuration: Set base_url and headers at session level, not per request</li> </ol>"},{"location":"api/client/summary/","title":"API Client Implementation Summary","text":"<p>This document summarizes the new API client architecture implementation and provides quick reference for developers.</p>"},{"location":"api/client/summary/#implementation-complete","title":"Implementation Complete \u2705","text":"<p>The new API client has been fully implemented with:</p> <ul> <li>4 core modules in <code>ichrisbirch/api/client/</code></li> <li>5 comprehensive documentation files in <code>docs/</code></li> <li>Full type safety with Pydantic model integration</li> <li>Industry-standard patterns following boto3/Stripe architecture</li> <li>Zero import errors - all dependencies resolved</li> <li>Backward compatibility - QueryAPI remains functional during migration</li> </ul>"},{"location":"api/client/summary/#quick-reference","title":"Quick Reference","text":""},{"location":"api/client/summary/#factory-functions","title":"Factory Functions","text":"<pre><code>from ichrisbirch.api.client import (\n    APIClient,              # Main client class\n    default_client,         # Context-aware (recommended)\n    internal_service_client,# Service-to-service auth\n    user_client,           # User token auth  \n    flask_session_client   # Flask session auth\n)\n</code></pre>"},{"location":"api/client/summary/#basic-usage-pattern","title":"Basic Usage Pattern","text":"<pre><code># Context-aware client (detects Flask vs non-Flask automatically)\nwith default_client() as client:\n    tasks = client.resource('tasks', TaskModel)\n\n    # CRUD operations\n    task = tasks.get(123)\n    all_tasks = tasks.list(status='active')\n    new_task = tasks.create({'title': 'New Task'})\n    updated_task = tasks.update(123, {'status': 'completed'})\n    tasks.delete(123)\n\n    # Custom actions\n    result = tasks.action('bulk_complete', {'task_ids': [1, 2, 3]})\n</code></pre>"},{"location":"api/client/summary/#file-structure","title":"File Structure","text":"<pre><code>ichrisbirch/api/client/\n\u251c\u2500\u2500 __init__.py          # Public API exports\n\u251c\u2500\u2500 auth.py              # Credential provider abstractions\n\u251c\u2500\u2500 session.py           # Session management and HTTP client\n\u251c\u2500\u2500 resource.py          # Generic resource client with CRUD\n\u2514\u2500\u2500 api.py               # Main APIClient with factory functions\n\ndocs/api/client/\n\u251c\u2500\u2500 index.md           # Main architecture overview\n\u251c\u2500\u2500 auth.md            # Credential providers guide\n\u251c\u2500\u2500 session.md         # Session management details\n\u251c\u2500\u2500 resources.md       # Resource client operations\n\u251c\u2500\u2500 migration.md       # QueryAPI migration guide\n\u251c\u2500\u2500 examples.md        # Practical usage examples\n\u2514\u2500\u2500 summary.md         # Implementation summary\n</code></pre>"},{"location":"api/client/summary/#key-features","title":"Key Features","text":""},{"location":"api/client/summary/#pluggable-authentication","title":"\ud83d\udd10 Pluggable Authentication","text":"<ul> <li>InternalServiceProvider: Service-to-service with API keys</li> <li>UserTokenProvider: User-based JWT tokens</li> <li>FlaskSessionProvider: Flask session integration</li> <li>Custom providers: Easy to extend for new auth methods</li> </ul>"},{"location":"api/client/summary/#context-aware-defaults","title":"\ud83c\udfaf Context-Aware Defaults","text":"<ul> <li>Flask request context: Automatically uses session auth</li> <li>Background jobs: Automatically uses internal service auth</li> <li>No defensive defaults: Fails fast on misconfiguration</li> </ul>"},{"location":"api/client/summary/#generic-resource-pattern","title":"\ud83d\udd04 Generic Resource Pattern","text":"<ul> <li>Single interface: All resources use same CRUD methods</li> <li>Type-safe: Full Pydantic model integration</li> <li>Extensible: Custom actions and endpoints supported</li> </ul>"},{"location":"api/client/summary/#session-management","title":"\u26a1 Session Management","text":"<ul> <li>Connection pooling: Reuses HTTP connections</li> <li>Lifecycle management: Proper cleanup with context managers  </li> <li>Configuration: Centralized base URL and headers</li> </ul>"},{"location":"api/client/summary/#architecture-benefits","title":"Architecture Benefits","text":"Aspect QueryAPI (Old) API Client (New) Pattern Custom implementation Industry standard (boto3-style) Authentication Boolean flag Pluggable providers Type Safety Raw dictionaries Pydantic models Resource Access Specific methods Generic pattern Session Management None Full lifecycle management Context Awareness Flask coupling Clean abstraction Testing Difficult to mock Easy mocking/testing Extensibility Hard to extend Easy to extend"},{"location":"api/client/summary/#migration-status","title":"Migration Status","text":"<ul> <li>\u2705 QueryAPI Enhanced: Added <code>use_internal_auth</code> option, preserved all functionality</li> <li>\u2705 New Architecture: Complete implementation with all features</li> <li>\u2705 Documentation: Comprehensive guides and examples</li> <li>\u2705 Error Handling: No import or syntax errors</li> <li>\ud83d\udfe1 Migration Pending: QueryAPI still in use, migration can begin</li> <li>\ud83d\udfe1 Testing: Needs integration testing with real API endpoints</li> </ul>"},{"location":"api/client/summary/#next-steps","title":"Next Steps","text":""},{"location":"api/client/summary/#for-immediate-use","title":"For Immediate Use","text":"<ol> <li>Import the new client: <code>from ichrisbirch.api.client import default_client</code></li> <li>Use context manager: <code>with default_client() as client:</code></li> <li>Create resource clients: <code>tasks = client.resource('tasks', TaskModel)</code></li> <li>Perform operations: <code>task = tasks.get(123)</code></li> </ol>"},{"location":"api/client/summary/#for-migration","title":"For Migration","text":"<ol> <li>Start Small: Migrate one endpoint at a time</li> <li>Test Thoroughly: Use integration tests with real API</li> <li>Monitor Performance: Compare response times</li> <li>Update Gradually: Keep QueryAPI during transition</li> </ol>"},{"location":"api/client/summary/#for-development","title":"For Development","text":"<ol> <li>Follow Patterns: Use the documented examples</li> <li>Handle Errors: Use comprehensive error handling</li> <li>Manage Sessions: Always use context managers</li> <li>Type Hints: Specify Pydantic models for resources</li> </ol>"},{"location":"api/client/summary/#configuration-required","title":"Configuration Required","text":""},{"location":"api/client/summary/#environment-variables","title":"Environment Variables","text":"<pre><code># Required for internal service authentication\nAUTH_INTERNAL_SERVICE_KEY=your_service_key_here\n\n# API base URL (if not using default)\nAPI_URL=https://your-api-server.com\n</code></pre>"},{"location":"api/client/summary/#pydantic-models","title":"Pydantic Models","text":"<p>Ensure all resources have corresponding Pydantic models:</p> <pre><code># Example model structure\nclass TaskModel(BaseModel):\n    id: Optional[int] = None\n    title: str\n    description: Optional[str] = None\n    status: str = 'pending'\n    assigned_to: Optional[str] = None\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api/client/summary/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/client/summary/#best-practices","title":"Best Practices","text":"<ul> <li>Reuse clients: Don't create new clients for each request</li> <li>Use context managers: Ensure proper cleanup</li> <li>Batch operations: Use custom actions for bulk operations</li> <li>Connection limits: Monitor concurrent connection usage</li> </ul>"},{"location":"api/client/summary/#monitoring-points","title":"Monitoring Points","text":"<ul> <li>Response times: Compare old vs new client performance</li> <li>Connection usage: Monitor HTTP connection pool</li> <li>Memory usage: Sessions should be properly closed</li> <li>Error rates: Track authentication and HTTP errors</li> </ul>"},{"location":"api/client/summary/#support-resources","title":"Support Resources","text":""},{"location":"api/client/summary/#documentation","title":"Documentation","text":"<ul> <li><code>docs/api/client/index.md</code> - Architecture overview</li> <li><code>docs/api/client/examples.md</code> - Practical examples</li> <li><code>docs/api/client/migration.md</code> - Migration guide</li> </ul>"},{"location":"api/client/summary/#code-references","title":"Code References","text":"<ul> <li><code>ichrisbirch/api/client/</code> - Implementation</li> <li><code>ichrisbirch/app/query_api.py</code> - Enhanced QueryAPI for comparison</li> <li><code>tests/</code> - Example test patterns (to be added)</li> </ul>"},{"location":"api/client/summary/#common-patterns","title":"Common Patterns","text":"<pre><code># Flask route\n@app.route('/tasks')\ndef get_tasks():\n    with default_client() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list(status='active')\n\n# Background job\ndef process_tasks():\n    with internal_service_client('processor') as client:\n        tasks = client.resource('tasks', TaskModel)\n        pending = tasks.list(status='pending')\n        # Process tasks...\n\n# Service class\nclass TaskService:\n    def __init__(self):\n        self.client = default_client()\n        self.tasks = self.client.resource('tasks', TaskModel)\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/summary/#success-criteria","title":"Success Criteria","text":"<p>The implementation is considered successful when:</p> <ul> <li>\u2705 Zero Import Errors: All modules load without issues</li> <li>\u2705 Complete Documentation: All usage patterns documented</li> <li>\u2705 Type Safety: Full Pydantic integration working</li> <li>\u2705 Industry Patterns: Follows established client libraries</li> <li>\ud83c\udfaf Migration Ready: QueryAPI can be gradually replaced</li> <li>\ud83c\udfaf Performance: Equal or better than existing QueryAPI</li> <li>\ud83c\udfaf Adoption: Team comfortable with new patterns</li> </ul> <p>Status: \u2705 Implementation Complete - Ready for Migration Last Updated: [Current Date] Maintainer: Engineering Team</p>"},{"location":"devops/","title":"DevOps","text":""},{"location":"devops/#scripts-that-run-periodically","title":"Scripts that Run periodically","text":"<p><code>scripts/postgres-snapshot-to-s3.sh</code> <code>scripts/create-project-stats.sh</code></p>"},{"location":"devops/#supervisor","title":"Supervisor","text":""},{"location":"devops/#nginx","title":"NGINX","text":""},{"location":"devops/#new-server-setup","title":"New Server Setup","text":""},{"location":"devops/new_database/","title":"Setup a New Postgres Database","text":""},{"location":"devops/new_database/#schemas","title":"Schemas","text":"<p>SQLAlchemy cannot create the schemas, neither can alembic, have to create them manually first time <code>create-schemas.py</code> to add the schemas</p>"},{"location":"devops/new_database/#alembic","title":"Alembic","text":"<p>Run in <code>ichrisbirch</code></p> <p>Create the initial tables from the SQLAlchemy models (purpose of --autogenerate) <code>alembic revision --autogenerate -m 'init_tables'</code></p> <p>Run the upgrade to actually create the tables <code>alembic upgrade head</code></p>"},{"location":"devops/new_server/","title":"Setup a New Server","text":""},{"location":"devops/new_server/#installs","title":"Installs","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n\n# base installs\nsudo apt install curl git git-secret -y\n\n# NOTE: Install the postgresql-client version that matches the database, this is for pg_dump backups with the scheduler.\nsudo apt install postgresql-client-16 tmux tldr supervisor nginx neovim pipx -y\n\n# for pyenv\nsudo apt install build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev -y\n\n# for building psycopg from source\nsudo apt install python3-dev libpq-dev -y\n\n# make sure\npipx ensurepath\n\n# Install pyenv\ncurl https://pyenv.run | bash\n\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\n\nexec $SHELL\n\n# Install python\npyenv install 3.12\npyenv global 3.12\n\n# Install poetry making sure to use pyenv python\npipx install --python $(/home/ubuntu/.pyenv/bin/pyenv which python) poetry\n\nsudo chown ubuntu /var/www\n\n##### AT THIS POINT THE AMI SHOULD BE MADE #####\n\n# Clone project\ngit clone https://github.com/datapointchris/ichrisbirch /var/www/ichrisbirch\ncd /var/www/ichrisbirch\n\n# REFER to https://docs.ichrisbirch.com/git_secret/ to get gpg key for git-secret\n\n# Install project\npoetry config virtualenvs.in-project true\n\n# Make log files for project\n./scripts/make_log_files.sh\n\n# Set up nginx and supervisor\nsudo rm /etc/nginx/sites-enabled/default\n\ncd deploy\n./deploy-nginx.sh\n\n./deploy-supervisor.sh\n</code></pre> <p>Change the elastic IP to point to the new server (if only using one server and not load balancer).</p>"},{"location":"devops/nginx/","title":"NGINX","text":""},{"location":"devops/nginx/#how-to-deploy","title":"How to Deploy","text":""},{"location":"devops/pg_cron/","title":"pg_cron","text":"<p>Location: <code>/scripts/sql/pg_cron_setup.sql</code></p> <ol> <li>pg_cron must be added to 'shared_preload_libraries'</li> <li>Reboot required</li> <li>pg_cron runs in the default postgres database, then jobs can be moved to specific databases</li> <li>For AWS RDS: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL_pg_cron.html</li> </ol>"},{"location":"devops/pg_cron/#basic-instructions","title":"Basic Instructions","text":"<ol> <li>Create a function / procedure to run</li> <li>Schedule it with a name</li> <li>Set the database name for the job to the correct db</li> <li>Check that the job details show it has run successfully</li> </ol> <p>Note</p> <p><code>pg_cron</code> is not being used anymore, in favor of <code>APScheduler</code> in the <code>ichrisbirch/scheduler</code> directory.</p>"},{"location":"devops/supervisor/","title":"Supervisor","text":""},{"location":"devops/supervisor/#how-to-deploy","title":"How to Deploy","text":""},{"location":"docker/","title":"Docker Documentation","text":"<p>This section contains comprehensive documentation for Docker containerization of the iChrisBirch application.</p>"},{"location":"docker/#overview","title":"Overview","text":"<p>The iChrisBirch application runs in a containerized environment using Docker and Docker Compose. This approach provides consistent development, testing, and production environments while simplifying deployment and scaling.</p>"},{"location":"docker/#architecture","title":"Architecture","text":"<p>The application consists of multiple services orchestrated through Docker Compose:</p> <ul> <li>API Service: FastAPI backend service handling REST API requests</li> <li>App Service: Flask frontend service serving web interface</li> <li>Scheduler Service: Background job processing service</li> <li>PostgreSQL: Database service for persistent storage</li> <li>Redis: Caching and session storage service</li> </ul>"},{"location":"docker/#documentation-structure","title":"Documentation Structure","text":""},{"location":"docker/#core-documentation","title":"Core Documentation","text":"<ul> <li>Docker Guide: Complete Docker setup and usage guide</li> <li>Docker Compose: Service orchestration and configuration</li> <li>Quick Reference: Essential commands and troubleshooting</li> </ul>"},{"location":"docker/#key-features","title":"Key Features","text":"<ul> <li>Multi-stage builds: Optimized container images for different environments</li> <li>Environment-based configuration: Separate configs for dev/test/prod</li> <li>Health checks: Automatic service health monitoring</li> <li>Volume management: Persistent data storage and development volumes</li> <li>Network isolation: Secure service communication</li> <li>Dependency management: Proper service startup ordering</li> </ul>"},{"location":"docker/#quick-start","title":"Quick Start","text":""},{"location":"docker/#development-environment","title":"Development Environment","text":"<pre><code># Start all services\ndocker-compose -f docker-compose.dev.yml up -d\n\n# View logs\ndocker-compose -f docker-compose.dev.yml logs -f\n\n# Stop services\ndocker-compose -f docker-compose.dev.yml down\n</code></pre>"},{"location":"docker/#testing-environment","title":"Testing Environment","text":"<pre><code># Start test environment\ndocker-compose -f docker-compose.test.yml up -d\n\n# Run tests\ndocker-compose -f docker-compose.test.yml exec api pytest\n\n# Cleanup\ndocker-compose -f docker-compose.test.yml down -v\n</code></pre>"},{"location":"docker/#production-environment","title":"Production Environment","text":"<pre><code># Deploy to production\ndocker-compose -f docker-compose.prod.yml up -d\n\n# Monitor services\ndocker-compose -f docker-compose.prod.yml ps\n\n# Update application\ndocker-compose -f docker-compose.prod.yml pull\ndocker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"docker/#environment-configuration","title":"Environment Configuration","text":"<p>Each environment uses dedicated configuration files:</p> <ul> <li>Development: <code>.dev.env</code> - Debug enabled, development database</li> <li>Testing: <code>.test.env</code> - Test database, isolated environment</li> <li>Production: <code>.prod.env</code> - Production database, optimized settings</li> </ul>"},{"location":"docker/#service-management","title":"Service Management","text":""},{"location":"docker/#individual-service-operations","title":"Individual Service Operations","text":"<pre><code># Start specific service\ndocker-compose -f docker-compose.dev.yml up api\n\n# View service logs\ndocker-compose -f docker-compose.dev.yml logs -f app\n\n# Execute commands in service\ndocker-compose -f docker-compose.dev.yml exec api bash\n\n# Rebuild service\ndocker-compose -f docker-compose.dev.yml build api\n</code></pre>"},{"location":"docker/#database-operations","title":"Database Operations","text":"<pre><code># Run database migrations\ndocker-compose -f docker-compose.dev.yml exec api alembic upgrade head\n\n# Access database\ndocker-compose -f docker-compose.dev.yml exec postgres psql -U postgres -d ichrisbirch\n\n# Backup database\ndocker-compose -f docker-compose.dev.yml exec postgres pg_dump -U postgres ichrisbirch &gt; backup.sql\n</code></pre>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker/#common-issues","title":"Common Issues","text":"<ol> <li>Port conflicts: Ensure ports 5000, 8000, 5432, 6379 are available</li> <li>Permission issues: Check file permissions in mounted volumes</li> <li>Service dependencies: Verify services start in correct order</li> <li>Environment variables: Confirm all required env vars are set</li> </ol>"},{"location":"docker/#health-checks","title":"Health Checks","text":"<p>All services include health checks that can be monitored:</p> <pre><code># Check service health\ndocker-compose -f docker-compose.dev.yml ps\n\n# View detailed health status\ndocker inspect --format='{{json .State.Health}}' container_name\n</code></pre>"},{"location":"docker/#security-considerations","title":"Security Considerations","text":"<ul> <li>Non-root user: All services run as non-root user</li> <li>Network isolation: Services communicate through Docker networks</li> <li>Secret management: Sensitive data managed through environment variables</li> <li>Image security: Regular base image updates and security scanning</li> </ul>"},{"location":"docker/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Multi-stage builds: Reduced image sizes</li> <li>Layer caching: Optimized Dockerfile for build performance</li> <li>Resource limits: Configured memory and CPU limits</li> <li>Volume optimization: Efficient data persistence strategies</li> </ul>"},{"location":"docker/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration: Application configuration management</li> <li>Testing: Testing strategies and setup</li> <li>DevOps: Deployment and infrastructure</li> <li>Troubleshooting: Common issues and solutions</li> </ul>"},{"location":"docker/docker-compose/","title":"Docker Compose Architecture","text":"<p>This document details how Docker Compose orchestrates the ichrisbirch application services across different environments.</p>"},{"location":"docker/docker-compose/#overview","title":"Overview","text":"<p>The ichrisbirch application uses a layered Docker Compose approach where environment-specific override files customize the base configuration for each deployment context.</p>"},{"location":"docker/docker-compose/#environment-comparison","title":"Environment Comparison","text":"Aspect Development Testing CI Production Purpose Local development with hot reload Running pytest suite GitHub Actions CI Live deployment Compose Files base + dev base + test base + test + ci base only External Ports Standard (8000, 5000, 5432) Alternate (8001, 5001, 5434) Alternate (same as test) Standard Hot Reload Yes No No No AWS Credentials <code>~/.config/aws</code> mount <code>~/.config/aws</code> mount Environment variables (OIDC) IAM role Database Persistent volume tmpfs (in-memory) tmpfs (in-memory) Persistent volume Traefik Dashboard enabled Dashboard enabled Dashboard disabled Dashboard disabled Network External <code>proxy</code> External <code>proxy</code> Internal bridge External <code>proxy</code>"},{"location":"docker/docker-compose/#file-structure","title":"File Structure","text":"<pre><code>docker-compose.yml           # Base/production configuration\ndocker-compose.dev.yml       # Development overrides (hot reload, mounts)\ndocker-compose.test.yml      # Testing overrides (alternate ports, tmpfs)\ndocker-compose.ci.yml        # CI overrides (no local mounts, internal network)\n</code></pre>"},{"location":"docker/docker-compose/#how-layering-works","title":"How Layering Works","text":"<p>Docker Compose merges multiple files in order, with later files overriding earlier ones:</p> <pre><code># Development\ndocker compose -f docker-compose.yml -f docker-compose.dev.yml up -d\n\n# Testing (local)\ndocker compose -f docker-compose.yml -f docker-compose.test.yml up -d\n\n# CI (GitHub Actions)\ndocker compose -f docker-compose.yml -f docker-compose.test.yml -f docker-compose.ci.yml up -d\n\n# Production\ndocker compose -f docker-compose.yml up -d\n</code></pre>"},{"location":"docker/docker-compose/#layering-example","title":"Layering Example","text":"<p>Base (<code>docker-compose.yml</code>):</p> <pre><code>services:\n  api:\n    build:\n      target: production\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n</code></pre> <p>Dev Override (<code>docker-compose.dev.yml</code>):</p> <pre><code>services:\n  api:\n    build:\n      target: development\n    volumes:\n      - .:/app  # Adds source mount for hot reload\n      - ~/.config/aws:/root/.aws:ro  # AWS credentials\n</code></pre> <p>Result: Dev environment gets development build target AND source mounts.</p>"},{"location":"docker/docker-compose/#base-configuration-docker-composeyml","title":"Base Configuration (<code>docker-compose.yml</code>)","text":"<p>The base file defines production-ready services:</p>"},{"location":"docker/docker-compose/#core-services","title":"Core Services","text":"Service Purpose Internal Port <code>traefik</code> Reverse proxy with HTTPS 80, 443, 8080 <code>postgres</code> PostgreSQL 16 database 5432 <code>redis</code> Redis 7 cache 6379 <code>api</code> FastAPI backend 8000 <code>app</code> Flask frontend 5000 <code>chat</code> Streamlit chat interface 8505 <code>scheduler</code> APScheduler background jobs N/A"},{"location":"docker/docker-compose/#service-dependencies","title":"Service Dependencies","text":"<pre><code>postgres \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u25ba api \u2500\u2500\u2500\u2500\u2500\u2500\u25ba app\n              \u2502       \u2502\nredis \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba chat\n              \u2502       \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u25ba scheduler\n</code></pre> <p>Services wait for dependencies via health checks:</p> <pre><code>depends_on:\n  postgres:\n    condition: service_healthy\n  redis:\n    condition: service_healthy\n</code></pre>"},{"location":"docker/docker-compose/#development-override-docker-composedevyml","title":"Development Override (<code>docker-compose.dev.yml</code>)","text":"<p>Purpose: Enable rapid development with hot reload and debugging.</p>"},{"location":"docker/docker-compose/#development-features","title":"Development Features","text":"<ul> <li>Hot reload: Source code mounted as volume, servers watch for changes</li> <li>Debug logging: Verbose log output</li> <li>Local credentials: AWS credentials mounted from host</li> <li>Traefik dashboard: Enabled for debugging routing</li> </ul>"},{"location":"docker/docker-compose/#volume-mounts","title":"Volume Mounts","text":"<pre><code>volumes:\n  - type: bind\n    source: .\n    target: /app\n  - type: bind\n    source: ~/.config/aws\n    target: /root/.aws\n    read_only: true\n</code></pre>"},{"location":"docker/docker-compose/#usage","title":"Usage","text":"<pre><code># Via CLI (recommended)\n./cli/ichrisbirch dev start\n\n# Direct Docker Compose\ndocker compose -f docker-compose.yml -f docker-compose.dev.yml up -d\n</code></pre>"},{"location":"docker/docker-compose/#testing-override-docker-composetestyml","title":"Testing Override (<code>docker-compose.test.yml</code>)","text":"<p>Purpose: Run pytest suite with isolated, fast database.</p>"},{"location":"docker/docker-compose/#testing-features","title":"Testing Features","text":"<ul> <li>Alternate ports: Avoids conflicts with running dev environment</li> <li>tmpfs database: In-memory PostgreSQL for speed</li> <li>No persistence: Each test run starts fresh</li> <li>Optimized Postgres: Disabled fsync/durability for performance</li> </ul>"},{"location":"docker/docker-compose/#port-mapping","title":"Port Mapping","text":"Service Dev Port Test Port API 8000 8001 App 5000 5001 Chat 8505 8507 PostgreSQL 5432 5434 Redis 6379 6380 Traefik HTTPS 443 8443"},{"location":"docker/docker-compose/#database-optimizations","title":"Database Optimizations","text":"<pre><code>postgres:\n  tmpfs:\n    - /tmp/postgres  # In-memory storage\n  command: &gt;\n    postgres\n    -c fsync=off\n    -c synchronous_commit=off\n    -c full_page_writes=off\n</code></pre>"},{"location":"docker/docker-compose/#testing-usage","title":"Testing Usage","text":"<pre><code># Recommended: Ephemeral test run (starts fresh, runs tests, stops)\n./cli/ichrisbirch test run\n\n# With coverage\n./cli/ichrisbirch test cov\n\n# Keep containers for debugging\n./cli/ichrisbirch test run --keep\n\n# Manual management (for extended debugging)\n./cli/ichrisbirch testing start\nuv run pytest\n./cli/ichrisbirch testing stop\n\n# Direct Docker Compose\ndocker compose -f docker-compose.yml -f docker-compose.test.yml \\\n  --project-name icb-test up -d\n</code></pre>"},{"location":"docker/docker-compose/#ci-override-docker-composeciyml","title":"CI Override (<code>docker-compose.ci.yml</code>)","text":"<p>Purpose: Run tests in GitHub Actions where local paths don't exist.</p>"},{"location":"docker/docker-compose/#why-ci-needs-special-handling","title":"Why CI Needs Special Handling","text":"<p>The CI environment differs from local development:</p> Difference Local CI Solution AWS credentials <code>~/.config/aws</code> exists Only env vars via OIDC Remove bind mount Proxy network Pre-created externally Doesn't exist Create as bridge Project files Full local checkout GitHub checkout only Use volumes not binds Traefik dashboard Useful for debugging Not needed Disable"},{"location":"docker/docker-compose/#ci-specific-overrides","title":"CI-Specific Overrides","text":"<pre><code>services:\n  api:\n    volumes:\n      # Remove AWS credentials mount, keep other volumes\n      - type: bind\n        source: .\n        target: /app\n      - type: volume\n        source: venv_shared\n        target: /app/.venv\n\nnetworks:\n  proxy:\n    external: false  # Create internally instead of expecting external\n    driver: bridge\n</code></pre>"},{"location":"docker/docker-compose/#container-startup-in-ci","title":"Container Startup in CI","text":"<p>The CI workflow pre-starts containers before pytest runs:</p> <pre><code># Phase 1: Database services\ndocker compose ... up -d --build postgres redis\nsleep 10\n\n# Phase 2: Application services\ndocker compose ... up -d --build api app chat scheduler\nsleep 30\n</code></pre> <p>Why this order:</p> <ol> <li><code>postgres</code> and <code>redis</code> must pass health checks first</li> <li><code>api</code> initializes the shared virtual environment</li> <li><code>scheduler</code> creates <code>apscheduler_jobs</code> table</li> <li>Sleep allows health checks to complete</li> </ol>"},{"location":"docker/docker-compose/#test-environment-detection","title":"Test Environment Detection","text":"<p>The test fixtures detect CI and skip container management:</p> <pre><code># In tests/environment.py\n@property\ndef is_ci(self) -&gt; bool:\n    return os.environ.get('CI', '').lower() == 'true'\n\ndef setup(self):\n    if self.is_ci:\n        # Containers pre-started by workflow\n        logger.info('Running in CI - containers should be pre-started')\n    else:\n        # Start containers ourselves\n        self.setup_test_services()\n</code></pre>"},{"location":"docker/docker-compose/#production-configuration","title":"Production Configuration","text":"<p>Purpose: Production deployment with security and reliability.</p>"},{"location":"docker/docker-compose/#production-features","title":"Production Features","text":"<ul> <li>No source mounts: Code baked into image</li> <li>Non-root user: Security hardening</li> <li>Gunicorn workers: Multi-process for performance</li> <li>Resource limits: Prevent runaway containers</li> <li>Health checks: Automatic recovery</li> </ul>"},{"location":"docker/docker-compose/#production-differences","title":"Production Differences","text":"<pre><code>services:\n  api:\n    build:\n      target: production  # Minimal image, no dev deps\n    user: appuser  # Non-root\n    volumes: []  # No source mounts\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n</code></pre>"},{"location":"docker/docker-compose/#network-architecture","title":"Network Architecture","text":""},{"location":"docker/docker-compose/#traefik-routing","title":"Traefik Routing","text":"<p>All HTTP(S) traffic flows through Traefik:</p> <pre><code>Internet \u2500\u2500\u25ba Traefik (:443) \u2500\u2500\u252c\u2500\u2500\u25ba api.domain.com \u2500\u2500\u25ba API (:8000)\n                              \u251c\u2500\u2500\u25ba app.domain.com \u2500\u2500\u25ba App (:5000)\n                              \u2514\u2500\u2500\u25ba chat.domain.com \u2500\u2500\u25ba Chat (:8505)\n</code></pre>"},{"location":"docker/docker-compose/#internal-communication","title":"Internal Communication","text":"<p>Services communicate via Docker DNS using service names:</p> <pre><code># Flask app calling FastAPI\nresponse = httpx.get('http://api:8000/tasks/')\n\n# Scheduler connecting to Postgres\nengine = create_engine('postgresql://postgres:5432/ichrisbirch')\n</code></pre>"},{"location":"docker/docker-compose/#common-operations","title":"Common Operations","text":""},{"location":"docker/docker-compose/#starting-environments","title":"Starting Environments","text":"<pre><code># Development\n./cli/ichrisbirch dev start\n\n# Testing\n./cli/ichrisbirch testing start\n\n# Production\n./cli/ichrisbirch prod start\n</code></pre>"},{"location":"docker/docker-compose/#viewing-logs","title":"Viewing Logs","text":"<pre><code># All services\n./cli/ichrisbirch dev logs\n\n# Specific service\ndocker compose logs -f api\n\n# With timestamps\ndocker compose logs -f --timestamps api\n</code></pre>"},{"location":"docker/docker-compose/#rebuilding-images","title":"Rebuilding Images","text":"<pre><code># Rebuild and restart\n./cli/ichrisbirch dev rebuild\n\n# Rebuild specific service\ndocker compose build api\ndocker compose up -d api\n</code></pre>"},{"location":"docker/docker-compose/#accessing-containers","title":"Accessing Containers","text":"<pre><code># Shell into running container\ndocker compose exec api bash\n\n# Run one-off command\ndocker compose run --rm api python -c \"print('hello')\"\n</code></pre>"},{"location":"docker/docker-compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker/docker-compose/#container-wont-start","title":"Container Won't Start","text":"<pre><code># Check logs\ndocker compose logs api\n\n# Check health\ndocker compose ps\n\n# Inspect container\ndocker inspect icb-dev-api\n</code></pre>"},{"location":"docker/docker-compose/#port-conflicts","title":"Port Conflicts","text":"<p>If ports are in use:</p> <pre><code># Find what's using the port\nlsof -i :8000\n\n# Use testing ports instead\n./cli/ichrisbirch testing start  # Uses 8001, 5001, etc.\n</code></pre>"},{"location":"docker/docker-compose/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Check postgres is healthy\ndocker compose ps postgres\n\n# Test connection\ndocker compose exec postgres psql -U postgres -d ichrisbirch -c \"SELECT 1\"\n\n# Check logs\ndocker compose logs postgres\n</code></pre>"},{"location":"docker/docker-compose/#network-issues","title":"Network Issues","text":"<pre><code># List networks\ndocker network ls\n\n# Inspect network\ndocker network inspect ichrisbirch_proxy\n\n# Recreate network\ndocker network rm ichrisbirch_proxy\ndocker network create proxy\n</code></pre>"},{"location":"docker/docker-compose/#best-practices","title":"Best Practices","text":"<ol> <li>Use the CLI: <code>./cli/ichrisbirch dev start</code> handles complexity</li> <li>Don't mix environments: Use testing ports when dev is running</li> <li>Check health first: <code>./cli/ichrisbirch dev health</code> before debugging</li> <li>Read logs: Most issues are visible in container logs</li> <li>Rebuild after Dockerfile changes: Images don't auto-update</li> </ol>"},{"location":"docker/docker-quick-reference/","title":"Docker Quick Reference","text":"<p>Quick commands and troubleshooting for the ichrisbirch Docker setup.</p>"},{"location":"docker/docker-quick-reference/#essential-commands","title":"Essential Commands","text":""},{"location":"docker/docker-quick-reference/#development","title":"Development","text":"<pre><code># Start development environment\n./scripts/dev-start.sh\n\n# Stop development environment\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml down\n\n# Rebuild and start\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml up --build\n\n# View logs\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml logs -f api\n</code></pre>"},{"location":"docker/docker-quick-reference/#testing","title":"Testing","text":"<pre><code># Run all tests\n./scripts/test-run.sh\n\n# Run specific test file\n./scripts/test-run.sh tests/ichrisbirch/api/endpoints/test_habits.py\n\n# Run with coverage\n./scripts/test-run.sh --cov=ichrisbirch --cov-report=html\n</code></pre>"},{"location":"docker/docker-quick-reference/#production","title":"Production","text":"<pre><code># Deploy production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Check status\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml ps\n\n# View production logs\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml logs -f\n</code></pre>"},{"location":"docker/docker-quick-reference/#debug-commands","title":"Debug Commands","text":""},{"location":"docker/docker-quick-reference/#container-inspection","title":"Container Inspection","text":"<pre><code># Shell into running container\ndocker-compose exec api bash\n\n# Run one-off command\ndocker-compose run --rm api python -c \"import ichrisbirch; print('OK')\"\n\n# Check environment variables\ndocker-compose exec api env | grep POSTGRES\n</code></pre>"},{"location":"docker/docker-quick-reference/#service-health","title":"Service Health","text":"<pre><code># Check if services are running\ndocker-compose ps\n\n# Test service connectivity\ndocker-compose exec api ping postgres\ndocker-compose exec api ping redis\n\n# Check port availability\ndocker-compose exec api netstat -tlnp\n</code></pre>"},{"location":"docker/docker-quick-reference/#database-access","title":"Database Access","text":"<pre><code># Connect to PostgreSQL\ndocker-compose exec postgres psql -U postgres -d ichrisbirch_dev\n\n# Check database tables\ndocker-compose exec postgres psql -U postgres -d ichrisbirch_dev -c \"\\\\dt\"\n\n# Connect to Redis\ndocker-compose exec redis redis-cli\n</code></pre>"},{"location":"docker/docker-quick-reference/#common-issues","title":"Common Issues","text":""},{"location":"docker/docker-quick-reference/#port-conflicts","title":"Port Conflicts","text":"<p>Error: <code>Port 8000 is already in use</code></p> <p>Solution:</p> <pre><code># Find process using port\nlsof -i :8000\n\n# Stop all containers\ndocker-compose down\n\n# Start with different ports\n# Edit .env file to change FASTAPI_PORT\n</code></pre>"},{"location":"docker/docker-quick-reference/#database-connection","title":"Database Connection","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solution:</p> <pre><code># Check if PostgreSQL is running\ndocker-compose ps postgres\n\n# Check network connectivity\ndocker-compose exec api ping postgres\n\n# Verify environment variables\ndocker-compose exec api env | grep POSTGRES\n</code></pre>"},{"location":"docker/docker-quick-reference/#permission-issues","title":"Permission Issues","text":"<p>Error: <code>Permission denied</code></p> <p>Solution:</p> <pre><code># Fix file ownership\nsudo chown -R $(id -u):$(id -g) .\n\n# Rebuild with correct permissions\ndocker-compose build --no-cache\n</code></pre>"},{"location":"docker/docker-quick-reference/#environment-files","title":"Environment Files","text":""},{"location":"docker/docker-quick-reference/#development-devenv","title":"Development (<code>.dev.env</code>)","text":"<pre><code>ENVIRONMENT=\"development\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch_dev\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#testing-testenv","title":"Testing (<code>.test.env</code>)","text":"<pre><code>ENVIRONMENT=\"testing\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch_test\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#production-prodenv","title":"Production (<code>.prod.env</code>)","text":"<pre><code>ENVIRONMENT=\"production\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#build-targets","title":"Build Targets","text":""},{"location":"docker/docker-quick-reference/#development-build","title":"Development Build","text":"<pre><code># Build development image\ndocker build --target development -t ichrisbirch:dev .\n\n# Features: hot-reload, dev dependencies, debugging\n</code></pre>"},{"location":"docker/docker-quick-reference/#production-build","title":"Production Build","text":"<pre><code># Build production image\ndocker build --target production -t ichrisbirch:prod .\n\n# Features: minimal size, security hardening, performance optimized\n</code></pre>"},{"location":"docker/docker-quick-reference/#service-urls","title":"Service URLs","text":"<p>http://localhost:8000/docs</p>"},{"location":"docker/docker-quick-reference/#developmenthttplocalhost5000","title":"Developmenthttp://localhost:5000","text":"<ul> <li>API Documentation: http://localhost:8000/docs</li> <li>Web Application: http://localhost:5000</li> <li>Database: localhost:5432</li> <li>Redis: localhost:6379 http://localhost:8001/docs</li> </ul>"},{"location":"docker/docker-quick-reference/#testinghttplocalhost5001","title":"Testinghttp://localhost:5001","text":"<ul> <li>API: http://localhost:8001/docs</li> <li>Web Application: http://localhost:5001</li> <li>Database: localhost:5434</li> <li>Redis: localhost:6380 http://localhost:8000</li> </ul>"},{"location":"docker/docker-quick-reference/#productionhttplocalhost5000","title":"Productionhttp://localhost:5000","text":"<ul> <li>API: http://localhost:8000</li> <li>Web Application: http://localhost:5000</li> <li>Database: localhost:5432 (if exposed)</li> <li>Redis: localhost:6379 (if exposed)</li> </ul>"},{"location":"docker/docker-quick-reference/#useful-docker-commands","title":"Useful Docker Commands","text":"<pre><code># Clean up unused containers, networks, images\ndocker system prune -a\n\n# View image sizes\ndocker images\n\n# View container resource usage\ndocker stats\n\n# Export container filesystem\ndocker export &lt;container_id&gt; &gt; backup.tar\n\n# Import container filesystem\ndocker import backup.tar ichrisbirch:backup\n</code></pre>"},{"location":"docker/docker-quick-reference/#monitoring","title":"Monitoring","text":""},{"location":"docker/docker-quick-reference/#health-checks","title":"Health Checks","text":"<pre><code># Check health status\ndocker-compose ps\n\n# Manual health check\ncurl -f http://localhost:8000/health\n</code></pre>"},{"location":"docker/docker-quick-reference/#resource-usage","title":"Resource Usage","text":"<pre><code># Container stats\ndocker stats\n\n# Detailed container info\ndocker inspect &lt;container_id&gt;\n\n# Process list in container\ndocker-compose exec api ps aux\n</code></pre>"},{"location":"docker/docker-quick-reference/#advanced-usage","title":"Advanced Usage","text":""},{"location":"docker/docker-quick-reference/#custom-commands","title":"Custom Commands","text":"<pre><code># Run database migrations\ndocker-compose run --rm api alembic upgrade head\n\n# Create new migration\ndocker-compose run --rm api alembic revision --autogenerate -m \"description\"\n\n# Run Python shell\ndocker-compose run --rm api python\n\n# Install new package\ndocker-compose run --rm api poetry add package-name\n</code></pre>"},{"location":"docker/docker-quick-reference/#volume-management","title":"Volume Management","text":"<pre><code># List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect ichrisbirch_postgres_data\n\n# Backup volume\ndocker run --rm -v ichrisbirch_postgres_data:/data -v $(pwd):/backup alpine tar czf /backup/backup.tar.gz /data\n\n# Restore volume\ndocker run --rm -v ichrisbirch_postgres_data:/data -v $(pwd):/backup alpine sh -c \"cd /data &amp;&amp; tar xzf /backup/backup.tar.gz --strip 1\"\n</code></pre> <p>This reference should help with day-to-day Docker operations and troubleshooting!</p>"},{"location":"docker/docker/","title":"Docker Architecture and Build Process","text":"<p>This document explains the Docker architecture for the ichrisbirch application, including multi-stage builds, environment-specific configurations, and integration with Docker Compose.</p> <ul> <li>Overview</li> <li>Dockerfile Architecture</li> <li>Multi-Stage Build Strategy</li> <li>Why Multi-Stage?</li> <li>Build Stages</li> <li>1. Python Base Stage (<code>python-base</code>)</li> <li>2. Builder Stage (<code>builder</code>)</li> <li>Production Environment</li> <li>docker-compose.prod.yml</li> <li>Environment File Integration</li> <li>Build Commands</li> <li>Direct Docker Build</li> <li>Docker Compose Build</li> <li>Convenience Scripts</li> <li>Service-Specific Commands</li> <li>Multiple Services from One Image</li> <li>Why One Dockerfile for Multiple Services?</li> <li>Best Practices</li> <li>1. Layer Caching</li> <li>4. Development Experience</li> <li>Common Issues<ul> <li>4. Development Hot-Reload Not Working</li> </ul> </li> <li>Check running processes</li> <li>View logs</li> <li>Check environment variables</li> <li>Performance Optimization</li> <li>Summary</li> </ul>"},{"location":"docker/docker/#overview","title":"Overview","text":"<p>The ichrisbirch application uses a multi-stage Docker build approach that creates optimized images for different environments:</p> <ul> <li>Development: Full development environment with hot-reload and debugging tools</li> <li>Production: Minimal, security-hardened runtime environment</li> <li>Testing: Containerized testing environment with all dependencies</li> </ul> <p>All three environments share the same base image and dependencies but differ in configuration, installed packages, and runtime commands.</p>"},{"location":"docker/docker/#dockerfile-architecture","title":"Dockerfile Architecture","text":""},{"location":"docker/docker/#multi-stage-build-strategy","title":"Multi-Stage Build Strategy","text":"<pre><code># Stage 1: python-base (shared foundation)\nFROM python:3.12-slim as python-base\n\n# Stage 2: builder (dependency compilation)\nFROM python-base as builder\n\n# Stage 3: development (dev tools + hot reload)\nFROM python-base as development\n\n# Stage 4: production (minimal runtime)\nFROM python-base as production\n</code></pre>"},{"location":"docker/docker/#why-multi-stage","title":"Why Multi-Stage?","text":"<ol> <li>Shared Base: All stages share the same Python runtime and basic configuration</li> <li>Build Isolation: Compilation tools are only in the builder stage</li> <li>Size Optimization: Production images exclude development dependencies</li> <li>Security: Production images have minimal attack surface</li> <li>Consistency: Same base ensures identical runtime behavior</li> </ol>"},{"location":"docker/docker/#build-stages","title":"Build Stages","text":""},{"location":"docker/docker/#1-python-base-stage-python-base","title":"1. Python Base Stage (<code>python-base</code>)","text":"<pre><code>FROM python:3.12-slim as python-base\n</code></pre> <p>Purpose: Establishes the foundation for all other stages</p> <p>Key Features:</p> <p>Environment Variables:</p> <pre><code>    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.8.3 \\\n    POETRY_HOME=\"/opt/poetry\" \\\n    APP_PATH=\"/app\"\n</code></pre>"},{"location":"docker/docker/#2-builder-stage-builder","title":"2. Builder Stage (<code>builder</code>)","text":"<pre><code>**Purpose**: Compiles dependencies and creates the virtual environment\n</code></pre> <pre><code>FROM python-base as development\n</code></pre> <p>Key Features:</p> <ul> <li>Copies virtual environment from builder</li> </ul> <p>Development-Specific:</p> <pre><code>RUN poetry install --no-root  # Includes dev dependencies\n### 4. Production Stage (`production`)\nFROM python-base as production\n</code></pre> <p>Key Features:</p> <ul> <li>Only runtime dependencies (no build tools)</li> <li> <p>Minimal system packages</p> </li> <li> <p>Security optimizations</p> </li> </ul> <pre><code>RUN poetry install --only=main --no-root  # No dev dependencies\nCMD [\"gunicorn\", \"ichrisbirch.wsgi_api:api\", \"--bind\", \"0.0.0.0:8000\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--workers\", \"4\"]\n\n## Environment Configurations\n\n\n### Development Environment\n\n\n**Image Target**: `development`\n\n\n\n- Hot-reload enabled\n\n- Volume mounts for live code editing\n\n\n# docker-compose.dev.yml\nservices:\n  api:\n\n      context: .\n\n\n    command: uvicorn ichrisbirch.wsgi_api:api --host 0.0.0.0 --port 8000 --reload\n    volumes:\n      - .:/app\n</code></pre>"},{"location":"docker/docker/#production-environment","title":"Production Environment","text":"<p>Image Target: <code>production</code></p> <p>Characteristics:</p> <ul> <li>Security hardening</li> <li>Health checks Docker Compose Usage:</li> </ul>"},{"location":"docker/docker/#docker-composeprodyml","title":"docker-compose.prod.yml","text":"<p>api:       context: .       target: production     command: gunicorn ichrisbirch.wsgi_api:api --bind 0.0.0.0:8000 --worker-class uvicorn.workers.UvicornWorker --workers 4</p> <pre><code>### Testing Environment\n\n**Image Target**: `development` (with test configuration)\n**Characteristics**:\n\n- Same as development but with test configuration\n\n- Test-specific environment variables\n- Isolated test networks\n\n## Docker Compose Integration\n\n\n### How Docker Compose Uses the Dockerfile\n\nDocker Compose extends the Dockerfile with:\n\n\n1. **Target Selection**: Chooses which stage to build\n2. **Environment Variables**: Injects configuration via `.env` files\n3. **Volume Mounts**: Overlays source code for development\n4. **Network Configuration**: Connects services\n5. **Command Overrides**: Customizes startup commands\n\n### Build Process Flow\n```mermaid\ngraph TD\n    A[docker-compose build] --&gt; B[Read Dockerfile]\n    B --&gt; C[Build python-base stage]\n    D --&gt; E{Target specified?}\n    E --&gt;|development| F[Build development stage]\n    E --&gt;|production| G[Build production stage]\n    F --&gt; H[Apply docker-compose overrides]\n    G --&gt; H\n    H --&gt; I[Start services]\n</code></pre>"},{"location":"docker/docker/#environment-file-integration","title":"Environment File Integration","text":"<p>Each environment uses its own <code>.env</code> file:</p> <pre><code># Development\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml up\n\n# Testing\ndocker-compose --env-file .test.env -f docker-compose.yml -f docker-compose.test.yml up\n\n# Production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up\n</code></pre>"},{"location":"docker/docker/#build-commands","title":"Build Commands","text":""},{"location":"docker/docker/#direct-docker-build","title":"Direct Docker Build","text":"<pre><code># Build development image\ndocker build --target development -t ichrisbirch:dev .\n\n# Build production image\ndocker build --target production -t ichrisbirch:prod .\n\n# Build with build arguments\ndocker build --target production --build-arg POETRY_VERSION=1.8.3 -t ichrisbirch:prod .\n</code></pre>"},{"location":"docker/docker/#docker-compose-build","title":"Docker Compose Build","text":"<pre><code># Build development environment\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml build\n\n# Build production environment\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml build\n\n# Build with no cache\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml build --no-cache\n</code></pre>"},{"location":"docker/docker/#convenience-scripts","title":"Convenience Scripts","text":"<pre><code># Development\n./scripts/dev-start.sh\n\n# Testing\n./scripts/test-run.sh\n\n# Production (manual)\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"docker/docker/#service-specific-commands","title":"Service-Specific Commands","text":""},{"location":"docker/docker/#multiple-services-from-one-image","title":"Multiple Services from One Image","text":"<p>The same Docker image can run different services by overriding the command:</p> <pre><code># docker-compose.yml\nservices:\n  # FastAPI Backend\n  api:\n    build: .\n    command: uvicorn ichrisbirch.wsgi_api:api --host 0.0.0.0 --port 8000\n\n  # Flask Frontend\n  app:\n    build: .\n    command: flask --app ichrisbirch.wsgi_app:app run --host 0.0.0.0 --port 5000\n\n\n  # Scheduler\n  scheduler:\n    build: .\n    command: python -m ichrisbirch.wsgi_scheduler\n</code></pre>"},{"location":"docker/docker/#why-one-dockerfile-for-multiple-services","title":"Why One Dockerfile for Multiple Services?","text":"<ol> <li>Consistency: Same dependencies and runtime environment</li> <li>Efficiency: Single build process, shared layers</li> <li>Maintainability: One file to update for all services</li> <li>Resource Optimization: Shared base images reduce storage</li> </ol>"},{"location":"docker/docker/#best-practices","title":"Best Practices","text":""},{"location":"docker/docker/#1-layer-caching","title":"1. Layer Caching","text":"<p>Dependencies are installed before copying source code:</p> <pre><code>COPY pyproject.toml poetry.lock* ./\n\n\nCOPY --chown=appuser:appuser ichrisbirch/ ./ichrisbirch/\n</code></pre> <ul> <li> <p>Non-root user for all processes</p> </li> <li> <p>Proper file permissions</p> </li> <li> <p>Multi-stage builds exclude build dependencies from production</p> </li> <li>Cleanup package caches after installation</li> </ul>"},{"location":"docker/docker/#4-development-experience","title":"4. Development Experience","text":"<ul> <li>Volume mounts for live code editing</li> </ul>"},{"location":"docker/docker/#common-issues","title":"Common Issues","text":"<pre><code># Rebuild without cache\ndocker-compose build --no-cache\n# Check Poetry version\ndocker run --rm ichrisbirch:dev poetry --version\n\n\n**Solution**:\n```dockerfile\n# Or fix permissions\n\ndocker run --rm ichrisbirch:dev python -c \"import psycopg2; print('OK')\"\n\n# Verify service communication\ndocker-compose exec api ping postgres\n</code></pre>"},{"location":"docker/docker/#4-development-hot-reload-not-working","title":"4. Development Hot-Reload Not Working","text":"<p>Error: Changes not reflected in running container</p> <p>Solution: Verify volume mounts in docker-compose.dev.yml:</p> <pre><code>volumes:\n  - .:/app\n  - /app/.venv  # Exclude virtual environment\n</code></pre>"},{"location":"docker/docker/#check-running-processes","title":"Check running processes","text":"<pre><code>docker-compose exec api ps aux\n</code></pre>"},{"location":"docker/docker/#view-logs","title":"View logs","text":"<pre><code>docker-compose exec api bash\n</code></pre>"},{"location":"docker/docker/#check-environment-variables","title":"Check environment variables","text":"<pre><code>docker-compose exec api env | grep -E \"(POSTGRES|REDIS|FASTAPI)\"\n</code></pre>"},{"location":"docker/docker/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Clean up unused images\ndocker system prune -a\n\n# Check image sizes\ndocker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"\n</code></pre>"},{"location":"docker/docker/#summary","title":"Summary","text":"<p>This Docker architecture provides:</p> <ul> <li>Flexibility: Same image, different configurations</li> <li>Security: Non-root execution, minimal attack surface</li> <li>Efficiency: Multi-stage builds, layer caching</li> <li> <p>Consistency: Identical environments across development, testing, and production</p> </li> <li> <p>Maintainability: Single Dockerfile for all services</p> </li> </ul> <p>The integration with Docker Compose allows for easy environment switching while maintaining consistency in the underlying application runtime.</p>"},{"location":"testing/environment/","title":"Test Environment Configuration","text":"<p>This document details how the test environment is configured, set up, and managed in the ichrisbirch project's testing infrastructure.</p>"},{"location":"testing/environment/#overview","title":"Overview","text":"<p>The test environment uses Docker Compose to provide isolated, reproducible infrastructure for running pytest. The <code>DockerComposeTestEnvironment</code> class manages the lifecycle of these containers.</p>"},{"location":"testing/environment/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Test Environment                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502 Postgres \u2502  \u2502  Redis   \u2502  \u2502   API    \u2502  \u2502   App    \u2502        \u2502\n\u2502  \u2502  :5434   \u2502  \u2502  :6380   \u2502  \u2502  :8001   \u2502  \u2502  :5001   \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502       \u2502             \u2502             \u2502             \u2502               \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502                           \u2502                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502   Chat   \u2502  \u2502    Scheduler     \u2502  \u2502 Traefik  \u2502              \u2502\n\u2502  \u2502  :8507   \u2502  \u2502  (creates jobs)  \u2502  \u2502  :8443   \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"testing/environment/#dockercomposetestenvironment-class","title":"DockerComposeTestEnvironment Class","text":"<p>Located in <code>tests/environment.py</code>, this class manages the Docker Compose test environment.</p>"},{"location":"testing/environment/#key-features","title":"Key Features","text":"<ul> <li>Docker Compose orchestration: Starts/stops all test containers</li> <li>CI detection: Adjusts behavior when running in GitHub Actions</li> <li>Health checking: Waits for services to be ready</li> <li>Database initialization: Creates tables and test users</li> </ul>"},{"location":"testing/environment/#class-structure","title":"Class Structure","text":"<pre><code>class DockerComposeTestEnvironment:\n    # Compose file combinations\n    COMPOSE_FILES = '-f docker-compose.yml -f docker-compose.test.yml'\n    COMPOSE_FILES_CI = '-f docker-compose.yml -f docker-compose.test.yml -f docker-compose.ci.yml'\n\n    @property\n    def is_ci(self) -&gt; bool:\n        \"\"\"Detect if running in CI environment.\"\"\"\n        return os.environ.get('CI', '').lower() == 'true'\n\n    def setup(self):\n        \"\"\"Start containers and initialize database.\"\"\"\n\n    def teardown(self):\n        \"\"\"Stop containers (skipped in CI).\"\"\"\n</code></pre>"},{"location":"testing/environment/#lifecycle-methods","title":"Lifecycle Methods","text":""},{"location":"testing/environment/#setup","title":"<code>setup()</code>","text":"<p>Called at the start of the test session:</p> <ol> <li>CI check: If in CI, verify containers are already running</li> <li>Local: Always do full cleanup and start fresh (handles back-to-back runs)</li> <li>Database init: Create tables and insert test users</li> </ol> <pre><code>def setup(self):\n    if self.is_ci:\n        # CI: Containers pre-started by workflow\n        if not self.docker_test_services_already_running():\n            raise RuntimeError('CI containers not running')\n    else:\n        # Local: Always do full cleanup first (handles race conditions from back-to-back runs)\n        self.stop_docker_compose()\n        time.sleep(3)  # Wait for volumes/networks to be fully released\n        self.setup_test_services()\n\n    self.ensure_database_ready()\n</code></pre> <p>Note: The local environment always does a full cleanup before starting. This ensures reliable behavior when running tests back-to-back (e.g., pre-commit hooks running affected tests then full suite). The time penalty (~50s for container restart) is accepted for reliability.</p>"},{"location":"testing/environment/#teardown","title":"<code>teardown()</code>","text":"<p>Called at the end of the test session:</p> <pre><code>def teardown(self):\n    if self.is_ci:\n        # CI: Cleanup handled by workflow\n        return\n    # Local: Stop containers\n    self.stop_docker_compose()\n</code></pre>"},{"location":"testing/environment/#running-tests-locally","title":"Running Tests Locally","text":""},{"location":"testing/environment/#clean-start-strategy","title":"Clean Start Strategy","text":"<p>The test environment uses a \"clean start\" strategy for reliability:</p> <pre><code># Run all tests (starts fresh containers each time)\n./cli/ichrisbirch test run\n\n# Run specific tests\n./cli/ichrisbirch test run tests/ichrisbirch/api/endpoints/test_tasks.py\n\n# Run with verbose output\n./cli/ichrisbirch test run -v\n\n# With coverage\n./cli/ichrisbirch test cov\n</code></pre> <p>This approach provides:</p> <ul> <li>Reliability: Each test run gets a fresh environment</li> <li>Clean state: Database initialized from scratch each run</li> <li>No race conditions: Handles back-to-back runs (e.g., pre-commit hooks)</li> <li>Predictable behavior: Same behavior every time</li> </ul>"},{"location":"testing/environment/#how-clean-start-works","title":"How Clean Start Works","text":"<ol> <li>Every run: Full cleanup of any existing containers</li> <li>Wait: 3 seconds for volumes/networks to be released</li> <li>Start fresh: New containers with clean database</li> <li>After tests: Containers are stopped and cleaned up</li> </ol> <p>Trade-off: Each test run takes ~50 seconds for container startup. This time penalty is accepted for reliability, especially when running pre-commit hooks that execute multiple pytest sessions back-to-back.</p>"},{"location":"testing/environment/#network-isolation","title":"Network Isolation","text":"<p>Test and dev environments use separate proxy networks to avoid conflicts:</p> <ul> <li>Development: <code>proxy-dev</code> network</li> <li>Testing: <code>proxy-test</code> network</li> </ul> <p>This allows both environments to run simultaneously without Traefik routing conflicts.</p>"},{"location":"testing/environment/#manual-environment-management","title":"Manual Environment Management","text":"<p>For extended debugging sessions, you can manage the environment manually:</p> <pre><code># Start containers manually (stays running)\n./cli/ichrisbirch testing start\n\n# Run pytest directly (uses existing containers)\nuv run pytest tests/ichrisbirch/api/endpoints/test_tasks.py::test_create -v\n\n# Stop when done\n./cli/ichrisbirch testing stop\n</code></pre>"},{"location":"testing/environment/#direct-docker-compose-commands","title":"Direct Docker Compose Commands","text":"<pre><code># Stop containers\ndocker compose -f docker-compose.yml -f docker-compose.test.yml \\\n  --project-name icb-test down -v\n</code></pre>"},{"location":"testing/environment/#test-environment-vs-development-environment","title":"Test Environment vs Development Environment","text":"<p>The test and development environments can run simultaneously on different ports:</p> Service Dev Port Test Port PostgreSQL 5432 5434 Redis 6379 6380 API 8000 8001 App 5000 5001 Chat 8505 8507 Traefik HTTPS 443 8443 <p>This allows you to run tests without stopping your development environment.</p>"},{"location":"testing/environment/#ci-environment-behavior","title":"CI Environment Behavior","text":"<p>In GitHub Actions, the environment behaves differently:</p>"},{"location":"testing/environment/#workflow-pre-starts-containers","title":"Workflow Pre-starts Containers","text":"<p>The CI workflow starts containers before pytest runs:</p> <pre><code>- name: Start Docker Compose test environment\n  run: |\n    docker compose ... up -d --build postgres redis\n    sleep 10\n    docker compose ... up -d --build api app chat scheduler\n    sleep 30\n</code></pre>"},{"location":"testing/environment/#test-fixtures-skip-container-management","title":"Test Fixtures Skip Container Management","text":"<pre><code>if self.is_ci:\n    logger.info('Running in CI - containers should be pre-started')\n    # Just verify they're running, don't try to start\n</code></pre>"},{"location":"testing/environment/#ci-override-file","title":"CI Override File","text":"<p>The <code>docker-compose.ci.yml</code> file removes local-only configurations:</p> <ul> <li>No AWS credentials bind mount (uses env vars)</li> <li>Internal bridge network (not external)</li> <li>Traefik dashboard disabled</li> </ul>"},{"location":"testing/environment/#database-configuration","title":"Database Configuration","text":""},{"location":"testing/environment/#in-memory-database","title":"In-Memory Database","text":"<p>The test environment uses tmpfs for PostgreSQL:</p> <pre><code>postgres:\n  tmpfs:\n    - /tmp/postgres\n  command: &gt;\n    postgres\n    -c fsync=off\n    -c synchronous_commit=off\n    -c full_page_writes=off\n</code></pre> <p>This provides:</p> <ul> <li>Fast writes (no disk I/O)</li> <li>Fresh database each run</li> <li>No persistence between runs</li> </ul>"},{"location":"testing/environment/#test-users","title":"Test Users","text":"<p>The fixture <code>insert_users_for_login</code> creates test users:</p> User Email Role Purpose Test User testlogin@test.com User Regular user tests Test Admin testloginadmin@testadmin.com Admin Admin-only tests"},{"location":"testing/environment/#health-checks","title":"Health Checks","text":""},{"location":"testing/environment/#service-health-check","title":"Service Health Check","text":"<p>The environment waits for services to be healthy:</p> <pre><code>def docker_test_services_already_running(self, required_services=None):\n    \"\"\"Check if required Docker services are running.\"\"\"\n    if required_services is None:\n        required_services = ['postgres', 'redis', 'api', 'app']\n    # Check each service status\n</code></pre>"},{"location":"testing/environment/#database-health-check","title":"Database Health Check","text":"<pre><code>def ensure_database_ready(self):\n    \"\"\"Wait for database to accept connections and create tables.\"\"\"\n    # Retry connection with backoff\n    # Create all SQLAlchemy tables\n    # Insert test users\n</code></pre>"},{"location":"testing/environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing/environment/#containers-not-starting","title":"Containers Not Starting","text":"<pre><code># Check container status\ndocker compose -f docker-compose.yml -f docker-compose.test.yml \\\n  --project-name icb-test ps\n\n# View logs\ndocker compose -f docker-compose.yml -f docker-compose.test.yml \\\n  --project-name icb-test logs\n</code></pre>"},{"location":"testing/environment/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># Test database connection\ndocker compose exec postgres psql -U postgres -d ichrisbirch -c \"SELECT 1\"\n\n# Check postgres logs\ndocker compose logs postgres\n</code></pre>"},{"location":"testing/environment/#port-conflicts","title":"Port Conflicts","text":"<p>If ports 5434, 6380, etc. are in use:</p> <pre><code># Find what's using the port\nlsof -i :5434\n\n# Stop conflicting process or change test ports\n</code></pre>"},{"location":"testing/environment/#ci-failures","title":"CI Failures","text":"<p>Check the CI workflow logs:</p> <pre><code># View failed job logs\ngh run view &lt;run-id&gt; --log-failed\n\n# Or use the tracking script\n./scripts/track-gh-actions-workflow.sh logs\n</code></pre>"},{"location":"testing/environment/#configuration-files","title":"Configuration Files","text":"File Purpose <code>docker-compose.yml</code> Base service definitions <code>docker-compose.test.yml</code> Test-specific overrides (ports, tmpfs) <code>docker-compose.ci.yml</code> CI-specific overrides (no local mounts) <code>tests/environment.py</code> Test environment management <code>tests/conftest.py</code> Pytest fixtures <code>tests/utils/database.py</code> Database utilities and test settings"},{"location":"testing/environment/#fixtures-reference","title":"Fixtures Reference","text":""},{"location":"testing/environment/#session-scoped-run-once-per-test-session","title":"Session-Scoped (run once per test session)","text":"Fixture Purpose <code>setup_test_environment</code> Start Docker Compose, create tables <code>create_drop_tables</code> Manage table lifecycle <code>insert_users_for_login</code> Create test users"},{"location":"testing/environment/#module-scoped-run-once-per-test-file","title":"Module-Scoped (run once per test file)","text":"Fixture Purpose <code>test_api</code> Unauthenticated API client <code>test_api_logged_in</code> Authenticated regular user <code>test_api_logged_in_admin</code> Authenticated admin user <code>test_app</code> Flask test client <code>test_app_logged_in</code> Flask client with session"},{"location":"testing/environment/#function-scoped-run-once-per-test","title":"Function-Scoped (run once per test)","text":"<p>Same fixtures with <code>_function</code> suffix for test isolation.</p>"},{"location":"testing/fixtures/","title":"Test Fixtures","text":"<p>This document details the test fixtures available in the ichrisbirch project's testing infrastructure. These fixtures provide standardized ways to set up test prerequisites and resources.</p>"},{"location":"testing/fixtures/#fixture-scopes","title":"Fixture Scopes","text":"<p>The project uses fixtures at different scopes to optimize test execution:</p> <p></p>"},{"location":"testing/fixtures/#session-fixtures","title":"Session Fixtures","text":"<p>Session fixtures run once per test session and are available throughout the testing process:</p>"},{"location":"testing/fixtures/#setup_test_environment","title":"<code>setup_test_environment</code>","text":"<ul> <li>Scope: Session</li> <li>Autouse: Yes</li> <li>Description: Sets up the entire test environment including Docker containers and server processes</li> <li>Implementation: Defined in <code>conftest.py</code> and calls the <code>TestEnvironment.setup()</code> method</li> </ul> setup_test_environment<pre><code>@pytest.fixture(scope='session', autouse=True)\ndef setup_test_environment(request):\n    \"\"\"Set up the test environment with parallel execution support.\n\n    When running with pytest-xdist, multiple workers start simultaneously.\n    This fixture uses file locking to ensure only one worker performs the\n    actual Docker/database setup, while others wait for completion.\n\n    For normal (non-parallel) execution, this works as before.\n\n    Note: Docker containers are NOT torn down during parallel runs to avoid\n    killing the environment while other workers are still using it.\n    \"\"\"\n    worker_id = getattr(request.config, 'workerinput', {}).get('workerid', 'main')\n    is_parallel = worker_id != 'main'\n\n    logger.warning('')\n    logger.warning(f'{\"=\" * 30}&gt;  STARTING TESTING [{worker_id}]  &lt;{\"=\" * 30}')\n    logger.warning('')\n\n    TEST_LOCK_DIR.mkdir(parents=True, exist_ok=True)\n\n    if is_parallel:\n        lock = filelock.FileLock(str(TEST_LOCK_FILE), timeout=300)\n        with lock:\n            if not TEST_READY_FILE.exists():\n                logger.info(f'Worker {worker_id}: Performing environment setup (first to acquire lock)')\n                test_env = DockerComposeTestEnvironment(test_settings, create_session)\n                test_env.setup()\n                TEST_READY_FILE.touch()\n                logger.info(f'Worker {worker_id}: Environment setup complete, marker created')\n            else:\n                logger.info(f'Worker {worker_id}: Environment already set up by another worker')\n\n        # Wait for ready file (in case another worker is still setting up)\n        timeout = 300\n        start = time.time()\n        while not TEST_READY_FILE.exists() and (time.time() - start) &lt; timeout:\n            logger.info(f'Worker {worker_id}: Waiting for environment to be ready...')\n            time.sleep(2)\n\n        if not TEST_READY_FILE.exists():\n            pytest.exit(f'Worker {worker_id}: Timeout waiting for test environment', returncode=1)\n\n        # Yield None - environment is shared\n        yield None\n\n        # Don't teardown in parallel mode - containers are shared across workers\n        logger.info(f'Worker {worker_id}: Skipping teardown (parallel mode)')\n    else:\n        with DockerComposeTestEnvironment(test_settings, create_session) as test_env:\n            logger.info('Docker Compose test environment is ready')\n            yield test_env\n\n    # Cleanup lock files on non-parallel exit\n    if not is_parallel:\n        TEST_READY_FILE.unlink(missing_ok=True)\n\n    logger.warning('')\n    logger.warning(f'{\"=\" * 30}&gt;  TESTING FINISHED [{worker_id}]  &lt;{\"=\" * 30}')\n    logger.warning('')\n</code></pre>"},{"location":"testing/fixtures/#module-fixtures","title":"Module Fixtures","text":"<p>Module fixtures run once per test module:</p>"},{"location":"testing/fixtures/#create_drop_tables","title":"<code>create_drop_tables</code>","text":"<ul> <li>Scope: Module</li> <li>Description: Creates tables in the test database at the beginning of a test module and drops them when the module completes</li> <li>Implementation: Uses SQLAlchemy's metadata to create and drop tables</li> </ul>"},{"location":"testing/fixtures/#insert_users_for_login","title":"<code>insert_users_for_login</code>","text":"<ul> <li>Scope: Module</li> <li>Autouse: Yes</li> <li>Description: Inserts test users needed for login authentication</li> <li>Dependencies: <code>create_drop_tables</code></li> <li>Implementation: Inserts predefined test users from <code>get_test_login_users()</code></li> </ul>"},{"location":"testing/fixtures/#test-client-fixtures-module-scope","title":"Test Client Fixtures (Module Scope)","text":"<p>Various test client fixtures provide access to the API and App with different authentication levels:</p> Fixture Name Description Authentication <code>test_api</code> Basic API test client None <code>test_api_logged_in</code> API client with regular user authentication Regular user <code>test_api_logged_in_admin</code> API client with admin authentication Admin user <code>test_app</code> Basic App test client None <code>test_app_logged_in</code> App client with regular user authentication Regular user <code>test_app_logged_in_admin</code> App client with admin authentication Admin user"},{"location":"testing/fixtures/#test_jobstore","title":"<code>test_jobstore</code>","text":"<ul> <li>Scope: Module</li> <li>Description: Provides access to the APScheduler job store for testing scheduler functionality</li> <li>Implementation: Calls <code>get_jobstore()</code> with test settings</li> </ul>"},{"location":"testing/fixtures/#function-fixtures","title":"Function Fixtures","text":"<p>Function fixtures run once per test function:</p>"},{"location":"testing/fixtures/#test-client-fixtures-function-scope","title":"Test Client Fixtures (Function Scope)","text":"<p>These fixtures are function-scoped versions of the module-scoped test client fixtures:</p> Fixture Name Description Authentication <code>test_api_function</code> Function-scoped basic API client None <code>test_api_logged_in_function</code> Function-scoped API client with regular user auth Regular user <code>test_api_logged_in_admin_function</code> Function-scoped API client with admin auth Admin user <code>test_app_function</code> Function-scoped basic App client None <code>test_app_logged_in_function</code> Function-scoped App client with regular user auth Regular user <code>test_app_logged_in_admin_function</code> Function-scoped App client with admin auth Admin user"},{"location":"testing/fixtures/#test-data-fixtures","title":"Test Data Fixtures","text":"<p>Test data fixtures handle the insertion and cleanup of test data:</p>"},{"location":"testing/fixtures/#insert_testing_data","title":"<code>insert_testing_data</code>","text":"<ul> <li>Autouse: Often set to <code>True</code> in individual test modules</li> <li>Description: Inserts specific test datasets and cleans them up after tests</li> <li>Implementation: Uses <code>insert_test_data()</code> and <code>delete_test_data()</code> functions</li> <li>Example:</li> </ul> <p>k</p> insert_testing_data<pre><code>def insert_test_data(*datasets):\n    test_data = get_test_data()\n    selected_datasets = [deepcopy(test_data[key]['data']) for key in datasets]\n\n    with create_session(test_settings) as session:\n        for data in selected_datasets:\n            session.add_all(data)\n        session.commit()\n    for d in datasets:\n        logger.info(f'inserted testing dataset: {d}')\n</code></pre>"},{"location":"testing/fixtures/#insert_jobs_in_test_scheduler","title":"<code>insert_jobs_in_test_scheduler</code>","text":"<ul> <li>Description: Adds test jobs to the scheduler for testing scheduler functionality</li> <li>Implementation: Creates a test scheduler and adds jobs from test data</li> </ul>"},{"location":"testing/fixtures/#user-fixtures","title":"User Fixtures","text":"<ul> <li><code>test_user</code>: Provides a test user instance</li> <li><code>test_admin_user</code>: Provides a test admin user instance</li> </ul>"},{"location":"testing/fixtures/#using-fixtures-in-tests","title":"Using Fixtures in Tests","text":"<p>To use fixtures in tests, you can either:</p> <ol> <li>Parameter Injection: Add the fixture name as a parameter to your test function</li> </ol> <pre><code>def test_api_endpoint(test_api_logged_in):\n    response = test_api_logged_in.get('/some-endpoint/')\n    assert response.status_code == 200\n</code></pre> <ol> <li>Autouse: Mark fixtures with <code>autouse=True</code> to apply them automatically to all tests in scope</li> </ol> <pre><code>@pytest.fixture(autouse=True)\ndef insert_testing_data():\n    insert_test_data('tasks')\n    yield\n    delete_test_data('tasks')\n</code></pre>"},{"location":"testing/fixtures/#fixture-dependencies","title":"Fixture Dependencies","text":"<p>Fixtures can depend on other fixtures to build complex test setups. For example:</p> <pre><code>@pytest.fixture\ndef test_feature(test_api_logged_in, insert_testing_data):\n    # This fixture depends on both an authenticated API client\n    # and test data being available\n    ...\n</code></pre>"},{"location":"testing/fixtures/#dynamic-fixture-analysis","title":"Dynamic Fixture Analysis","text":"<p>The diagram at the top of this page is generated dynamically from the actual fixture code in the project. This ensures it always reflects the current state of the testing infrastructure. Additional visualizations of the fixture hierarchy:</p> <ul> <li>Fixture Scope Hierarchy - Shows the pytest fixture scope levels</li> <li>Fixture Dependencies - Dependencies between fixtures</li> <li>Comprehensive Fixture View - Complete view of all fixtures</li> </ul>"},{"location":"testing/fixtures/#best-practices","title":"Best Practices","text":"<ol> <li>Use the appropriate fixture scope to optimize test performance</li> <li>Keep fixtures focused on a single responsibility</li> <li>Use <code>autouse=True</code> sparingly to avoid confusion</li> <li>Document fixture dependencies clearly</li> <li>Clean up test data in fixture teardown phases</li> </ol>"},{"location":"testing/overview/","title":"Testing Infrastructure Overview","text":"<p>This document provides a comprehensive overview of the testing infrastructure in the ichrisbirch project. The testing infrastructure is designed to support automated testing of both the FastAPI-based backend API and the Flask-based frontend application.</p>"},{"location":"testing/overview/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Testing Architecture</li> <li>Test Environment Setup</li> <li>Test Data Management</li> <li>Test Clients</li> <li>Fixtures</li> <li>Writing Tests</li> <li>Related Documentation</li> </ol>"},{"location":"testing/overview/#testing-architecture","title":"Testing Architecture","text":"<p>The testing infrastructure follows a layered approach:</p> <p></p> <p>The architecture ensures that:</p> <ol> <li>Tests run in a controlled, isolated environment</li> <li>Both API and App servers are available for testing</li> <li>Test data is managed consistently</li> <li>Authentication and authorization can be tested effectively</li> </ol>"},{"location":"testing/overview/#test-environment-setup","title":"Test Environment Setup","text":"<p>The test environment is managed by the <code>TestEnvironment</code> class, which handles:</p> <ol> <li>Creating a PostgreSQL container in Docker</li> <li>Starting the FastAPI server in a separate process</li> <li>Starting the Flask server in a separate process</li> <li>Setting up database schemas</li> <li>Managing the lifecycle of these components</li> </ol> <p>This ensures that tests run against real server instances, providing high confidence in test results.</p>"},{"location":"testing/overview/#test-data-management","title":"Test Data Management","text":"<p>Test data is managed through a set of modules in the <code>tests.test_data</code> package, with functions for:</p> <ol> <li>Inserting test data for specific datasets</li> <li>Deleting test data after tests</li> <li>Special handling for test users to support authentication</li> </ol>"},{"location":"testing/overview/#test-clients","title":"Test Clients","text":"<p>Test clients provide interfaces to interact with the API and App servers:</p> <ol> <li>API Test Client: Based on FastAPI's TestClient</li> <li>App Test Client: Based on Flask's FlaskClient and FlaskLoginClient</li> </ol> <p>Both support authenticated and anonymous requests.</p>"},{"location":"testing/overview/#fixtures","title":"Fixtures","text":"<p>The testing infrastructure provides fixtures at different scopes:</p> <ol> <li>Session-scoped: Environment setup/teardown</li> <li>Module-scoped: Database tables and user creation</li> <li>Function-scoped: Test clients with different authentication levels</li> </ol> <p>See the fixtures documentation for more details.</p>"},{"location":"testing/overview/#writing-tests","title":"Writing Tests","text":"<p>See the writing tests guide for information on how to write effective tests using this infrastructure.</p>"},{"location":"testing/overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Test Fixtures</li> <li>Test Data Management</li> <li>Test Environment Configuration</li> <li>Writing Tests</li> </ul>"},{"location":"testing/persistent-logging/","title":"Test Environment Logging","text":""},{"location":"testing/persistent-logging/#overview","title":"Overview","text":"<p>The test environment uses the same stdout-only logging architecture as development and production. All logs go to stdout, and Docker handles persistence via its json-file logging driver. This provides consistent behavior across all environments.</p>"},{"location":"testing/persistent-logging/#viewing-test-logs","title":"Viewing Test Logs","text":""},{"location":"testing/persistent-logging/#using-the-cli","title":"Using the CLI","text":"<p>The CLI provides persistent log viewing that automatically reconnects when containers restart:</p> <pre><code># View all test service logs\n./cli/ichrisbirch testing logs\n\n# View specific service logs\n./cli/ichrisbirch testing logs api\n./cli/ichrisbirch testing logs app\n./cli/ichrisbirch testing logs chat\n./cli/ichrisbirch testing logs scheduler\n</code></pre> <p>The logs command uses a watch loop (<code>cli/ichrisbirch:316-324</code>) that:</p> <ul> <li>Follows logs in real-time</li> <li>Automatically reconnects when containers restart</li> <li>Colorizes service names for readability</li> <li>Press Ctrl+C to exit</li> </ul>"},{"location":"testing/persistent-logging/#using-docker-directly","title":"Using Docker Directly","text":"<pre><code># Follow logs for a specific container\ndocker logs -f icb-test-api\n\n# View recent logs\ndocker logs --tail 100 icb-test-api\n\n# View logs since a specific time\ndocker logs --since 5m icb-test-api\n</code></pre>"},{"location":"testing/persistent-logging/#log-format","title":"Log Format","text":"<p>Test environment logs use the same structlog format as other environments. The format is controlled by the <code>LOG_FORMAT</code> environment variable (default: <code>console</code>).</p>"},{"location":"testing/persistent-logging/#console-format-default","title":"Console Format (Default)","text":"<pre><code>2026-01-13T15:30:45Z [info     ] test_user_created     filename=fixtures.py func_name=setup lineno=45 request_id=abc12345 user_id=1\n2026-01-13T15:30:46Z [debug    ] database_query        filename=session.py func_name=execute lineno=89 query=SELECT...\n</code></pre>"},{"location":"testing/persistent-logging/#json-format","title":"JSON Format","text":"<p>Set <code>LOG_FORMAT=json</code> in docker-compose.test.yml to enable:</p> <pre><code>{\"timestamp\": \"2026-01-13T15:30:45Z\", \"level\": \"info\", \"event\": \"test_user_created\", \"filename\": \"fixtures.py\", \"func_name\": \"setup\", \"lineno\": 45, \"request_id\": \"abc12345\", \"user_id\": 1}\n</code></pre>"},{"location":"testing/persistent-logging/#request-tracing-in-tests","title":"Request Tracing in Tests","text":"<p>Test logs include request IDs for tracing requests across services. When running integration tests that call the API through HTTP, you can trace the full request flow:</p> <pre><code># Make a request and note the request_id in logs\ncurl -v https://api.test.localhost:8443/tasks/\n# Response includes: X-Request-ID: abc12345\n\n# Search logs for that request\n./cli/ichrisbirch testing logs | grep abc12345\n</code></pre>"},{"location":"testing/persistent-logging/#log-persistence","title":"Log Persistence","text":"<p>Docker persists logs to disk via the json-file driver. Logs survive container restarts but are removed when containers are removed with <code>--volumes</code> flag.</p>"},{"location":"testing/persistent-logging/#accessing-raw-log-files","title":"Accessing Raw Log Files","text":"<pre><code># Find log file location for a container\ndocker inspect icb-test-api --format='{{.LogPath}}'\n\n# View raw log file (requires sudo on Linux)\nsudo cat /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log\n</code></pre>"},{"location":"testing/persistent-logging/#log-lifecycle","title":"Log Lifecycle","text":"Action Logs Preserved <code>testing restart</code> Yes <code>testing stop</code> Yes (until container removed) <code>testing stop</code> then <code>testing start</code> Yes Container recreated No (new container, new logs)"},{"location":"testing/persistent-logging/#debugging-test-failures","title":"Debugging Test Failures","text":""},{"location":"testing/persistent-logging/#viewing-logs-during-test-run","title":"Viewing Logs During Test Run","text":"<p>When using <code>test run</code>, containers stay running after tests complete. View logs immediately:</p> <pre><code># Run tests\n./cli/ichrisbirch test run tests/ichrisbirch/api/test_tasks.py\n\n# View logs after failure\n./cli/ichrisbirch testing logs api\n</code></pre>"},{"location":"testing/persistent-logging/#correlating-test-failures-with-logs","title":"Correlating Test Failures with Logs","text":"<ol> <li>Run tests with verbose output to get timestamps:</li> </ol> <pre><code>./cli/ichrisbirch test run -v\n</code></pre> <ol> <li>View logs around the failure time:</li> </ol> <pre><code>docker logs --since 1m icb-test-api\n</code></pre> <ol> <li>Search for error events:</li> </ol> <pre><code>./cli/ichrisbirch testing logs | grep -i error\n</code></pre>"},{"location":"testing/persistent-logging/#configuration","title":"Configuration","text":""},{"location":"testing/persistent-logging/#environment-variables","title":"Environment Variables","text":"<p>Test environment logging is configured in <code>docker-compose.test.yml</code>:</p> <pre><code>services:\n  api:\n    environment:\n      - LOG_FORMAT=${LOG_FORMAT:-console}\n      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}\n      - LOG_COLORS=true  # Force colors in containers\n</code></pre>"},{"location":"testing/persistent-logging/#log-level-for-tests","title":"Log Level for Tests","text":"<p>Tests typically run with <code>LOG_LEVEL=DEBUG</code> to capture all diagnostic information. Override for less verbose output:</p> <pre><code>LOG_LEVEL=INFO ./cli/ichrisbirch test run\n</code></pre>"},{"location":"testing/persistent-logging/#comparison-with-development","title":"Comparison with Development","text":"Aspect Development Testing Log destination stdout stdout Persistence Docker json-file Docker json-file CLI command <code>dev logs</code> <code>testing logs</code> Watch loop Yes Yes Log format console (default) console (default) Log level DEBUG (default) DEBUG (default)"},{"location":"testing/persistent-logging/#related-documentation","title":"Related Documentation","text":"<ul> <li>Logging Configuration - Full logging architecture details</li> <li>Test Environment Configuration - Test environment setup</li> <li>CLI Management Guide - CLI command reference</li> </ul>"},{"location":"testing/test_configuration/","title":"Testing Configuration","text":"<p>This document explains the updated approach to handling configuration in the testing environment.</p>"},{"location":"testing/test_configuration/#key-changes","title":"Key Changes","text":"<ol> <li>Eliminated the need for the <code>ENVIRONMENT</code> variable</li> <li>Moved to a single <code>.env</code> file approach</li> <li>Hardcoded test settings for more reliable testing</li> <li>Removed the environment-specific <code>.dev.env</code>, <code>.test.env</code>, and <code>.prod.env</code> files</li> </ol>"},{"location":"testing/test_configuration/#how-it-works","title":"How It Works","text":""},{"location":"testing/test_configuration/#test-settings","title":"Test Settings","text":"<p>Test settings are now defined in a centralized location:</p> <pre><code># In tests/utils/test_settings.py\nTEST_ENV_VARS = {\n    \"ENVIRONMENT\": \"testing\",\n    \"PROTOCOL\": \"http\",\n    # ... more hardcoded test settings\n}\n</code></pre> <p>These settings are used consistently throughout the test environment. This eliminates the need to set the <code>ENVIRONMENT</code> variable and makes tests more reliable.</p>"},{"location":"testing/test_configuration/#accessing-test-settings","title":"Accessing Test Settings","text":"<p>To get test settings, use:</p> <pre><code>from tests.utils.settings import get_test_settings\n\nsettings = get_test_settings()\n</code></pre> <p>This function returns a Settings object initialized with the hardcoded test values, not environment variables or <code>.env</code> files.</p>"},{"location":"testing/test_configuration/#production-settings","title":"Production Settings","text":"<p>In production, settings are now loaded from a single <code>.env</code> file in the project root, instead of using environment-specific files. This simplifies deployment and configuration.</p>"},{"location":"testing/test_configuration/#running-tests","title":"Running Tests","text":"<p>To run tests, you no longer need to set the <code>ENVIRONMENT</code> variable:</p> <pre><code># Old approach (no longer needed)\nENVIRONMENT=testing poetry run pytest\n\n# New approach\npoetry run pytest\n</code></pre> <p>Tests will automatically use the hardcoded test settings.</p>"},{"location":"testing/test_configuration/#benefits","title":"Benefits","text":"<ol> <li>Reliability: Tests are no longer affected by the environment they run in</li> <li>Simplicity: No need to manage multiple <code>.env</code> files</li> <li>Consistency: All tests use the same configuration</li> <li>Isolation: Test environment is isolated from production environment</li> </ol>"},{"location":"testing/test_configuration/#implementation-details","title":"Implementation Details","text":"<p>The key implementation changes are:</p> <ol> <li><code>get_settings()</code> function in <code>config.py</code> now accepts a <code>test_mode</code> parameter</li> <li>When <code>test_mode=True</code>, it uses hardcoded test settings instead of loading from <code>.env</code> files</li> <li>TestEnvironment class uses these settings for the test infrastructure</li> <li>All test fixtures and utilities use the same settings consistently</li> </ol>"},{"location":"testing/test_data/","title":"Test Data Management","text":"<p>This document explains how test data is organized, managed, and used in the ichrisbirch project testing infrastructure.</p>"},{"location":"testing/test_data/#test-data-structure","title":"Test Data Structure","text":"<p>The test data is organized into modules within the <code>tests.test_data</code> package, with each module responsible for providing data for a specific model or feature:</p> <p>Each module typically contains:</p> <ul> <li>A <code>BASE_DATA</code> constant containing instances of the relevant model</li> <li>Additional test data constants for specific test scenarios</li> <li>Helper functions for creating or modifying test data</li> </ul>"},{"location":"testing/test_data/#data-format","title":"Data Format","text":"<p>Test data is defined using direct model instantiation:</p> <pre><code>from datetime import datetime\nfrom ichrisbirch.models import Event\n\nBASE_DATA: list[Event] = [\n    Event(\n        name='Event 1',\n        date=datetime(2022, 10, 1, 10, 0).isoformat(),\n        venue='Venue 1',\n        url='https://example.com/event1',\n        cost=10.0,\n        attending=True,\n        notes='Notes for Event 1',\n    ),\n    # More events...\n]\n</code></pre>"},{"location":"testing/test_data/#test-data-registry","title":"Test Data Registry","text":"<p>The <code>get_test_data()</code> function in <code>tests/utils/database.py</code> provides a registry of all available test datasets:</p> <pre><code>def get_test_data() -&gt; Dict[str, Dict[str, Any]]:\n    return {\n        'articles': {'model': models.Article, 'data': tests.test_data.articles.BASE_DATA},\n        'autotasks': {'model': models.AutoTask, 'data': tests.test_data.autotasks.BASE_DATA},\n        'books': {'model': models.Book, 'data': tests.test_data.books.BASE_DATA},\n        # More datasets...\n    }\n</code></pre> <p>This registry maps dataset names to their corresponding models and data collections.</p>"},{"location":"testing/test_data/#data-management-functions","title":"Data Management Functions","text":""},{"location":"testing/test_data/#inserting-test-data","title":"Inserting Test Data","text":"<p>The <code>insert_test_data()</code> function inserts specific datasets into the database:</p> <pre><code>def insert_test_data(*datasets):\n    \"\"\"Insert testing data for specific datasets.\n\n    Args:\n        *datasets: Names of datasets to insert (e.g., 'tasks', 'users')\n    \"\"\"\n    test_data = get_test_data()\n    selected_datasets = [deepcopy(test_data[key]['data']) for key in datasets]\n\n    with TestSessionLocal() as session:\n        for data in selected_datasets:\n            session.add_all(data)\n        session.commit()\n</code></pre>"},{"location":"testing/test_data/#deleting-test-data","title":"Deleting Test Data","text":"<p>The <code>delete_test_data()</code> function removes specific datasets from the database:</p> <pre><code>def delete_test_data(*datasets):\n    \"\"\"Delete test data except login users.\n\n    Args:\n        *datasets: Names of datasets to delete (e.g., 'tasks', 'users')\n    \"\"\"\n    # Implementation details...\n</code></pre> <p>This function includes special handling for user data to preserve login users.</p>"},{"location":"testing/test_data/#using-test-data-in-fixtures","title":"Using Test Data in Fixtures","text":"<p>Test data is typically inserted and deleted using fixtures:</p> <pre><code>@pytest.fixture(autouse=True)\ndef insert_testing_data():\n    insert_test_data('tasks')\n    yield\n    delete_test_data('tasks')\n</code></pre> <p>This pattern ensures that:</p> <ol> <li>The required test data is available before tests run</li> <li>Test data is cleaned up after tests complete</li> <li>Tests start with a consistent, known state</li> </ol>"},{"location":"testing/test_data/#multiple-dataset-dependencies","title":"Multiple Dataset Dependencies","text":"<p>For tests requiring multiple related datasets, you can insert them together:</p> <pre><code>@pytest.fixture(autouse=True)\ndef insert_testing_data():\n    insert_test_data('habitcategories', 'habits', 'habitscompleted')\n    yield\n    delete_test_data('habits', 'habitscompleted', 'habitcategories')\n</code></pre> <p>Note that when deleting related datasets, you must consider foreign key relationships and delete in the correct order.</p>"},{"location":"testing/test_data/#login-users","title":"Login Users","text":"<p>Special handling exists for test login users:</p> <pre><code>def get_test_login_users() -&gt; List[Dict[str, Any]]:\n    \"\"\"Return a list of test users for login testing.\"\"\"\n    settings = get_test_settings()\n\n    return [\n        {\n            'name': 'Test User to be Sacrificed for Delete Test',\n            'email': 'sacrifice@testgods.com',\n            'password': 'repentance',\n        },\n        {\n            'name': 'Test Login Regular User',\n            'email': 'testloginregular@testuser.com',\n            'password': 'regularpassword',\n        },\n        {\n            'name': 'Test Login Admin User',\n            'email': 'testloginadmin@testadmin.com',\n            'password': 'adminpassword',\n            'is_admin': True,\n        },\n        {\n            'name': settings.users.service_account_user_name,\n            'email': settings.users.service_account_user_email,\n            'password': settings.users.service_account_user_password,\n        },\n    ]\n</code></pre> <p>These users are:</p> <ol> <li>Inserted at the module level via the <code>insert_users_for_login</code> fixture</li> <li>Preserved when deleting user test data</li> <li>Available for authentication in tests</li> </ol>"},{"location":"testing/test_data/#best-practices-for-test-data","title":"Best Practices for Test Data","text":"<ol> <li>Keep test data minimal: Include only the fields necessary for tests</li> <li>Use realistic values: Test data should represent real-world scenarios</li> <li>Avoid dependencies: When possible, make test data self-contained</li> <li>Document relationships: When test data has dependencies, document them</li> <li>Consider database constraints: Ensure test data satisfies database constraints</li> <li>Handle cleanup: Always clean up test data after tests</li> <li>Use consistent IDs: When IDs matter, use explicit, consistent IDs</li> <li>Reset sequences: Reset ID sequences after deleting test data</li> </ol>"},{"location":"testing/testing/","title":"Testing Guide","text":"<p>This document provides a high-level overview of the testing system in the ichrisbirch project. For more detailed information, please refer to the specific documentation files linked below.</p>"},{"location":"testing/testing/#comprehensive-testing-documentation","title":"Comprehensive Testing Documentation","text":"<p>We now have detailed documentation for the testing infrastructure:</p> <ol> <li>Testing Overview - The big picture of the testing infrastructure</li> <li>Test Fixtures - Documentation of available test fixtures and their usage</li> <li>Test Data Management - How test data is organized and managed</li> <li>Test Environment Configuration - Details of the test environment setup</li> <li>Writing Tests - Guide to writing effective tests</li> </ol>"},{"location":"testing/testing/#running-tests","title":"Running Tests","text":"<p>In order to run pytest, you have to set <code>ENVIRONMENT=development</code> so that the config can pick it up and set the correct variables. Note: Config is not actually setting anything in tests, but the config is called in some of the files that are imported and it will error if not set.</p>"},{"location":"testing/testing/#dev-testing-on-mac","title":"Dev Testing on Mac","text":"<ul> <li>Make sure to change <code>/etc/hosts</code> file:   <code>127.0.0.1   localhost</code> --&gt; <code>127.0.0.1 localhost api.localhost books.localhost</code></li> </ul> <p>Docroot is: /usr/local/var/www</p> <p>The default port has been set in /usr/local/etc/nginx/nginx.conf to 8080 so that nginx can run without sudo.</p> <p>nginx will load all files in /usr/local/etc/nginx/servers/.</p> <p>To restart nginx after an upgrade:   brew services restart nginx Or, if you don't want/need a background service you can just run:   /usr/local/opt/nginx/bin/nginx -g daemon off;</p>"},{"location":"testing/testing/#pytest-xdist","title":"<code>pytest-xdist</code>","text":"<p>This plugin does not work with the current configuration (08/28/2024) using a local Docker Postgres and running the app, api, and postgres in a separate thread. <code>pytest-xdist</code> bypasses the start of the docker container and all tests fail.</p>"},{"location":"testing/visualization_options/","title":"Diagram Visualization Options","text":"<p>This document discusses alternatives to Mermaid for creating and visualizing diagrams in documentation, with a focus on options that work well within the Python ecosystem.</p>"},{"location":"testing/visualization_options/#current-solution-mermaid","title":"Current Solution: Mermaid","text":"<p>Mermaid is currently used for diagrams in our documentation. It offers:</p> <ul> <li>Markdown integration</li> <li>Simple text-based diagram definition</li> <li>Support in GitHub and many Markdown viewers</li> <li>Multiple diagram types (flowcharts, sequence diagrams, etc.)</li> </ul> <p>However, Mermaid has some limitations:</p> <ul> <li>Limited layout control</li> <li>Can produce hard-to-read diagrams with complex relationships</li> <li>Limited styling options</li> <li>Performance issues with large diagrams</li> </ul>"},{"location":"testing/visualization_options/#python-based-alternatives","title":"Python-Based Alternatives","text":""},{"location":"testing/visualization_options/#1-graphviz-with-python-bindings","title":"1. Graphviz with Python Bindings","text":"<p>Graphviz is a powerful graph visualization software with excellent Python bindings.</p>"},{"location":"testing/visualization_options/#installation","title":"Installation","text":"<pre><code># Install Graphviz system package\nbrew install graphviz  # macOS\n# or\napt-get install graphviz  # Ubuntu/Debian\n\n# Install Python bindings\npip install graphviz\n</code></pre>"},{"location":"testing/visualization_options/#benefits","title":"Benefits","text":"<ul> <li>Much better automatic layout algorithms than Mermaid</li> <li>Multiple layout engines for different graph types</li> <li>Fine-grained control over node and edge appearance</li> <li>Can generate SVG, PNG, PDF, and other formats</li> <li>Highly optimized for complex diagrams</li> </ul>"},{"location":"testing/visualization_options/#example","title":"Example","text":"<pre><code>from graphviz import Digraph\n\ndot = Digraph(comment='Test Fixtures')\n\n# Create nodes\ndot.node('SessionFixtures', 'Session Fixtures')\ndot.node('ModuleFixtures', 'Module Fixtures')\ndot.node('FunctionFixtures', 'Function Fixtures')\n\n# Create edges\ndot.edge('SessionFixtures', 'ModuleFixtures')\ndot.edge('ModuleFixtures', 'FunctionFixtures')\n\n# Save and render\ndot.render('fixtures_hierarchy', format='svg')\n</code></pre>"},{"location":"testing/visualization_options/#2-diagrams-library","title":"2. Diagrams Library","text":"<p>Diagrams is a Python library for creating cloud system architecture diagrams.</p>"},{"location":"testing/visualization_options/#diagrams-installation","title":"Diagrams Installation","text":"<pre><code>pip install diagrams\n</code></pre>"},{"location":"testing/visualization_options/#diagrams-benefits","title":"Diagrams Benefits","text":"<ul> <li>Specifically designed for system architecture diagrams</li> <li>Clean, professional-looking output</li> <li>Easy to use Python API</li> <li>Many pre-defined icons and resources</li> <li>Good for deployment and infrastructure documentation</li> </ul>"},{"location":"testing/visualization_options/#diagrams-example","title":"Diagrams Example","text":"<pre><code>from diagrams import Diagram, Cluster\nfrom diagrams.programming.framework import Flask, FastAPI\nfrom diagrams.programming.language import Python\n\nwith Diagram(\"Test Environment\", show=False):\n    with Cluster(\"Test Services\"):\n        api = FastAPI(\"API Server\")\n        app = Flask(\"App Server\")\n        db = Python(\"Database\")\n\n    api &gt;&gt; db\n    app &gt;&gt; db\n</code></pre>"},{"location":"testing/visualization_options/#3-networkx-with-matplotlibplotly","title":"3. NetworkX with Matplotlib/Plotly","text":"<p>NetworkX is a Python package for complex network analysis with visualization capabilities.</p>"},{"location":"testing/visualization_options/#networkx-installation","title":"NetworkX Installation","text":"<pre><code>pip install networkx matplotlib plotly\n</code></pre>"},{"location":"testing/visualization_options/#networkx-benefits","title":"NetworkX Benefits","text":"<ul> <li>Highly customizable graph layouts</li> <li>Interactive visualizations with Plotly</li> <li>Excellent for relationship-heavy diagrams</li> <li>Can perform advanced network analysis</li> <li>Integration with data science tools</li> </ul>"},{"location":"testing/visualization_options/#networkx-example","title":"NetworkX Example","text":"<pre><code>import networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes with groups\nnodes = [\n    (\"session_env\", {\"group\": \"session\"}),\n    (\"module_tables\", {\"group\": \"module\"}),\n    (\"module_users\", {\"group\": \"module\"}),\n    (\"function_client\", {\"group\": \"function\"}),\n]\nG.add_nodes_from(nodes)\n\n# Add edges\nedges = [\n    (\"session_env\", \"module_tables\"),\n    (\"module_tables\", \"module_users\"),\n    (\"module_users\", \"function_client\"),\n]\nG.add_edges_from(edges)\n\n# Set positions using a layout algorithm\npos = nx.spring_layout(G)\n\n# Draw the graph\nplt.figure(figsize=(10, 8))\n\n# Draw nodes\ncolors = ['lightblue', 'lightgreen', 'lightcoral']\nfor i, group in enumerate(['session', 'module', 'function']):\n    nx.draw_networkx_nodes(\n        G, pos,\n        nodelist=[n for n, attr in G.nodes(data=True) if attr.get('group') == group],\n        node_color=colors[i],\n        node_size=1500,\n        alpha=0.8,\n        label=f\"{group} fixtures\"\n    )\n\n# Draw edges and labels\nnx.draw_networkx_edges(G, pos, width=2, edge_color='gray')\nnx.draw_networkx_labels(G, pos, font_size=12, font_family='sans-serif')\n\nplt.title(\"Test Fixture Hierarchy\")\nplt.axis('off')\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"fixtures_hierarchy.png\", format=\"PNG\", dpi=300)\nplt.show()\n</code></pre>"},{"location":"testing/visualization_options/#4-pygraphviz","title":"4. PyGraphviz","text":"<p>PyGraphviz provides a Python interface to the Graphviz graph visualization software.</p>"},{"location":"testing/visualization_options/#pygraphviz-installation","title":"PyGraphviz Installation","text":"<pre><code># Install Graphviz system package first\nbrew install graphviz  # macOS\n# or\napt-get install graphviz  # Ubuntu/Debian\n\n# Then install PyGraphviz\npip install pygraphviz\n</code></pre>"},{"location":"testing/visualization_options/#pygraphviz-benefits","title":"PyGraphviz Benefits","text":"<ul> <li>Full access to all Graphviz features</li> <li>More direct control than the graphviz package</li> <li>Can handle very large graphs efficiently</li> <li>Good for programmatically generating complex diagrams</li> </ul>"},{"location":"testing/visualization_options/#pygraphviz-example","title":"PyGraphviz Example","text":"<pre><code>import pygraphviz as pgv\n\nG = pgv.AGraph(directed=True)\nG.graph_attr['label'] = 'Test Fixtures Hierarchy'\nG.node_attr['shape'] = 'box'\nG.edge_attr['color'] = 'blue'\n\n# Add nodes with styling\nG.add_node(\"Session\", style=\"filled\", fillcolor=\"lightblue\")\nG.add_node(\"Module\", style=\"filled\", fillcolor=\"lightgreen\")\nG.add_node(\"Function\", style=\"filled\", fillcolor=\"lightcoral\")\n\n# Add edges\nG.add_edge(\"Session\", \"Module\")\nG.add_edge(\"Module\", \"Function\")\n\n# Add subgraphs for clustering\nsession_cluster = G.add_subgraph([\"Session\"], name=\"cluster_session\", label=\"Session Fixtures\")\nmodule_cluster = G.add_subgraph([\"Module\"], name=\"cluster_module\", label=\"Module Fixtures\")\nfunction_cluster = G.add_subgraph([\"Function\"], name=\"cluster_function\", label=\"Function Fixtures\")\n\n# Layout and output\nG.layout(prog=\"dot\")\nG.draw(\"fixtures_hierarchy.svg\", format=\"svg\")\n</code></pre>"},{"location":"testing/visualization_options/#integration-with-documentation","title":"Integration with Documentation","text":""},{"location":"testing/visualization_options/#static-image-approach","title":"Static Image Approach","text":"<ol> <li>Generate the diagrams as static images (SVG or PNG)</li> <li>Include them in Markdown documentation with standard image syntax:</li> </ol> <pre><code>![Test Fixtures Hierarchy](../images/generated/fixtures_diagram.svg)\n</code></pre>"},{"location":"testing/visualization_options/#interactive-solution","title":"Interactive Solution","text":"<p>For interactive diagrams within documentation:</p> <ol> <li>Use Plotly to create interactive diagrams</li> <li>Export them as HTML files</li> <li>Create an iframe to embed them in documentation</li> </ol> <pre><code>&lt;iframe src=\"../images/interactive_fixtures.html\" width=\"100%\" height=\"600px\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"testing/visualization_options/#workflow-integration","title":"Workflow Integration","text":"<p>To integrate diagram generation into your workflow:</p> <ol> <li>Create a <code>diagrams</code> module in your project</li> <li>Implement Python scripts that generate diagrams</li> <li>Add a build step that generates diagrams before documentation builds</li> <li>Store generated images in a dedicated folder in docs</li> </ol> <p>Example script structure:</p> <pre><code>ichrisbirch/\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 diagrams/\n    \u2502   \u251c\u2500\u2500 generate_fixtures_diagram.py\n    \u2502   \u251c\u2500\u2500 generate_environment_diagram.py\n    \u2502   \u2514\u2500\u2500 generate_all.py\n    \u2514\u2500\u2500 images/\n        \u251c\u2500\u2500 fixtures_hierarchy.svg\n        \u251c\u2500\u2500 environment_setup.svg\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"testing/visualization_options/#recommendation","title":"Recommendation","text":"<p>Based on your requirements:</p> <ol> <li>For complex hierarchies and relationships: Use Graphviz with Python bindings</li> <li>For system architecture diagrams: Use the Diagrams library</li> <li>For interactive exploration: Use NetworkX with Plotly</li> </ol> <p>Graphviz is likely the best all-around solution for replacing the current Mermaid diagrams, as it provides much better automatic layout capabilities while still generating clean SVG output that can be included in documentation.</p>"},{"location":"testing/visualization_options/#getting-started","title":"Getting Started","text":"<p>To begin experimenting with Graphviz (recommended):</p> <ol> <li>Install Graphviz and the Python bindings:</li> </ol> <pre><code>brew install graphviz\npip install graphviz\n</code></pre> <ol> <li>Convert one of your existing Mermaid diagrams to Graphviz:</li> </ol> <pre><code>from graphviz import Digraph\n\n# Create a new directed graph\ndot = Digraph(comment='Test Fixtures')\ndot.attr(rankdir='TB')  # Top to bottom layout\n\n# Add nodes for fixture types\nwith dot.subgraph(name='cluster_session') as c:\n    c.attr(label='Session Fixtures')\n    c.node('setup_test_env', 'setup_test_environment')\n    c.node('test_env', 'TestEnvironment')\n    c.edge('setup_test_env', 'test_env')\n\n# Add more nodes and edges...\n\n# Save and render the graph\ndot.render('fixtures_hierarchy', format='svg')\n</code></pre> <ol> <li>Include the generated SVG in your documentation.</li> </ol>"},{"location":"testing/writing_tests/","title":"Writing Tests","text":"<p>This document provides guidance on writing effective tests using the ichrisbirch project testing infrastructure.</p>"},{"location":"testing/writing_tests/#test-organization","title":"Test Organization","text":"<p>Tests in the project are organized by component and functionality:</p> <p></p>"},{"location":"testing/writing_tests/#test-types","title":"Test Types","text":"<p>The project uses several types of tests:</p> <ol> <li>API Tests: Test API endpoints using FastAPI TestClient</li> <li>App Tests: Test Flask routes and views using FlaskClient</li> <li>Integration Tests: Test interactions between components</li> <li>Frontend Tests: Test UI interactions using Playwright</li> <li>Model Tests: Test model behavior and validation</li> <li>Unit Tests: Test individual functions and methods</li> </ol>"},{"location":"testing/writing_tests/#test-file-naming","title":"Test File Naming","text":"<p>Test files follow a consistent naming pattern:</p> <ul> <li>API tests: <code>tests/ichrisbirch/api/endpoints/test_*.py</code></li> <li>App tests: <code>tests/ichrisbirch/app/routes/test_*.py</code></li> <li>Model tests: <code>tests/ichrisbirch/models/test_*.py</code></li> <li>Utility tests: <code>tests/ichrisbirch/test_*.py</code> or <code>tests/utils/test_*.py</code></li> <li>Script tests: <code>tests/scripts/test_*.py</code></li> </ul>"},{"location":"testing/writing_tests/#basic-test-structure","title":"Basic Test Structure","text":"<p>A typical test file includes:</p> <ol> <li>Import statements</li> <li>Test data fixture setup</li> <li>Individual test functions</li> <li>Helper functions (if needed)</li> </ol> <p>Example:</p> <pre><code>import pytest\nfrom fastapi import status\n\nfrom tests.utils.database import delete_test_data\nfrom tests.utils.database import insert_test_data\n\n\n@pytest.fixture(autouse=True)\ndef insert_testing_data():\n    insert_test_data('events')\n    yield\n    delete_test_data('events')\n\n\ndef test_read_events(test_api_logged_in):\n    response = test_api_logged_in.get('/events/')\n    assert response.status_code == status.HTTP_200_OK\n    assert len(response.json()) &gt; 0\n\n\ndef test_create_event(test_api_logged_in):\n    # Test implementation...\n</code></pre>"},{"location":"testing/writing_tests/#testing-api-endpoints","title":"Testing API Endpoints","text":""},{"location":"testing/writing_tests/#basic-api-test","title":"Basic API Test","text":"<pre><code>def test_api_endpoint(test_api_logged_in):\n    response = test_api_logged_in.get('/endpoint/')\n    assert response.status_code == status.HTTP_200_OK\n    assert response.json() is not None\n</code></pre>"},{"location":"testing/writing_tests/#testing-crud-operations","title":"Testing CRUD Operations","text":"<p>The project includes a <code>ApiCrudTester</code> utility for testing CRUD operations:</p> <pre><code>from tests.ichrisbirch.api.endpoints.crud_test import ApiCrudTester\n\nNEW_OBJ = schemas.EventCreate(\n    name='Event 4',\n    date=datetime(2022, 10, 4, 20, 0),\n    venue='Venue 4',\n    url='https://example.com/event4',\n    cost=40.0,\n    attending=False,\n    notes='Notes for Event 4',\n)\n\nENDPOINT = '/events/'\n\ncrud_tests = ApiCrudTester(endpoint=ENDPOINT, new_obj=NEW_OBJ)\n\ndef test_read_one(test_api_logged_in):\n    crud_tests.test_read_one(test_api_logged_in)\n\ndef test_create(test_api_logged_in):\n    crud_tests.test_create(test_api_logged_in)\n</code></pre>"},{"location":"testing/writing_tests/#testing-app-routes","title":"Testing App Routes","text":""},{"location":"testing/writing_tests/#basic-app-test","title":"Basic App Test","text":"<pre><code>def test_app_route(test_app_logged_in):\n    response = test_app_logged_in.get('/route/')\n    assert response.status_code == status.HTTP_200_OK\n    assert b'Expected Content' in response.data\n</code></pre>"},{"location":"testing/writing_tests/#testing-form-submissions","title":"Testing Form Submissions","text":"<pre><code>def test_form_submission(test_app_logged_in):\n    form_data = {\n        'field1': 'value1',\n        'field2': 'value2',\n        'action': 'submit',\n    }\n    response = test_app_logged_in.post('/form-route/', data=form_data)\n    assert response.status_code == status.HTTP_200_OK\n</code></pre>"},{"location":"testing/writing_tests/#testing-frontend-with-playwright","title":"Testing Frontend with Playwright","text":"<pre><code>def test_ui_interaction(page: Page):\n    page.get_by_label('name').fill('Test Event')\n    page.get_by_label('date').fill('2050-01-01')\n    page.click('css=button[value=\"add\"]')\n\n    # Verify the result\n    expect(page).to_have_title('Events')\n</code></pre>"},{"location":"testing/writing_tests/#testing-model-functions","title":"Testing Model Functions","text":"<pre><code>def test_model_method(test_user):\n    result = test_user.validate_preferences(\n        key='preferences',\n        updated_preferences={'theme_color': 'blue'}\n    )\n    assert result is True  # or appropriate assertion\n</code></pre>"},{"location":"testing/writing_tests/#testing-authentication-and-authorization","title":"Testing Authentication and Authorization","text":""},{"location":"testing/writing_tests/#testing-with-different-user-types","title":"Testing with Different User Types","text":"<pre><code>def test_admin_only_endpoint(test_api_logged_in_admin):\n    response = test_api_logged_in_admin.get('/admin-only/')\n    assert response.status_code == status.HTTP_200_OK\n\ndef test_admin_only_endpoint_unauthorized(test_api_logged_in):\n    response = test_api_logged_in.get('/admin-only/')\n    assert response.status_code == status.HTTP_401_UNAUTHORIZED\n</code></pre>"},{"location":"testing/writing_tests/#parameterized-tests","title":"Parameterized Tests","text":"<p>Use <code>pytest.mark.parametrize</code> for testing multiple scenarios:</p> <pre><code>@pytest.mark.parametrize('category', list(TaskCategory))\ndef test_task_categories(test_api_logged_in, category):\n    test_task = schemas.TaskCreate(\n        name='Test Task',\n        notes='Test Notes',\n        category=category,\n        priority=3,\n    )\n    response = test_api_logged_in.post('/tasks/', json=test_task.model_dump())\n    assert response.status_code == status.HTTP_201_CREATED\n</code></pre>"},{"location":"testing/writing_tests/#testing-error-cases","title":"Testing Error Cases","text":"<pre><code>def test_invalid_input(test_api_logged_in):\n    invalid_data = {'incomplete': 'data'}\n    response = test_api_logged_in.post('/endpoint/', json=invalid_data)\n    assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY\n</code></pre>"},{"location":"testing/writing_tests/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Use <code>unittest.mock</code> or <code>pytest-mock</code> to mock external dependencies:</p> <pre><code>@patch('httpx.get')\ndef test_external_api(mock_httpx_get, test_api_logged_in):\n    # Configure the mock\n    mock_response = MagicMock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {'key': 'value'}\n    mock_httpx_get.return_value = mock_response\n\n    # Test the function that uses httpx.get\n    response = test_api_logged_in.get('/endpoint-using-external-api/')\n    assert response.status_code == status.HTTP_200_OK\n</code></pre>"},{"location":"testing/writing_tests/#testing-asynchronous-code","title":"Testing Asynchronous Code","text":"<p>For testing async functions:</p> <pre><code>import pytest_asyncio\n\n@pytest_asyncio.fixture\nasync def async_client():\n    # Setup async client...\n    yield client\n    # Teardown...\n\n@pytest.mark.asyncio\nasync def test_async_function(async_client):\n    result = await async_function()\n    assert result == expected_value\n</code></pre>"},{"location":"testing/writing_tests/#testing-database-interactions","title":"Testing Database Interactions","text":"<pre><code>def test_database_operation(test_api_logged_in):\n    # Create an entity\n    create_response = test_api_logged_in.post('/entity/', json={'name': 'Test'})\n    assert create_response.status_code == status.HTTP_201_CREATED\n    entity_id = create_response.json()['id']\n\n    # Verify it's in the database\n    get_response = test_api_logged_in.get(f'/entity/{entity_id}/')\n    assert get_response.status_code == status.HTTP_200_OK\n    assert get_response.json()['name'] == 'Test'\n</code></pre>"},{"location":"testing/writing_tests/#best-practices","title":"Best Practices","text":"<ol> <li>Test one thing per test function: Each test should focus on a single behavior or condition</li> <li>Use descriptive test names: Test names should clearly describe what they're testing</li> <li>Minimize test dependencies: Avoid tests that depend on the outcome of other tests</li> <li>Clean up test data: Always clean up any data created during tests</li> <li>Use appropriate fixtures: Use the right scope for fixtures to optimize test performance</li> <li>Test edge cases: Include tests for boundary conditions and error cases</li> <li>Use assertions effectively: Make assertions specific and informative</li> <li>Avoid test logic: Keep conditional logic in tests to a minimum</li> <li>Keep tests fast: Optimize tests to run quickly</li> <li>Make tests deterministic: Tests should produce the same result on each run</li> </ol>"},{"location":"testing/writing_tests/#troubleshooting-common-test-issues","title":"Troubleshooting Common Test Issues","text":"<ol> <li>Test data not available: Ensure you have the right data fixtures</li> <li>Authentication failures: Check that you're using the right test client</li> <li>Database errors: Verify that database transactions are being managed properly</li> <li>Flaky tests: Look for race conditions or external dependencies</li> <li>Slow tests: Consider using more focused fixtures or optimizing test logic</li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This comprehensive troubleshooting guide documents common issues, their root causes, attempted solutions, and final resolutions encountered during the development and operation of the iChrisBirch project.</p>"},{"location":"troubleshooting/#overview","title":"Overview","text":"<p>The troubleshooting documentation is organized by component and includes:</p> <ul> <li>Problem description: Clear statement of the issue</li> <li>Root cause analysis: Technical explanation of why the issue occurs</li> <li>Attempted solutions: What was tried and why it didn't work</li> <li>Final resolution: Working solution with implementation details</li> <li>Prevention: How to avoid the issue in the future</li> </ul>"},{"location":"troubleshooting/#quick-reference","title":"Quick Reference","text":""},{"location":"troubleshooting/#common-issues-by-component","title":"Common Issues by Component","text":"Component Common Issues Quick Solutions Docker Build failures, container networking Check Dockerfile syntax, network configuration Poetry to UV Migration Dependency management, virtual environments Follow migration checklist Testing Test failures, pytest not found, Docker issues Verify test dependencies, rebuild images Database Connection errors, schema issues Check connection strings, run migrations Development Environment Setup problems, tooling conflicts Follow setup guide step-by-step"},{"location":"troubleshooting/#recent-critical-issues-july-2025","title":"Recent Critical Issues (July 2025)","text":"<p>Docker Network Conflicts During Testing:</p> <ul> <li>Issue: Test runs fail with \"failed to set up container networking: network not found\" errors</li> <li>Resolution: Implemented comprehensive cleanup function with pre/post cleanup and multiple fallback strategies</li> <li>Details: Docker Network Conflicts</li> </ul> <p>Testing Infrastructure Failures:</p> <ul> <li><code>error: Failed to spawn: 'pytest'</code> \u2192 Missing <code>--group test</code> in Dockerfile</li> <li><code>service has neither an image nor a build context</code> \u2192 Add build directive to compose services  </li> <li><code>ModuleNotFoundError: tests.utils.environment</code> \u2192 Fix import paths in conftest.py</li> <li>Docker network conflicts \u2192 Run comprehensive Docker cleanup</li> </ul> <p>See Testing Issues - Recent Critical Issues for detailed solutions.</p>"},{"location":"troubleshooting/#emergency-fixes","title":"Emergency Fixes","text":"<p>For urgent production issues:</p> <ol> <li>Service Down: Check deployment issues</li> <li>Database Connection: See database troubleshooting</li> <li>Test Failures: Review testing diagnostics</li> </ol>"},{"location":"troubleshooting/#how-to-use-this-guide","title":"How to Use This Guide","text":"<ol> <li>Identify the component where the issue is occurring</li> <li>Check the quick reference for immediate solutions</li> <li>Read the detailed troubleshooting page for comprehensive guidance</li> <li>Follow the resolution steps with provided code examples</li> <li>Apply prevention measures to avoid future occurrences</li> </ol>"},{"location":"troubleshooting/#contributing","title":"Contributing","text":"<p>When you encounter and solve a new issue:</p> <ol> <li>Document it in the appropriate section</li> <li>Include error messages, logs, and code snippets</li> <li>Explain the root cause and why the solution works</li> <li>Add prevention tips for future reference</li> </ol>"},{"location":"troubleshooting/#legacy-issues","title":"Legacy Issues","text":"<p>Historical troubleshooting information has been migrated from the original troubleshooting.md file and integrated into the appropriate component-specific pages while maintaining all the original solutions and context.</p>"},{"location":"troubleshooting/cli-commands/","title":"CLI Command Troubleshooting","text":"<p>This guide covers common CLI issues and solutions, particularly focusing on the elimination of confusing command duplication in the iChrisBirch CLI.</p>"},{"location":"troubleshooting/cli-commands/#overview","title":"\ud83c\udfaf Overview","text":"<p>The CLI underwent a major simplification to eliminate confusing command duplication where multiple commands performed the same operation. This guide helps users adapt to the new simplified interface.</p>"},{"location":"troubleshooting/cli-commands/#common-cli-issues","title":"\ud83d\udea8 Common CLI Issues","text":""},{"location":"troubleshooting/cli-commands/#1-command-not-found-traefik-commands","title":"1. Command Not Found - traefik Commands","text":"<p>Problem: Trying to use removed <code>traefik-*</code> commands</p> <p>Error Messages:</p> <pre><code>$ ichrisbirch traefik start dev\nUnknown command: traefik\n\n$ ichrisbirch traefik status dev  \nUnknown command: traefik\n</code></pre> <p>Root Cause: All <code>traefik-*</code> commands were intentionally removed to eliminate confusing duplication</p> <p>Attempted Solutions (That Failed):</p> <ul> <li>Looking for <code>traefik</code> command in help output (it's no longer there)</li> <li>Trying variations like <code>ichrisbirch traefik-start dev</code> (doesn't exist)</li> <li>Assuming the CLI is broken (it's working correctly, just simplified)</li> </ul> <p>Resolution: Use the simplified commands instead</p> <pre><code># WRONG (removed commands)\nichrisbirch traefik start dev     \u2192 Use: ichrisbirch dev start\nichrisbirch traefik stop dev      \u2192 Use: ichrisbirch dev stop\nichrisbirch traefik restart dev   \u2192 Use: ichrisbirch dev restart\nichrisbirch traefik status dev    \u2192 Use: ichrisbirch dev status\nichrisbirch traefik logs dev      \u2192 Use: ichrisbirch dev logs\nichrisbirch traefik health dev    \u2192 Use: ichrisbirch dev health\n\n# CORRECT (current commands)\nichrisbirch dev start             # Starts dev with Traefik + HTTPS\nichrisbirch dev stop              # Stops dev environment  \nichrisbirch dev restart           # Restarts dev environment\nichrisbirch dev status            # Shows status + HTTPS URLs\nichrisbirch dev logs              # Shows service logs\nichrisbirch dev health            # Runs health checks\n</code></pre> <p>Prevention: Use <code>ichrisbirch help</code> to see all available commands. The simplified interface only shows actual working commands.</p>"},{"location":"troubleshooting/cli-commands/#2-confusion-about-which-command-to-use","title":"2. Confusion About Which Command to Use","text":"<p>Problem: Users confused about whether to use <code>dev start</code> or old <code>traefik start dev</code></p> <p>Root Cause: Historical documentation or muscle memory from the old duplicated interface</p> <p>Resolution: Always use the environment-first commands</p> <pre><code># SIMPLE RULE: Environment first, then action\nichrisbirch &lt;environment&gt; &lt;action&gt;\n\n# Examples:\nichrisbirch dev start         # Development environment\nichrisbirch testing start    # Testing environment  \nichrisbirch prod start       # Production environment\n</code></pre> <p>Benefits of Simplified Interface:</p> <ul> <li>\u2705 No confusion: Only one command per operation</li> <li>\u2705 Consistent patterns: Same structure across all environments</li> <li>\u2705 Hidden complexity: Users don't need to know about Traefik implementation</li> <li>\u2705 Professional UX: Follows industry-standard CLI design patterns</li> </ul>"},{"location":"troubleshooting/cli-commands/#3-ssl-manager-command-location","title":"3. SSL Manager Command Location","text":"<p>Problem: Looking for SSL commands under <code>traefik</code> namespace</p> <p>Error Messages:</p> <pre><code>$ ichrisbirch traefik ssl-manager generate dev\nUnknown command: traefik\n</code></pre> <p>Root Cause: SSL manager is now a top-level command, not under <code>traefik</code></p> <p>Resolution: Use ssl-manager as a top-level command</p> <pre><code># WRONG (old location)\nichrisbirch traefik ssl-manager generate dev\n\n# CORRECT (current location)\nichrisbirch ssl-manager generate dev\nichrisbirch ssl-manager info dev\nichrisbirch ssl-manager validate dev\n</code></pre> <p>Why This Changed: SSL management is a fundamental operation that applies to all environments, not just Traefik-specific functionality.</p>"},{"location":"troubleshooting/cli-commands/#4-documentation-references-to-old-commands","title":"4. Documentation References to Old Commands","text":"<p>Problem: Following outdated documentation that references <code>traefik-*</code> commands</p> <p>Root Cause: Documentation that wasn't updated after CLI simplification</p> <p>Resolution: Use the command translation guide</p> Old Command (Removed) New Command (Current) Description <code>ichrisbirch traefik start &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; start</code> Start environment <code>ichrisbirch traefik stop &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; stop</code> Stop environment <code>ichrisbirch traefik restart &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; restart</code> Restart environment <code>ichrisbirch traefik status &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; status</code> Environment status <code>ichrisbirch traefik logs &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; logs</code> View logs <code>ichrisbirch traefik health &lt;env&gt;</code> <code>ichrisbirch &lt;env&gt; health</code> Health checks <code>ichrisbirch traefik ssl-manager &lt;cmd&gt;</code> <code>ichrisbirch ssl-manager &lt;cmd&gt;</code> SSL management <p>Prevention: Always refer to the current CLI Management Guide for up-to-date command references.</p>"},{"location":"troubleshooting/cli-commands/#5-help-command-not-showing-expected-options","title":"5. Help Command Not Showing Expected Options","text":"<p>Problem: Looking for <code>traefik</code> in help output</p> <pre><code>$ ichrisbirch help\n# traefik commands not listed\n</code></pre> <p>Root Cause: This is correct behavior - simplified CLI only shows available commands</p> <p>Resolution: The help output is correct. Use environment-specific help</p> <pre><code># General help (shows all top-level commands)\nichrisbirch help\n\n# Environment-specific help\nichrisbirch dev help\n\n# SSL manager help\nichrisbirch ssl-manager help\n</code></pre> <p>Expected Help Output:</p> <pre><code>Available commands:\n  dev start/stop/restart/status/logs/health    # Development environment\n  testing start/stop/restart/status/logs/health # Testing environment\n  prod start/stop/restart/status/logs/health    # Production environment\n  ssl-manager generate/info/validate            # SSL certificate management\n</code></pre>"},{"location":"troubleshooting/cli-commands/#6-script-or-automation-using-old-commands","title":"6. Script or Automation Using Old Commands","text":"<p>Problem: Automated scripts failing due to removed commands</p> <p>Error Messages:</p> <pre><code>./deploy.sh: line 15: ichrisbirch traefik start dev: command failed\n</code></pre> <p>Root Cause: Scripts written for the old duplicated CLI interface</p> <p>Resolution: Update scripts to use simplified commands</p> <pre><code># WRONG (will fail)\n#!/bin/bash\nichrisbirch traefik start dev\nichrisbirch traefik health dev\n\n# CORRECT (current)\n#!/bin/bash\nichrisbirch dev start\nichrisbirch dev health\n</code></pre> <p>Prevention: Update all automation scripts and CI/CD pipelines to use the new simplified interface.</p>"},{"location":"troubleshooting/cli-commands/#advanced-cli-debugging","title":"\ud83d\udd27 Advanced CLI Debugging","text":""},{"location":"troubleshooting/cli-commands/#check-cli-installation","title":"Check CLI Installation","text":"<pre><code># Verify CLI is executable\nls -la ./cli/ichrisbirch\n\n# Make executable if needed\nchmod +x ./cli/ichrisbirch\n\n# Test basic functionality\n./cli/ichrisbirch help\n</code></pre>"},{"location":"troubleshooting/cli-commands/#verify-current-cli-version","title":"Verify Current CLI Version","text":"<pre><code># Check CLI script for version information\nhead -20 ./cli/ichrisbirch\n\n# Look for the function definitions to confirm simplified interface\ngrep -n \"function.*start\\|function.*traefik\" ./cli/ichrisbirch\n</code></pre>"},{"location":"troubleshooting/cli-commands/#debug-command-parsing","title":"Debug Command Parsing","text":"<pre><code># Enable bash debugging to see command parsing\nbash -x ./cli/ichrisbirch dev start\n\n# Check if commands are being recognized\n./cli/ichrisbirch dev help\n</code></pre>"},{"location":"troubleshooting/cli-commands/#cli-migration-guide","title":"\ud83d\ude80 CLI Migration Guide","text":""},{"location":"troubleshooting/cli-commands/#for-individual-users","title":"For Individual Users","text":"<ol> <li>Update muscle memory: Practice using <code>&lt;env&gt; &lt;action&gt;</code> pattern</li> <li>Update bookmarks: Replace any documented commands with new syntax</li> <li>Check scripts: Update any personal scripts or aliases</li> <li>Use help commands: Rely on <code>ichrisbirch help</code> for current command reference</li> </ol>"},{"location":"troubleshooting/cli-commands/#for-teams","title":"For Teams","text":"<ol> <li>Team training: Ensure all developers know about the simplified interface</li> <li>Update documentation: Review and update any team-specific documentation</li> <li>Update CI/CD: Modify deployment scripts and automation</li> <li>Code review: Check for old command usage in new scripts</li> </ol>"},{"location":"troubleshooting/cli-commands/#for-documentation","title":"For Documentation","text":"<ol> <li>Search and replace: Find all references to <code>traefik-*</code> commands</li> <li>Update examples: Replace with simplified command syntax</li> <li>Add migration notes: Include transition guidance for users</li> <li>Test all examples: Verify all documented commands actually work</li> </ol>"},{"location":"troubleshooting/cli-commands/#benefits-of-cli-simplification","title":"\ud83c\udf1f Benefits of CLI Simplification","text":""},{"location":"troubleshooting/cli-commands/#user-experience-improvements","title":"User Experience Improvements","text":"<p>Before (Confusing Duplication):</p> <pre><code># Users had to choose between equivalent commands\nichrisbirch traefik start dev    # Started dev environment\nichrisbirch dev start            # Also started dev environment (same result)\n\n# Problems:\n# - Confusion about which command to use\n# - Implementation details exposed (Traefik)\n# - Inconsistent command patterns\n# - More commands to remember\n</code></pre> <p>After (Clean &amp; Professional):</p> <pre><code># One command per operation\nichrisbirch dev start            # Clear, simple, consistent\n\n# Benefits:\n# - No confusion about which command to use\n# - Implementation details hidden\n# - Consistent patterns across all environments\n# - Fewer commands to remember\n</code></pre>"},{"location":"troubleshooting/cli-commands/#technical-benefits","title":"Technical Benefits","text":"<ol> <li>Reduced Cognitive Load: Fewer commands to learn and remember</li> <li>Better Discoverability: <code>help</code> command shows only working commands</li> <li>Consistent Patterns: Same structure for all environments</li> <li>Professional CLI Design: Follows industry-standard CLI conventions</li> <li>Easier Maintenance: Fewer code paths to maintain and test</li> </ol>"},{"location":"troubleshooting/cli-commands/#development-workflow-benefits","title":"Development Workflow Benefits","text":"<ol> <li>Faster Onboarding: New developers learn one simple pattern</li> <li>Reduced Errors: Fewer command options reduce user mistakes</li> <li>Better Documentation: Clearer examples without command duplication</li> <li>Improved Scripts: Simpler automation with consistent command patterns</li> </ol>"},{"location":"troubleshooting/cli-commands/#command-reference-quick-card","title":"\ud83d\udee0\ufe0f Command Reference Quick Card","text":""},{"location":"troubleshooting/cli-commands/#current-simplified-commands","title":"Current Simplified Commands","text":"<pre><code># Environment Management (all use Traefik + HTTPS automatically)\nichrisbirch dev start           # Start development\nichrisbirch dev stop            # Stop development  \nichrisbirch dev restart         # Restart development\nichrisbirch dev status          # Status + URLs\nichrisbirch dev logs            # View logs\nichrisbirch dev health          # Health checks\n\nichrisbirch testing start       # Start testing\nichrisbirch testing stop        # Stop testing\nichrisbirch testing restart     # Restart testing\nichrisbirch testing status      # Status + URLs\nichrisbirch testing logs        # View logs\nichrisbirch testing health      # Health checks\n\nichrisbirch prod start          # Start production\nichrisbirch prod stop           # Stop production\nichrisbirch prod restart        # Restart production\nichrisbirch prod status         # Status + URLs\nichrisbirch prod logs           # View logs\nichrisbirch prod health         # Health checks\n\n# SSL Certificate Management (top-level commands)\nichrisbirch ssl-manager generate dev    # Generate certificates\nichrisbirch ssl-manager info dev        # Certificate information\nichrisbirch ssl-manager validate dev    # Validate certificates\nichrisbirch ssl-manager help            # SSL manager help\n\n# Help Commands\nichrisbirch help                # General help\nichrisbirch dev help            # Development help\nichrisbirch ssl-manager help    # SSL help\n</code></pre>"},{"location":"troubleshooting/cli-commands/#removed-commands-do-not-use","title":"Removed Commands (Do Not Use)","text":"<pre><code># THESE COMMANDS NO LONGER EXIST:\nichrisbirch traefik start &lt;env&gt;     # REMOVED\nichrisbirch traefik stop &lt;env&gt;      # REMOVED\nichrisbirch traefik restart &lt;env&gt;   # REMOVED\nichrisbirch traefik status &lt;env&gt;    # REMOVED\nichrisbirch traefik logs &lt;env&gt;      # REMOVED\nichrisbirch traefik health &lt;env&gt;    # REMOVED\n</code></pre> <p>The CLI simplification provides a clean, professional interface that eliminates confusion and follows modern CLI design principles. The new simplified commands are more intuitive, easier to remember, and provide a better developer experience.</p>"},{"location":"troubleshooting/database-issues/","title":"Database Troubleshooting","text":"<p>This document covers database-related issues encountered during development, testing, and deployment of the iChrisBirch project.</p>"},{"location":"troubleshooting/database-issues/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/database-issues/#database-connection-refused","title":"Database Connection Refused","text":"<p>Problem: Application cannot connect to PostgreSQL database.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.OperationalError: connection to server at \"localhost\" (127.0.0.1) port 5432 refused</code></li> <li><code>FATAL: database \"ichrisbirch\" does not exist</code></li> <li><code>FATAL: role \"ichrisbirch\" does not exist</code></li> </ul> <p>Common Causes and Solutions:</p>"},{"location":"troubleshooting/database-issues/#1-wrong-database-host","title":"1. Wrong Database Host","text":"<pre><code># Wrong: Using localhost in containerized environment\nDATABASE_URL = \"postgresql://user:pass@localhost:5432/db\"\n\n# Correct: Using Docker service name\nDATABASE_URL = \"postgresql://user:pass@postgres:5432/db\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-database-service-not-ready","title":"2. Database Service Not Ready","text":"<p>Add health checks to docker-compose:</p> <pre><code>services:\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  app:\n    depends_on:\n      postgres:\n        condition: service_healthy\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-missing-database-or-user","title":"3. Missing Database or User","text":"<pre><code>-- Connect as superuser and create database/user\nCREATE USER ichrisbirch WITH PASSWORD 'password';\nCREATE DATABASE ichrisbirch OWNER ichrisbirch;\nGRANT ALL PRIVILEGES ON DATABASE ichrisbirch TO ichrisbirch;\n</code></pre>"},{"location":"troubleshooting/database-issues/#schema-and-table-issues","title":"Schema and Table Issues","text":"<p>Problem: Tables or schemas don't exist.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.errors.InvalidSchemaName: schema \"ichrisbirch_test\" does not exist</code></li> <li><code>sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \"users\" does not exist</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-run-database-migrations","title":"1. Run Database Migrations","text":"<pre><code># In development\ndocker-compose exec app uv run alembic upgrade head\n\n# In testing\ndocker-compose -f docker-compose.test.yml run test-runner uv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-set-schema-environment-variable","title":"2. Set Schema Environment Variable","text":"<pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    environment:\n      - POSTGRES_DB_SCHEMA=ichrisbirch_test\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-create-schema-manually","title":"3. Create Schema Manually","text":"<pre><code>-- Connect to database and create schema\nCREATE SCHEMA IF NOT EXISTS ichrisbirch_test;\nALTER USER ichrisbirch SET search_path TO ichrisbirch_test;\n</code></pre>"},{"location":"troubleshooting/database-issues/#migration-issues","title":"Migration Issues","text":""},{"location":"troubleshooting/database-issues/#alembic-migration-failures","title":"Alembic Migration Failures","text":"<p>Problem: Database migrations fail to run or create inconsistent state.</p> <p>Common Issues:</p>"},{"location":"troubleshooting/database-issues/#1-migration-revision-conflicts","title":"1. Migration Revision Conflicts","text":"<pre><code># Error: Multiple heads in migration history\nFAILED: Multiple head revisions are present for given argument 'head'\n\n# Resolution: Merge migration heads\nuv run alembic merge -m \"merge migration heads\" head_1 head_2\nuv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-missing-migration-dependencies","title":"2. Missing Migration Dependencies","text":"<pre><code># Error: Can't locate revision identified by 'abc123'\n# Resolution: Check migration file exists and revision ID is correct\nls ichrisbirch/alembic/versions/\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-manual-schema-changes","title":"3. Manual Schema Changes","text":"<pre><code># If manual changes were made, mark as current\nuv run alembic stamp head\n\n# Or create new migration from current state\nuv run alembic revision --autogenerate -m \"sync with manual changes\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#schema-synchronization","title":"Schema Synchronization","text":"<p>Problem: Development and test databases have different schemas.</p> <p>Resolution:</p> <pre><code># Reset test database to match development\ndocker-compose -f docker-compose.test.yml down -v\ndocker-compose -f docker-compose.test.yml up -d postgres\n\n# Wait for database to be ready\nsleep 10\n\n# Run migrations\ndocker-compose -f docker-compose.test.yml run test-runner uv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/database-issues/#slow-query-performance","title":"Slow Query Performance","text":"<p>Problem: Database queries are slow, causing application timeouts.</p> <p>Diagnosis:</p> <pre><code>-- Enable query logging in PostgreSQL\nALTER SYSTEM SET log_statement = 'all';\nALTER SYSTEM SET log_min_duration_statement = 100;  -- Log queries &gt; 100ms\nSELECT pg_reload_conf();\n\n-- Check for missing indexes\nSELECT schemaname, tablename, attname, n_distinct, correlation\nFROM pg_stats\nWHERE schemaname = 'ichrisbirch_test'\nORDER BY n_distinct DESC;\n</code></pre> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-add-database-indexes","title":"1. Add Database Indexes","text":"<pre><code>-- Create indexes for frequently queried columns\nCREATE INDEX CONCURRENTLY idx_habits_user_id ON habits(user_id);\nCREATE INDEX CONCURRENTLY idx_habits_created_at ON habits(created_at);\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-optimize-sqlalchemy-queries","title":"2. Optimize SQLAlchemy Queries","text":"<pre><code># Bad: N+1 query problem\nfor habit in session.query(Habit).all():\n    print(habit.user.username)  # Separate query for each habit\n\n# Good: Use joinedload to eager load relationships\nfrom sqlalchemy.orm import joinedload\n\nhabits = session.query(Habit).options(joinedload(Habit.user)).all()\nfor habit in habits:\n    print(habit.user.username)  # No additional queries\n</code></pre>"},{"location":"troubleshooting/database-issues/#connection-pool-issues","title":"Connection Pool Issues","text":"<p>Problem: Application runs out of database connections.</p> <p>Error Messages:</p> <ul> <li><code>QueuePool limit of size 20 overflow 10 reached</code></li> <li><code>remaining connection slots are reserved for non-replication superuser connections</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-configure-connection-pool","title":"1. Configure Connection Pool","text":"<pre><code># In database configuration\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\n    DATABASE_URL,\n    pool_size=10,          # Maximum persistent connections\n    max_overflow=20,       # Additional overflow connections\n    pool_timeout=30,       # Timeout for getting connection\n    pool_recycle=1800,     # Recycle connections every 30 minutes\n    pool_pre_ping=True     # Validate connections before use\n)\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-ensure-proper-connection-cleanup","title":"2. Ensure Proper Connection Cleanup","text":"<pre><code># Use context managers for database sessions\nfrom ichrisbirch.database import get_sqlalchemy_session\n\n# Good: Automatic cleanup\ndef get_user_habits(user_id: int):\n    with get_sqlalchemy_session() as session:\n        return session.query(Habit).filter(Habit.user_id == user_id).all()\n    # Session automatically closed\n\n# Bad: Manual cleanup required\ndef get_user_habits_bad(user_id: int):\n    session = SessionLocal()\n    try:\n        return session.query(Habit).filter(Habit.user_id == user_id).all()\n    finally:\n        session.close()  # Easy to forget!\n</code></pre>"},{"location":"troubleshooting/database-issues/#data-integrity-issues","title":"Data Integrity Issues","text":""},{"location":"troubleshooting/database-issues/#foreign-key-constraint-violations","title":"Foreign Key Constraint Violations","text":"<p>Problem: Data operations fail due to referential integrity constraints.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.errors.ForeignKeyViolation: insert or update on table \"habits\" violates foreign key constraint</code></li> <li><code>psycopg2.errors.IntegrityError: duplicate key value violates unique constraint</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-check-data-dependencies","title":"1. Check Data Dependencies","text":"<pre><code>-- Verify referenced records exist\nSELECT u.id, u.username FROM users u WHERE u.id = 123;\n\n-- Check for constraint violations\nSELECT * FROM habits WHERE user_id NOT IN (SELECT id FROM users);\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-handle-dependencies-in-code","title":"2. Handle Dependencies in Code","text":"<pre><code># Ensure user exists before creating habit\nfrom ichrisbirch.database import get_sqlalchemy_session\nfrom ichrisbirch.models import User, Habit\n\ndef create_habit(user_id: int, habit_data: dict):\n    with get_sqlalchemy_session() as session:\n        # Verify user exists\n        user = session.get(User, user_id)\n        if not user:\n            raise ValueError(f\"User {user_id} does not exist\")\n\n        # Create habit\n        habit = Habit(user_id=user_id, **habit_data)\n        session.add(habit)\n        session.commit()\n        return habit\n</code></pre>"},{"location":"troubleshooting/database-issues/#data-corruption-issues","title":"Data Corruption Issues","text":"<p>Problem: Database contains inconsistent or corrupted data.</p> <p>Diagnosis:</p> <pre><code>-- Check for data inconsistencies\nSELECT\n    h.id, h.user_id, u.id as actual_user_id\nFROM habits h\nLEFT JOIN users u ON h.user_id = u.id\nWHERE u.id IS NULL;\n\n-- Verify constraints\nSELECT conname, contype FROM pg_constraint WHERE contype = 'f';\n</code></pre> <p>Resolution:</p> <pre><code># Backup database before repairs\ndocker-compose exec postgres pg_dump -U ichrisbirch ichrisbirch &gt; backup.sql\n\n# Run integrity checks\ndocker-compose exec postgres psql -U ichrisbirch -d ichrisbirch -c \"\nVACUUM ANALYZE;\nREINDEX DATABASE ichrisbirch;\n\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#environment-specific-issues","title":"Environment-Specific Issues","text":""},{"location":"troubleshooting/database-issues/#development-vs-production-differences","title":"Development vs Production Differences","text":"<p>Problem: Database works in development but fails in production.</p> <p>Common Causes:</p>"},{"location":"troubleshooting/database-issues/#1-environment-variable-differences","title":"1. Environment Variable Differences","text":"<pre><code># Check environment variables in containers\ndocker-compose exec app env | grep -i postgres\ndocker-compose -f docker-compose.prod.yml exec app env | grep -i postgres\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-different-postgresql-versions","title":"2. Different PostgreSQL Versions","text":"<pre><code># Pin PostgreSQL version in docker-compose\nservices:\n  postgres:\n    image: postgres:15.4  # Specific version instead of 'latest'\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-missing-extensions","title":"3. Missing Extensions","text":"<pre><code>-- Check installed extensions\nSELECT * FROM pg_extension;\n\n-- Install required extensions\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"troubleshooting/database-issues/#ssl-connection-issues","title":"SSL Connection Issues","text":"<p>Problem: SSL connection errors in production.</p> <p>Error Messages:</p> <ul> <li><code>FATAL: SSL connection is required</code></li> <li><code>SSL error: certificate verify failed</code></li> </ul> <p>Resolution:</p> <pre><code># Update connection string for SSL\nDATABASE_URL = \"postgresql://user:pass@host:5432/db?sslmode=require\"\n\n# For development with self-signed certificates\nDATABASE_URL = \"postgresql://user:pass@host:5432/db?sslmode=require&amp;sslcert=client.crt&amp;sslkey=client.key&amp;sslrootcert=ca.crt\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"troubleshooting/database-issues/#database-backup-issues","title":"Database Backup Issues","text":"<p>Problem: Unable to create or restore database backups.</p> <p>Backup Creation:</p> <pre><code># Create backup with custom format\ndocker-compose exec postgres pg_dump \\\n  -U ichrisbirch \\\n  -d ichrisbirch \\\n  -f /backup/ichrisbirch_$(date +%Y%m%d_%H%M%S).dump \\\n  --format=custom \\\n  --verbose\n\n# Create SQL backup\ndocker-compose exec postgres pg_dump \\\n  -U ichrisbirch \\\n  -d ichrisbirch \\\n  -f /backup/ichrisbirch_$(date +%Y%m%d_%H%M%S).sql \\\n  --verbose\n</code></pre> <p>Backup Restoration:</p> <pre><code># Restore from custom format\ndocker-compose exec postgres pg_restore \\\n  -U ichrisbirch \\\n  -d ichrisbirch_restored \\\n  --clean --create \\\n  --verbose \\\n  /backup/ichrisbirch_20231201_120000.dump\n\n# Restore from SQL\ndocker-compose exec postgres psql \\\n  -U ichrisbirch \\\n  -d ichrisbirch_restored \\\n  -f /backup/ichrisbirch_20231201_120000.sql\n</code></pre>"},{"location":"troubleshooting/database-issues/#monitoring-and-diagnosis","title":"Monitoring and Diagnosis","text":""},{"location":"troubleshooting/database-issues/#database-health-checks","title":"Database Health Checks","text":"<p>Create a database health check script:</p> <pre><code># scripts/db_health_check.py\nimport sys\nfrom ichrisbirch.config import settings\nfrom ichrisbirch.database import get_sqlalchemy_session\nfrom sqlalchemy import text\n\ndef check_database_health():\n    \"\"\"Comprehensive database health check.\"\"\"\n    checks = []\n\n    try:\n        with get_sqlalchemy_session() as session:\n            # Test basic connectivity\n            session.execute(text(\"SELECT 1\"))\n            checks.append((\"Connection\", \"OK\"))\n\n            # Check table existence\n            result = session.execute(text(\"\"\"\n                SELECT COUNT(*) FROM information_schema.tables\n                WHERE table_schema = 'public'\n            \"\"\"))\n            table_count = result.scalar()\n            checks.append((\"Tables\", f\"{table_count} tables\"))\n\n            # Check recent activity\n            result = session.execute(text(\"SELECT COUNT(*) FROM users\"))\n            user_count = result.scalar()\n            checks.append((\"Users\", f\"{user_count} users\"))\n\n            # Check for locks\n            result = session.execute(text(\"\"\"\n                SELECT COUNT(*) FROM pg_locks\n                WHERE NOT granted\n            \"\"\"))\n            lock_count = result.scalar()\n            checks.append((\"Blocked Queries\", f\"{lock_count} blocked\"))\n\n    except Exception as e:\n        print(f\"Database health check failed: {e}\")\n        return False\n\n    # Print results\n    print(\"Database Health Check Results:\")\n    for check, result in checks:\n        print(f\"  {check}: {result}\")\n\n    return True\n\nif __name__ == \"__main__\":\n    if not check_database_health():\n        sys.exit(1)\n</code></pre>"},{"location":"troubleshooting/database-issues/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>-- Monitor active connections\nSELECT\n    state,\n    COUNT(*) as connection_count,\n    MAX(now() - state_change) as max_duration\nFROM pg_stat_activity\nWHERE state IS NOT NULL\nGROUP BY state;\n\n-- Check slow queries\nSELECT\n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Monitor database size\nSELECT\n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n</code></pre>"},{"location":"troubleshooting/database-issues/#prevention-checklist","title":"Prevention Checklist","text":"<p>Database management best practices:</p> <ul> <li> Use health checks in Docker Compose</li> <li> Pin PostgreSQL version in containers</li> <li> Run migrations in controlled manner</li> <li> Monitor connection pool usage</li> <li> Regular backup validation</li> <li> Set up query performance monitoring</li> <li> Document schema changes</li> <li> Test migrations on copy of production data</li> <li> Use transactions for data modifications</li> <li> Implement proper error handling for database operations</li> </ul>"},{"location":"troubleshooting/deployment-issues/","title":"Deployment Troubleshooting","text":"<p>This document covers issues encountered during deployment and production operations of the iChrisBirch project.</p>"},{"location":"troubleshooting/deployment-issues/#service-recovery","title":"Service Recovery","text":"<p>When services are down in production, follow this emergency recovery procedure:</p>"},{"location":"troubleshooting/deployment-issues/#quick-service-recovery","title":"Quick Service Recovery","text":""},{"location":"troubleshooting/deployment-issues/#1-check-service-status","title":"1. Check Service Status","text":"<pre><code># Check all containers\ndocker ps -a\n\n# Check specific service\ndocker-compose -f docker-compose.prod.yml ps app\n\n# Check logs for errors\ndocker-compose -f docker-compose.prod.yml logs --tail=50 app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#2-restart-failed-services","title":"2. Restart Failed Services","text":"<pre><code># Restart single service\ndocker-compose -f docker-compose.prod.yml restart app\n\n# Restart all services\ndocker-compose -f docker-compose.prod.yml restart\n\n# Full recreation if needed\ndocker-compose -f docker-compose.prod.yml down\ndocker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#3-verify-recovery","title":"3. Verify Recovery","text":"<pre><code># Test endpoints\ncurl -f http://localhost/health\ncurl -f http://localhost/api/health\n\n# Check service logs\ndocker-compose -f docker-compose.prod.yml logs -f app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#build-and-deployment-failures","title":"Build and Deployment Failures","text":""},{"location":"troubleshooting/deployment-issues/#docker-build-failures","title":"Docker Build Failures","text":"<p>Problem: Production builds fail during deployment.</p> <p>Common Causes:</p> <ol> <li>Dependency conflicts</li> <li>Missing environment variables</li> <li>Resource limitations</li> </ol> <p>Diagnosis:</p> <pre><code># Build with verbose output\ndocker-compose -f docker-compose.prod.yml build --no-cache --progress=plain\n\n# Check build context\ndocker build --dry-run .\n\n# Verify Dockerfile syntax\ndocker build --target=production .\n</code></pre> <p>Resolution:</p> <pre><code># Clear Docker cache\ndocker system prune -f\ndocker builder prune -f\n\n# Rebuild with no cache\ndocker-compose -f docker-compose.prod.yml build --no-cache\n\n# Check resource usage\ndocker system df\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#environment-variable-issues","title":"Environment Variable Issues","text":"<p>Problem: Services fail due to missing or incorrect environment variables.</p> <p>Diagnosis:</p> <pre><code># Check environment in container\ndocker-compose -f docker-compose.prod.yml exec app env | grep -i postgres\n\n# Verify environment file\ncat .env.prod\n</code></pre> <p>Resolution:</p> <p>Ensure all required variables are set:</p> <pre><code># .env.prod\nDATABASE_URL=postgresql://user:pass@postgres:5432/ichrisbirch\nAPI_URL=https://yourdomain.com/api\nFLASK_ENV=production\nSECRET_KEY=your-secret-key-here\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#ssltls-certificate-issues","title":"SSL/TLS Certificate Issues","text":""},{"location":"troubleshooting/deployment-issues/#certificate-expiration","title":"Certificate Expiration","text":"<p>Problem: SSL certificates expire causing HTTPS errors.</p> <p>Diagnosis:</p> <pre><code># Check certificate expiration\necho | openssl s_client -connect yourdomain.com:443 | openssl x509 -noout -dates\n\n# Check Let's Encrypt certificates\nsudo certbot certificates\n</code></pre> <p>Resolution:</p> <pre><code># Renew certificates\nsudo certbot renew\n\n# Test renewal\nsudo certbot renew --dry-run\n\n# Restart nginx to load new certificates\ndocker-compose -f docker-compose.prod.yml restart nginx\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#certificate-installation-issues","title":"Certificate Installation Issues","text":"<p>Problem: New certificates not being recognized.</p> <p>Resolution:</p> <pre><code># Update nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -t\n\n# Reload nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -s reload\n\n# Restart nginx service\ndocker-compose -f docker-compose.prod.yml restart nginx\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#database-deployment-issues","title":"Database Deployment Issues","text":""},{"location":"troubleshooting/deployment-issues/#migration-failures","title":"Migration Failures","text":"<p>Problem: Database migrations fail during deployment.</p> <p>Safe Migration Process:</p> <pre><code># 1. Backup database first\ndocker-compose -f docker-compose.prod.yml exec postgres pg_dump \\\n  -U ichrisbirch ichrisbirch &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# 2. Test migrations on backup\ndocker-compose -f docker-compose.test.yml run test-runner \\\n  uv run alembic upgrade head\n\n# 3. Apply to production\ndocker-compose -f docker-compose.prod.yml exec app \\\n  uv run alembic upgrade head\n\n# 4. Verify migration\ndocker-compose -f docker-compose.prod.yml exec app \\\n  uv run python -c \"\nfrom ichrisbirch.database import get_sqlalchemy_session\nwith get_sqlalchemy_session() as session:\n    print('Migration successful')\n\"\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#database-connection-issues","title":"Database Connection Issues","text":"<p>Problem: Application cannot connect to production database.</p> <p>Diagnosis:</p> <pre><code># Test database connectivity\ndocker-compose -f docker-compose.prod.yml exec app \\\n  pg_isready -h postgres -p 5432\n\n# Check database logs\ndocker-compose -f docker-compose.prod.yml logs postgres\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/deployment-issues/#high-resource-usage","title":"High Resource Usage","text":"<p>Problem: Production services consuming too many resources.</p> <p>Monitoring:</p> <pre><code># Check container resource usage\ndocker stats\n\n# Check system resources\nhtop\ndf -h\nfree -h\n</code></pre> <p>Resolution:</p> <pre><code># docker-compose.prod.yml - Set resource limits\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '0.5'\n        reservations:\n          memory: 512M\n          cpus: '0.25'\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#slow-response-times","title":"Slow Response Times","text":"<p>Problem: Application responding slowly to requests.</p> <p>Diagnosis:</p> <pre><code># Check response times\ncurl -w \"@curl-format.txt\" -o /dev/null -s http://yourdomain.com/\n\n# Where curl-format.txt contains:\n#     time_namelookup:  %{time_namelookup}\\n\n#        time_connect:  %{time_connect}\\n\n#     time_appconnect:  %{time_appconnect}\\n\n#    time_pretransfer:  %{time_pretransfer}\\n\n#       time_redirect:  %{time_redirect}\\n\n#  time_starttransfer:  %{time_starttransfer}\\n\n#                     ----------\\n\n#          time_total:  %{time_total}\\n\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#load-balancer-and-proxy-issues","title":"Load Balancer and Proxy Issues","text":""},{"location":"troubleshooting/deployment-issues/#nginx-configuration-problems","title":"Nginx Configuration Problems","text":"<p>Problem: Nginx proxy not routing requests correctly.</p> <p>Common Issues:</p> <ol> <li>Upstream server unavailable</li> <li>Wrong proxy configuration</li> <li>SSL termination issues</li> </ol> <p>Diagnosis:</p> <pre><code># Test nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -t\n\n# Check nginx logs\ndocker-compose -f docker-compose.prod.yml logs nginx\n\n# Test upstream connectivity\ndocker-compose -f docker-compose.prod.yml exec nginx \\\n  curl -f http://app:8000/health\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"troubleshooting/deployment-issues/#automated-backup-failures","title":"Automated Backup Failures","text":"<p>Problem: Scheduled backups are failing.</p> <p>Check Backup Status:</p> <pre><code># Check backup cron job\ncrontab -l\n\n# Check backup logs\ntail -f /var/log/backup.log\n\n# Test backup script manually\n./scripts/backup.sh\n</code></pre> <p>Backup Recovery Process:</p> <pre><code># List available backups\nls -la /backup/\n\n# Test backup integrity\npg_restore --list backup_20231201_120000.dump\n\n# Restore from backup if needed\ndocker-compose -f docker-compose.prod.yml down\ndocker volume rm ichrisbirch_postgres_data\ndocker-compose -f docker-compose.prod.yml up -d postgres\n\n# Wait for postgres to be ready\nsleep 30\n\n# Restore data\ndocker-compose -f docker-compose.prod.yml exec postgres \\\n  pg_restore -U ichrisbirch -d ichrisbirch \\\n  /backup/backup_20231201_120000.dump\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"troubleshooting/deployment-issues/#health-check-failures","title":"Health Check Failures","text":"<p>Problem: Health check endpoints returning errors.</p> <p>Diagnosis:</p> <pre><code># Test health endpoints\ncurl -f http://yourdomain.com/health\ncurl -f http://yourdomain.com/api/health\n\n# Check internal health\ndocker-compose -f docker-compose.prod.yml exec app \\\n  curl -f http://localhost:8000/health\n</code></pre> <p>Health Check Implementation:</p> <pre><code># In your application\nfrom fastapi import FastAPI, HTTPException\nfrom ichrisbirch.database import get_sqlalchemy_session\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Comprehensive health check.\"\"\"\n    try:\n        # Test database connection\n        with get_sqlalchemy_session() as session:\n            session.execute(\"SELECT 1\")\n\n        return {\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n    except Exception as e:\n        raise HTTPException(status_code=503, detail=f\"Unhealthy: {str(e)}\")\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#log-management","title":"Log Management","text":"<p>Problem: Logs growing too large or not being rotated.</p> <p>Log Rotation Setup:</p> <pre><code># Create logrotate configuration\nsudo tee /etc/logrotate.d/ichrisbirch &lt;&lt; EOF\n/var/log/ichrisbirch/*.log {\n    daily\n    rotate 30\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 0644 root root\n    postrotate\n        docker-compose -f docker-compose.prod.yml restart app\n    endscript\n}\nEOF\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#disaster-recovery-procedures","title":"Disaster Recovery Procedures","text":""},{"location":"troubleshooting/deployment-issues/#complete-service-recovery","title":"Complete Service Recovery","text":"<p>When everything is down:</p> <pre><code># 1. Check system resources\ndf -h\nfree -h\ndocker system df\n\n# 2. Stop all services\ndocker-compose -f docker-compose.prod.yml down\n\n# 3. Clean up if needed\ndocker system prune -f\n\n# 4. Restore from backup if necessary\n# (Follow backup recovery process above)\n\n# 5. Start services in order\ndocker-compose -f docker-compose.prod.yml up -d postgres\nsleep 30\ndocker-compose -f docker-compose.prod.yml up -d app\ndocker-compose -f docker-compose.prod.yml up -d nginx\n\n# 6. Verify services\n./scripts/verify_deployment.sh\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#data-recovery","title":"Data Recovery","text":"<p>If data is corrupted or lost:</p> <pre><code># 1. Stop application immediately\ndocker-compose -f docker-compose.prod.yml stop app\n\n# 2. Assess damage\ndocker-compose -f docker-compose.prod.yml exec postgres \\\n  psql -U ichrisbirch -c \"SELECT COUNT(*) FROM users;\"\n\n# 3. Restore from most recent backup\n# (Use backup recovery procedure)\n\n# 4. Verify data integrity\n# (Run data validation queries)\n\n# 5. Resume service\ndocker-compose -f docker-compose.prod.yml start app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#prevention-and-monitoring","title":"Prevention and Monitoring","text":""},{"location":"troubleshooting/deployment-issues/#deployment-checklist","title":"Deployment Checklist","text":"<p>Before each deployment:</p> <ul> <li> Run tests in staging environment</li> <li> Backup production database</li> <li> Verify environment variables</li> <li> Test migration scripts</li> <li> Check disk space and resources</li> <li> Verify SSL certificate validity</li> <li> Update deployment documentation</li> </ul>"},{"location":"troubleshooting/deployment-issues/#monitoring-setup","title":"Monitoring Setup","text":"<p>Essential monitoring:</p> <pre><code># System monitoring\nhtop\niotop\nnethogs\n\n# Docker monitoring\ndocker stats\ndocker system df\n\n# Application monitoring\ncurl -f http://yourdomain.com/health\n</code></pre> <p>Automated monitoring script:</p> <pre><code>#!/bin/bash\n# scripts/monitor.sh\n\n# Check service health\ncheck_service() {\n    if curl -f -s http://localhost/$1/health &gt; /dev/null; then\n        echo \"\u2713 $1 service healthy\"\n    else\n        echo \"\u2717 $1 service unhealthy\"\n        # Send alert here\n    fi\n}\n\ncheck_service \"api\"\ncheck_service \"\"  # Main app\n\n# Check disk space\nDISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')\nif [ \"$DISK_USAGE\" -gt 80 ]; then\n    echo \"\u26a0\ufe0f  Disk usage high: ${DISK_USAGE}%\"\nfi\n\n# Check memory\nMEMORY_USAGE=$(free | awk 'NR==2{print $3/$2 * 100.0}')\nif (( $(echo \"$MEMORY_USAGE &gt; 80\" | bc -l) )); then\n    echo \"\u26a0\ufe0f  Memory usage high: ${MEMORY_USAGE}%\"\nfi\n</code></pre> <p>Run monitoring periodically:</p> <pre><code># Add to crontab\n*/5 * * * * /path/to/scripts/monitor.sh &gt;&gt; /var/log/monitoring.log 2&gt;&amp;1\n</code></pre>"},{"location":"troubleshooting/development-issues/","title":"Development Environment Troubleshooting","text":"<p>This document covers issues related to setting up and maintaining the development environment for the iChrisBirch project.</p>"},{"location":"troubleshooting/development-issues/#initial-setup-issues","title":"Initial Setup Issues","text":""},{"location":"troubleshooting/development-issues/#python-version-compatibility","title":"Python Version Compatibility","text":"<p>Problem: Wrong Python version installed or being used.</p> <p>Symptoms:</p> <ul> <li><code>Python version not supported</code> errors</li> <li>Package installation failures</li> <li>Syntax errors with modern Python features</li> </ul> <p>Resolution:</p> <ol> <li>Verify Python version:</li> </ol> <pre><code>python --version  # Should be 3.12.x\nwhich python      # Verify correct Python binary\n</code></pre> <ol> <li>Install correct Python version:</li> </ol> <pre><code># macOS with Homebrew\nbrew install python@3.12\n\n# Ubuntu/Debian\nsudo apt update\nsudo apt install python3.12 python3.12-venv\n\n# Using pyenv (recommended)\npyenv install 3.12.0\npyenv local 3.12.0\n</code></pre>"},{"location":"troubleshooting/development-issues/#uv-package-manager-setup","title":"UV Package Manager Setup","text":"<p>Problem: UV not installed or not working correctly.</p> <p>Installation:</p> <pre><code># Install UV\npip install uv\n\n# Verify installation\nuv --version\n\n# Update UV\npip install --upgrade uv\n</code></pre> <p>Common UV Issues:</p> <ol> <li>Cache corruption:</li> </ol> <pre><code># Clear UV cache\nuv cache clean\n\n# Reinstall dependencies\nrm uv.lock\nuv sync\n</code></pre> <ol> <li>Virtual environment issues:</li> </ol> <pre><code># Recreate virtual environment\nrm -rf .venv\nuv sync\n</code></pre>"},{"location":"troubleshooting/development-issues/#docker-setup-problems","title":"Docker Setup Problems","text":"<p>Problem: Docker not installed or not working properly.</p> <p>Basic Docker Setup:</p> <pre><code># Verify Docker installation\ndocker --version\ndocker-compose --version\n\n# Test Docker functionality\ndocker run hello-world\n\n# Check Docker daemon status\ndocker info\n</code></pre> <p>Common Docker Issues:</p> <ol> <li>Permission issues on Linux:</li> </ol> <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and back in, or restart\n</code></pre> <ol> <li>Docker daemon not running:</li> </ol> <pre><code># Start Docker service (Linux)\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# macOS - Start Docker Desktop application\n</code></pre>"},{"location":"troubleshooting/development-issues/#environment-configuration-issues","title":"Environment Configuration Issues","text":""},{"location":"troubleshooting/development-issues/#environment-variables-not-loading","title":"Environment Variables Not Loading","text":"<p>Problem: Application can't find required environment variables.</p> <p>Diagnosis:</p> <pre><code># Check if environment file exists\nls -la .env*\n\n# Verify environment variables are loaded\ndocker-compose exec app env | grep -i postgres\n</code></pre> <p>Resolution:</p> <ol> <li>Create environment files:</li> </ol> <pre><code># Copy example files\ncp .env.example .env\ncp .env.test.example .env.test\n\n# Edit with your values\nvim .env\n</code></pre> <ol> <li>Verify Docker Compose loads environment:</li> </ol> <pre><code># docker-compose.yml\nservices:\n  app:\n    env_file:\n      - .env\n    environment:\n      - NODE_ENV=development\n</code></pre>"},{"location":"troubleshooting/development-issues/#configuration-conflicts","title":"Configuration Conflicts","text":"<p>Problem: Different configuration values between environments.</p> <p>Common Issues:</p> <ol> <li>Database URL mismatches</li> <li>API endpoint differences</li> <li>Debug settings conflicts</li> </ol> <p>Resolution:</p> <p>Create environment-specific configuration:</p> <pre><code># ichrisbirch/config.py\nimport os\nfrom typing import Literal\n\nclass Settings:\n    def __init__(self):\n        self.environment: Literal[\"development\", \"testing\", \"production\"] = \\\n            os.environ[\"ENVIRONMENT\"]\n\n        # Environment-specific settings\n        if self.environment == \"testing\":\n            self.database_url = os.environ[\"TEST_DATABASE_URL\"]\n            self.debug = True\n        elif self.environment == \"production\":\n            self.database_url = os.environ[\"PROD_DATABASE_URL\"]\n            self.debug = False\n        else:  # development\n            self.database_url = os.environ[\"DEV_DATABASE_URL\"]\n            self.debug = True\n\nsettings = Settings()\n</code></pre>"},{"location":"troubleshooting/development-issues/#ide-and-editor-issues","title":"IDE and Editor Issues","text":""},{"location":"troubleshooting/development-issues/#vs-code-extension-problems","title":"VS Code Extension Problems","text":"<p>Problem: Python extension not working or missing features.</p> <p>Resolution:</p> <ol> <li>Install required extensions:</li> </ol> <pre><code>code --install-extension ms-python.python\ncode --install-extension ms-python.pylint\ncode --install-extension ms-python.black-formatter\n</code></pre> <ol> <li>Configure Python interpreter:</li> </ol> <pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./.venv/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.formatting.provider\": \"black\"\n}\n</code></pre>"},{"location":"troubleshooting/development-issues/#import-resolution-issues","title":"Import Resolution Issues","text":"<p>Problem: IDE can't find imports or shows false errors.</p> <p>Common Causes:</p> <ol> <li>Wrong Python interpreter selected</li> <li>Package not installed in editable mode</li> <li>Missing <code>__init__.py</code> files</li> </ol> <p>Resolution:</p> <pre><code># Install package in editable mode\nuv sync\n\n# Verify package is installed\nuv run python -c \"import ichrisbirch; print(ichrisbirch.__file__)\"\n\n# Check Python path\nuv run python -c \"import sys; print('\\n'.join(sys.path))\"\n</code></pre>"},{"location":"troubleshooting/development-issues/#development-workflow-issues","title":"Development Workflow Issues","text":""},{"location":"troubleshooting/development-issues/#git-configuration-problems","title":"Git Configuration Problems","text":"<p>Problem: Git not configured or authentication issues.</p> <p>Setup:</p> <pre><code># Configure Git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Configure SSH key for GitHub\nssh-keygen -t ed25519 -C \"your.email@example.com\"\ncat ~/.ssh/id_ed25519.pub  # Add to GitHub\n\n# Test SSH connection\nssh -T git@github.com\n</code></pre>"},{"location":"troubleshooting/development-issues/#pre-commit-hook-issues","title":"Pre-commit Hook Issues","text":"<p>Problem: Pre-commit hooks fail or don't run.</p> <p>Setup:</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n\n# Skip hooks if needed (emergency only)\ngit commit --no-verify\n</code></pre>"},{"location":"troubleshooting/development-issues/#code-formatting-conflicts","title":"Code Formatting Conflicts","text":"<p>Problem: Different formatting tools producing conflicting results.</p> <p>Resolution:</p> <p>Create consistent configuration:</p> <pre><code># pyproject.toml\n[tool.black]\nline-length = 88\ntarget-version = ['py312']\n\n[tool.isort]\nprofile = \"black\"\nline_length = 88\n\n[tool.pylint.messages_control]\nmax-line-length = 88\n</code></pre>"},{"location":"troubleshooting/development-issues/#performance-and-resource-issues","title":"Performance and Resource Issues","text":""},{"location":"troubleshooting/development-issues/#slow-development-environment","title":"Slow Development Environment","text":"<p>Problem: Local development is slow or unresponsive.</p> <p>Common Causes:</p> <ol> <li>Insufficient resources for Docker</li> <li>Too many services running</li> <li>Large log files</li> </ol> <p>Resolution:</p> <ol> <li>Optimize Docker resources:</li> </ol> <pre><code># docker-compose.yml - limit resource usage\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n        reservations:\n          memory: 256M\n</code></pre> <ol> <li>Selective service startup:</li> </ol> <pre><code># Start only essential services\ndocker-compose up app postgres\n\n# Start additional services as needed\ndocker-compose up redis nginx\n</code></pre> <ol> <li>Clean up logs:</li> </ol> <pre><code># Rotate log files\ndocker-compose logs --no-color &gt; logs/app.log\ndocker-compose down\ndocker system prune -f\n</code></pre>"},{"location":"troubleshooting/development-issues/#port-conflicts","title":"Port Conflicts","text":"<p>Problem: Ports already in use by other applications.</p> <p>Diagnosis:</p> <pre><code># Check what's using specific port\nlsof -i :8000\nnetstat -tulpn | grep :8000\n\n# Kill process using port\nkill -9 $(lsof -t -i:8000)\n</code></pre> <p>Resolution:</p> <ol> <li>Change ports in docker-compose:</li> </ol> <pre><code>services:\n  app:\n    ports:\n      - \"8001:8000\"  # Use different external port\n</code></pre> <ol> <li>Use different ports for different projects:</li> </ol> <pre><code># Project-specific environment variables\necho \"APP_PORT=8001\" &gt;&gt; .env\necho \"POSTGRES_PORT=5433\" &gt;&gt; .env\n</code></pre>"},{"location":"troubleshooting/development-issues/#debugging-and-troubleshooting-tools","title":"Debugging and Troubleshooting Tools","text":""},{"location":"troubleshooting/development-issues/#development-debugging","title":"Development Debugging","text":"<p>Enable Debug Mode:</p> <pre><code># In your application\nimport logging\n\n# Set up debug logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n# Add debug prints\nlogger.debug(f\"Settings loaded: {settings}\")\nlogger.debug(f\"Database URL: {settings.database_url}\")\n</code></pre>"},{"location":"troubleshooting/development-issues/#container-debugging","title":"Container Debugging","text":"<p>Access container shell:</p> <pre><code># Get shell in running container\ndocker-compose exec app bash\n\n# Run container with override\ndocker-compose run --rm app bash\n\n# Debug specific service\ndocker-compose run --rm --entrypoint=\"\" app bash\n</code></pre>"},{"location":"troubleshooting/development-issues/#network-debugging","title":"Network Debugging","text":"<p>Test service connectivity:</p> <pre><code># Test from host to container\ncurl http://localhost:8000/health\n\n# Test container to container\ndocker-compose exec app curl http://api:8000/health\n\n# Check DNS resolution\ndocker-compose exec app nslookup postgres\n</code></pre>"},{"location":"troubleshooting/development-issues/#environment-validation-script","title":"Environment Validation Script","text":"<p>Create a validation script to check environment setup:</p> <pre><code>#!/usr/bin/env python3\n# scripts/validate_dev_env.py\n\"\"\"\nDevelopment environment validation script.\nRun this to verify your development setup is correct.\n\"\"\"\n\nimport sys\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef check_command(command: str, name: str) -&gt; bool:\n    \"\"\"Check if a command is available.\"\"\"\n    try:\n        result = subprocess.run(\n            [command, \"--version\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            print(f\"\u2713 {name}: {result.stdout.strip()}\")\n            return True\n    except FileNotFoundError:\n        pass\n\n    print(f\"\u2717 {name}: Not found\")\n    return False\n\ndef check_file(filepath: str, name: str) -&gt; bool:\n    \"\"\"Check if a file exists.\"\"\"\n    if Path(filepath).exists():\n        print(f\"\u2713 {name}: Found\")\n        return True\n    else:\n        print(f\"\u2717 {name}: Missing\")\n        return False\n\ndef check_docker_compose() -&gt; bool:\n    \"\"\"Check if Docker Compose services can start.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"docker-compose\", \"config\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            print(\"\u2713 Docker Compose: Configuration valid\")\n            return True\n        else:\n            print(f\"\u2717 Docker Compose: Configuration error\\n{result.stderr}\")\n            return False\n    except Exception as e:\n        print(f\"\u2717 Docker Compose: {e}\")\n        return False\n\ndef main():\n    \"\"\"Run all validation checks.\"\"\"\n    print(\"\ud83d\udd0d Validating development environment...\\n\")\n\n    checks = [\n        # Core tools\n        (lambda: check_command(\"python3\", \"Python\"), \"Python 3\"),\n        (lambda: check_command(\"uv\", \"UV Package Manager\"), \"UV\"),\n        (lambda: check_command(\"docker\", \"Docker\"), \"Docker\"),\n        (lambda: check_command(\"docker-compose\", \"Docker Compose\"), \"Docker Compose\"),\n        (lambda: check_command(\"git\", \"Git\"), \"Git\"),\n\n        # Configuration files\n        (lambda: check_file(\".env\", \".env file\"), \"Environment file\"),\n        (lambda: check_file(\"pyproject.toml\", \"pyproject.toml\"), \"Project config\"),\n        (lambda: check_file(\"docker-compose.yml\", \"docker-compose.yml\"), \"Docker Compose config\"),\n\n        # Docker validation\n        (check_docker_compose, \"Docker Compose validation\"),\n    ]\n\n    passed = 0\n    total = len(checks)\n\n    for check_func, name in checks:\n        if check_func():\n            passed += 1\n        print()\n\n    # Summary\n    print(f\"\ud83d\udcca Validation Summary: {passed}/{total} checks passed\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 Development environment is ready!\")\n        return 0\n    else:\n        print(\"\u274c Some issues need to be resolved before development\")\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>Run the validation script:</p> <pre><code># Make executable\nchmod +x scripts/validate_dev_env.py\n\n# Run validation\n./scripts/validate_dev_env.py\n</code></pre>"},{"location":"troubleshooting/development-issues/#quick-setup-guide","title":"Quick Setup Guide","text":"<p>For new developers, here's a quick setup checklist:</p> <pre><code># 1. Clone repository\ngit clone https://github.com/username/ichrisbirch.git\ncd ichrisbirch\n\n# 2. Copy environment files\ncp .env.example .env\ncp .env.test.example .env.test\n\n# 3. Install UV\npip install uv\n\n# 4. Install dependencies\nuv sync\n\n# 5. Start development environment\ndocker-compose up -d\n\n# 6. Run validation\n./scripts/validate_dev_env.py\n\n# 7. Run tests to verify setup\ndocker-compose -f docker-compose.test.yml up test-runner\n</code></pre>"},{"location":"troubleshooting/development-issues/#common-development-commands","title":"Common Development Commands","text":"<p>Keep these handy for daily development:</p> <pre><code># Start development environment\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f app\n\n# Run tests\ndocker-compose -f docker-compose.test.yml up test-runner\n\n# Access application shell\ndocker-compose exec app bash\n\n# Install new package\nuv add package-name\n\n# Run database migrations\ndocker-compose exec app uv run alembic upgrade head\n\n# Format code\nuv run black ichrisbirch/\nuv run isort ichrisbirch/\n\n# Lint code\nuv run pylint ichrisbirch/\n\n# Stop all services\ndocker-compose down\n\n# Clean up Docker resources\ndocker system prune -f\ndocker-compose down -v  # Remove volumes too\n</code></pre>"},{"location":"troubleshooting/docker-issues/","title":"Docker Configuration Troubleshooting","text":"<p>This document covers common Docker-related issues encountered during development and deployment of the iChrisBirch project, including build failures, networking problems, and container orchestration issues.</p>"},{"location":"troubleshooting/docker-issues/#multi-stage-build-issues","title":"Multi-Stage Build Issues","text":""},{"location":"troubleshooting/docker-issues/#missing-executables-in-container-stages","title":"Missing Executables in Container Stages","text":"<p>Problem: Files copied from previous stages don't work or are missing.</p> <p>Symptoms:</p> <ul> <li><code>/app/.venv/bin/pytest: No such file or directory</code></li> <li>Commands work in builder stage but fail in runtime stage</li> <li>Broken symlinks after <code>COPY --from=builder</code></li> </ul> <p>Root Cause: Package managers like Poetry create complex directory structures with symlinks that don't survive Docker layer copying.</p> <p>Resolution: Use UV instead of Poetry for better Docker compatibility:</p> <pre><code># Good: UV creates self-contained installations\nFROM python:3.12-slim as builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\nFROM builder as development\nCOPY . .\nRUN uv sync --frozen --group dev --group test\n</code></pre> <p>Prevention:</p> <ul> <li>Test each stage individually: <code>docker build --target=development .</code></li> <li>Verify executables exist: <code>docker run --rm image which pytest</code></li> <li>Use package managers designed for containers</li> </ul>"},{"location":"troubleshooting/docker-issues/#container-networking-issues","title":"Container Networking Issues","text":""},{"location":"troubleshooting/docker-issues/#service-communication-failures","title":"Service Communication Failures","text":"<p>Problem: Services can't communicate with each other in Docker Compose.</p> <p>Symptoms:</p> <ul> <li><code>Connection refused</code> errors between services</li> <li>Services start but can't reach database/API</li> <li>Works locally but fails in containers</li> </ul> <p>Common Causes:</p> <ol> <li>Wrong hostname: Using <code>localhost</code> instead of service name</li> <li>Port mapping confusion: Mixing internal and external ports</li> <li>Network isolation: Services on different networks</li> </ol> <p>Resolution:</p> <pre><code># docker-compose.yml\nservices:\n  app:\n    environment:\n      - DATABASE_URL=postgresql://user:pass@postgres:5432/db\n      # Use service name 'postgres', not 'localhost'\n    networks:\n      - app-network\n\n  postgres:\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n    driver: bridge\n</code></pre> <p>Debugging Steps:</p> <pre><code># Check service connectivity\ndocker-compose exec app ping postgres\n\n# Verify environment variables\ndocker-compose exec app env | grep DATABASE\n\n# Check port availability\ndocker-compose exec app nc -zv postgres 5432\n</code></pre>"},{"location":"troubleshooting/docker-issues/#build-context-and-performance","title":"Build Context and Performance","text":""},{"location":"troubleshooting/docker-issues/#large-build-contexts","title":"Large Build Contexts","text":"<p>Problem: Docker builds are slow due to large context.</p> <p>Symptoms:</p> <ul> <li><code>Sending build context to Docker daemon</code> takes minutes</li> <li>Builds timeout or run out of space</li> <li>Unnecessary files in containers</li> </ul> <p>Resolution:</p> <p>Create comprehensive <code>.dockerignore</code>:</p> <pre><code># Version control\n.git/\n.gitignore\n\n# Python\n__pycache__/\n*.pyc\n.pytest_cache/\n\n# Development\n.vscode/\n*.log\nnode_modules/\n\n# Documentation\ndocs/\n*.md\n\n# CI/CD\n.github/\n\n# Local development\n.env.local\ndocker-compose.override.yml\n</code></pre> <p>Performance Tips:</p> <ul> <li>Put frequently changing files (source code) after stable dependencies</li> <li>Use multi-stage builds to separate build dependencies from runtime</li> <li>Cache dependency installation layers</li> </ul>"},{"location":"troubleshooting/docker-issues/#layer-caching-issues","title":"Layer Caching Issues","text":"<p>Problem: Dependencies reinstall on every build despite no changes.</p> <p>Common Mistake:</p> <pre><code># Bad: Any file change invalidates dependency cache\nCOPY . .\nRUN uv sync --frozen\n</code></pre> <p>Correct Approach:</p> <pre><code># Good: Cache dependencies separately\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\n# Source code in separate layer\nCOPY . .\nRUN uv sync --frozen --group dev\n</code></pre>"},{"location":"troubleshooting/docker-issues/#environment-variable-management","title":"Environment Variable Management","text":""},{"location":"troubleshooting/docker-issues/#missing-environment-variables","title":"Missing Environment Variables","text":"<p>Problem: Services fail to start with configuration errors.</p> <p>Symptoms:</p> <ul> <li>Services exit immediately after starting</li> <li>\"Environment variable not set\" errors</li> <li>Database connection failures</li> </ul> <p>Resolution:</p> <ol> <li>Use environment files:</li> </ol> <pre><code># docker-compose.yml\nservices:\n  app:\n    env_file:\n      - .env\n      - .env.local  # Optional overrides\n</code></pre> <ol> <li>Provide defaults:</li> </ol> <pre><code>services:\n  app:\n    environment:\n      - POSTGRES_DB=${POSTGRES_DB:-ichrisbirch}\n      - DEBUG=${DEBUG:-false}\n</code></pre> <ol> <li>Validate environment:</li> </ol> <pre><code># In your application startup\nfrom ichrisbirch.config import settings\n\n# Let it fail fast if misconfigured\nassert settings.database_url, \"DATABASE_URL must be set\"\n</code></pre>"},{"location":"troubleshooting/docker-issues/#volume-and-persistence-issues","title":"Volume and Persistence Issues","text":""},{"location":"troubleshooting/docker-issues/#database-data-loss","title":"Database Data Loss","text":"<p>Problem: Database data disappears when containers restart.</p> <p>Cause: No persistent volume configured for database.</p> <p>Resolution:</p> <pre><code>services:\n  postgres:\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n    driver: local\n</code></pre>"},{"location":"troubleshooting/docker-issues/#file-permission-problems","title":"File Permission Problems","text":"<p>Problem: Files created in containers have wrong ownership.</p> <p>Symptoms:</p> <ul> <li>Permission denied when accessing files from host</li> <li>Files owned by root instead of user</li> <li>Build failures due to permission issues</li> </ul> <p>Resolution:</p> <pre><code># Match host user ID in development\nARG UID=1000\nARG GID=1000\nRUN groupadd -g $GID appgroup &amp;&amp; \\\n    useradd -u $UID -g $GID -m appuser\nUSER appuser\n</code></pre> <p>Or in docker-compose for development:</p> <pre><code>services:\n  app:\n    user: \"${UID:-1000}:${GID:-1000}\"\n</code></pre>"},{"location":"troubleshooting/docker-issues/#service-orchestration-issues","title":"Service Orchestration Issues","text":""},{"location":"troubleshooting/docker-issues/#service-startup-order","title":"Service Startup Order","text":"<p>Problem: Services start before dependencies are ready.</p> <p>Symptoms:</p> <ul> <li>Application fails to connect to database during startup</li> <li>Race conditions in service initialization</li> <li>Containers restart repeatedly</li> </ul> <p>Resolution:</p> <p>Use <code>depends_on</code> with health checks:</p> <pre><code>services:\n  app:\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n</code></pre>"},{"location":"troubleshooting/docker-issues/#resource-constraints","title":"Resource Constraints","text":"<p>Problem: Services crash due to insufficient resources.</p> <p>Resolution:</p> <p>Set resource limits:</p> <pre><code>services:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n        reservations:\n          memory: 256M\n</code></pre>"},{"location":"troubleshooting/docker-issues/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/docker-issues/#container-inspection","title":"Container Inspection","text":"<pre><code># Check running containers\ndocker-compose ps\n\n# View logs\ndocker-compose logs app\ndocker-compose logs -f --tail=100 app\n\n# Execute commands in container\ndocker-compose exec app bash\ndocker-compose exec app uv run python -c \"import sys; print(sys.path)\"\n\n# Inspect container configuration\ndocker inspect $(docker-compose ps -q app)\n</code></pre>"},{"location":"troubleshooting/docker-issues/#network-debugging","title":"Network Debugging","text":"<pre><code># List networks\ndocker network ls\n\n# Inspect network configuration\ndocker network inspect ichrisbirch_default\n\n# Test connectivity between services\ndocker-compose exec app ping postgres\ndocker-compose exec app nc -zv postgres 5432\n</code></pre>"},{"location":"troubleshooting/docker-issues/#build-debugging","title":"Build Debugging","text":"<pre><code># Build with verbose output\ndocker-compose build --progress=plain --no-cache\n\n# Build specific stage\ndocker build --target=development .\n\n# Inspect intermediate stages\ndocker build --rm=false .\ndocker run -it &lt;intermediate-id&gt; bash\n</code></pre>"},{"location":"troubleshooting/docker-issues/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/docker-issues/#no-such-file-or-directory","title":"\"No such file or directory\"","text":"<p>Error: <code>/app/.venv/bin/pytest: No such file or directory</code></p> <p>Cause: Broken symlinks in virtual environment after Docker layer copy.</p> <p>Fix: Use UV instead of Poetry, ensure proper multi-stage build.</p>"},{"location":"troubleshooting/docker-issues/#connection-refused","title":"\"Connection refused\"","text":"<p>Error: <code>psycopg2.OperationalError: connection to server at \"localhost\" (127.0.0.1) port 5432 refused</code></p> <p>Cause: Using <code>localhost</code> instead of Docker service name.</p> <p>Fix: Update connection string to use service name: <code>postgres:5432</code></p>"},{"location":"troubleshooting/docker-issues/#port-already-in-use","title":"\"Port already in use\"","text":"<p>Error: <code>Error starting userland proxy: listen tcp 0.0.0.0:5432: bind: address already in use</code></p> <p>Cause: Port conflict with host system or another container.</p> <p>Fix: Change external port mapping or stop conflicting service.</p>"},{"location":"troubleshooting/docker-issues/#prevention-checklist","title":"Prevention Checklist","text":"<ul> <li> Use <code>.dockerignore</code> to exclude unnecessary files</li> <li> Test each Docker stage individually</li> <li> Use service names for inter-service communication</li> <li> Configure health checks for critical services</li> <li> Set up persistent volumes for data</li> <li> Document required environment variables</li> <li> Test builds on clean systems (CI/CD)</li> <li> Monitor resource usage and set limits</li> </ul>"},{"location":"troubleshooting/poetry-uv-migration/","title":"Poetry to UV Migration Troubleshooting","text":"<p>This document chronicles the complete migration from Poetry to UV package management, including all issues encountered, solutions attempted, and the final working configuration.</p>"},{"location":"troubleshooting/poetry-uv-migration/#background","title":"Background","text":"<p>The iChrisBirch project originally used Poetry for dependency management but encountered significant issues with Docker containerization, particularly with virtual environment handling and pytest execution in containerized environments.</p> <p>Poetry Docker Incompatibility</p> <p>Poetry's virtual environment copying between Docker stages is fundamentally broken in containerized environments, leading to missing executables and broken symlinks.</p>"},{"location":"troubleshooting/poetry-uv-migration/#issue-timeline","title":"Issue Timeline","text":""},{"location":"troubleshooting/poetry-uv-migration/#initial-problem","title":"Initial Problem","text":"<p>Error Encountered:</p> <pre><code>docker-compose -f docker-compose.test.yml up test-runner\n# Output: /app/.venv/bin/pytest: No such file or directory\n</code></pre> <p>Root Cause Analysis: Poetry creates virtual environments with complex symlink structures that don't survive Docker's multi-stage build copying. The <code>.venv/bin/pytest</code> executable was missing despite Poetry claiming successful installation.</p>"},{"location":"troubleshooting/poetry-uv-migration/#attempted-solutions-that-failed","title":"Attempted Solutions (That Failed)","text":""},{"location":"troubleshooting/poetry-uv-migration/#1-fix-poetry-docker-configuration","title":"1. Fix Poetry Docker Configuration","text":"<p>What We Tried:</p> <pre><code># Attempted fix in Dockerfile\nENV POETRY_VENV_IN_PROJECT=1\nENV POETRY_NO_INTERACTION=1\nCOPY --from=builder /app/.venv /app/.venv\n</code></pre> <p>Why It Failed: Poetry's virtual environment structure with symlinks doesn't copy correctly between Docker layers. The symlinks become broken references.</p>"},{"location":"troubleshooting/poetry-uv-migration/#2-manual-pytest-installation","title":"2. Manual Pytest Installation","text":"<p>What We Tried:</p> <pre><code>RUN .venv/bin/pip install pytest pytest-cov\n</code></pre> <p>Why It Failed: The <code>.venv/bin/pip</code> itself was a broken symlink, creating a circular dependency problem.</p>"},{"location":"troubleshooting/poetry-uv-migration/#3-poetry-path-modifications","title":"3. Poetry Path Modifications","text":"<p>What We Tried:</p> <pre><code>ENV PATH=\"/app/.venv/bin:$PATH\"\nRUN which pytest  # Still not found\n</code></pre> <p>Why It Failed: Setting PATH doesn't fix broken symlinks; the underlying executable files were never properly copied.</p>"},{"location":"troubleshooting/poetry-uv-migration/#final-resolution-complete-migration-to-uv","title":"Final Resolution: Complete Migration to UV","text":""},{"location":"troubleshooting/poetry-uv-migration/#why-uv-was-chosen","title":"Why UV Was Chosen","text":"<ol> <li>Better Docker Support: UV handles containerized environments correctly</li> <li>Faster Installation: UV is significantly faster than Poetry</li> <li>PEP 621 Compliance: Uses modern Python packaging standards</li> <li>Simplified Dependencies: Cleaner dependency group management</li> </ol>"},{"location":"troubleshooting/poetry-uv-migration/#migration-steps","title":"Migration Steps","text":""},{"location":"troubleshooting/poetry-uv-migration/#1-convert-pyprojecttoml","title":"1. Convert pyproject.toml","text":"<p>Before (Poetry format):</p> <pre><code>[tool.poetry]\nname = \"ichrisbirch\"\nversion = \"0.1.0\"\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nfastapi = \"^0.104.1\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.4.3\"\n</code></pre> <p>After (UV/PEP 621 format):</p> <pre><code>[project]\nname = \"ichrisbirch\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"fastapi&gt;=0.104.1\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=7.4.3\",\n]\ntest = [\n    \"pytest&gt;=7.4.3\",\n    \"pytest-cov&gt;=4.1.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#2-update-dockerfile","title":"2. Update Dockerfile","text":"<p>Before (Poetry):</p> <pre><code>FROM python:3.12-slim as builder\nRUN pip install poetry\nCOPY pyproject.toml poetry.lock ./\nRUN poetry config venv.in-project true &amp;&amp; poetry install --no-root\n</code></pre> <p>After (UV):</p> <pre><code>FROM python:3.12-slim as builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\nFROM builder as development\nCOPY . .\nRUN uv sync --frozen --group dev --group test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#3-update-docker-compose","title":"3. Update Docker Compose","text":"<p>Before (Poetry):</p> <pre><code>test-runner:\n  build:\n    target: development\n  command: [\"poetry\", \"run\", \"pytest\", \"-vv\", \"--cov=ichrisbirch\"]\n</code></pre> <p>After (UV):</p> <pre><code>test-runner:\n  build:\n    target: development\n  command: [\"uv\", \"run\", \"pytest\", \"-vv\", \"--cov=ichrisbirch\"]\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#4-update-cicd-workflows","title":"4. Update CI/CD Workflows","text":"<p>Before (Poetry):</p> <pre><code>- name: Install Poetry\n  run: pip install poetry\n- name: Install dependencies\n  run: poetry install\n- name: Run tests\n  run: poetry run pytest\n</code></pre> <p>After (UV):</p> <pre><code>- name: Install UV\n  run: pip install uv\n- name: Install dependencies\n  run: uv sync --frozen --group dev --group test\n- name: Run tests\n  run: uv run pytest\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#additional-issues-resolved","title":"Additional Issues Resolved","text":""},{"location":"troubleshooting/poetry-uv-migration/#database-schema-creation","title":"Database Schema Creation","text":"<p>Issue: Tests failing because database schema wasn't created in test environment.</p> <p>Root Cause: <code>POSTGRES_DB_SCHEMA</code> environment variable missing from test configuration.</p> <p>Solution:</p> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    environment:\n      - POSTGRES_DB_SCHEMA=ichrisbirch_test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#package-source-code-access","title":"Package Source Code Access","text":"<p>Issue: UV couldn't access package source code for local development.</p> <p>Solution: Ensure source code is copied before <code>uv sync</code>:</p> <pre><code>FROM builder as development\nCOPY . .  # Copy source before sync\nRUN uv sync --frozen --group dev --group test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#verification-steps","title":"Verification Steps","text":"<p>After migration, verify everything works:</p> <pre><code># 1. Check UV installation\ndocker-compose -f docker-compose.test.yml build\n\n# 2. Verify pytest is available\ndocker-compose -f docker-compose.test.yml run test-runner uv run which pytest\n\n# 3. Run actual tests\ndocker-compose -f docker-compose.test.yml up test-runner\n\n# 4. Check test database connection\ndocker-compose -f docker-compose.test.yml run test-runner uv run python -c \"\nfrom ichrisbirch.config import settings\nprint(f'Database URL: {settings.database_url}')\n\"\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#benefits-achieved","title":"Benefits Achieved","text":"<ol> <li>Working Tests: Docker test environment now functions correctly</li> <li>Faster Builds: UV installation is significantly faster than Poetry</li> <li>Simpler Configuration: Fewer environment variables and cleaner setup</li> <li>Better CI/CD: More reliable GitHub Actions workflows</li> <li>Future-Proof: Using modern PEP 621 standards</li> </ol>"},{"location":"troubleshooting/poetry-uv-migration/#prevention","title":"Prevention","text":"<p>To avoid similar issues in the future:</p> <ol> <li>Test Docker Builds Regularly: Don't let Docker configurations drift</li> <li>Use UV for New Projects: UV has better containerization support</li> <li>Validate Multi-Stage Builds: Ensure all stages have required executables</li> <li>Monitor Dependency Management: Keep dependency management tools updated</li> <li>Document Environment Variables: All required variables must be documented</li> </ol> <p>Migration Checklist</p> <p>When migrating package managers:</p> <ul> <li> Convert pyproject.toml format</li> <li> Update all Dockerfiles</li> <li> Modify Docker Compose files</li> <li> Update CI/CD workflows</li> <li> Update deployment scripts</li> <li> Test all environments (dev, test, prod)</li> <li> Update documentation</li> </ul>"},{"location":"troubleshooting/ssl-certificates/","title":"SSL Certificate Troubleshooting","text":"<p>This guide covers common SSL certificate issues and solutions for the iChrisBirch application using Traefik and mkcert.</p>"},{"location":"troubleshooting/ssl-certificates/#overview","title":"\ud83d\udd12 Overview","text":"<p>The application uses a two-tier certificate strategy:</p> <ol> <li>mkcert (Preferred): Browser-trusted certificates for local development</li> <li>OpenSSL (Fallback): Self-signed certificates when mkcert unavailable</li> </ol>"},{"location":"troubleshooting/ssl-certificates/#common-ssl-certificate-issues","title":"\ud83d\udea8 Common SSL Certificate Issues","text":""},{"location":"troubleshooting/ssl-certificates/#1-browser-security-warnings","title":"1. Browser Security Warnings","text":"<p>Problem: Browser shows \"Not Secure\" or \"Your connection is not private\"</p> <p>Error Messages:</p> <pre><code>NET::ERR_CERT_AUTHORITY_INVALID\nERR_SSL_KEY_USAGE_INCOMPATIBLE  \nSSL_ERROR_SELF_SIGNED_CERT\n</code></pre> <p>Root Cause: Using self-signed certificates without proper browser trust</p> <p>Resolution: Install and use mkcert for browser-trusted certificates</p> <pre><code># Install mkcert (macOS)\nbrew install mkcert\n\n# Install mkcert (Linux)\ncurl -JLO \"https://dl.filippo.io/mkcert/latest?for=linux/amd64\"\nchmod +x mkcert-v*-linux-amd64\nsudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\n# Install the local Certificate Authority\nmkcert -install\n\n# Regenerate certificates with mkcert\nichrisbirch ssl-manager generate dev\n\n# Restart environment to use new certificates\nichrisbirch dev restart\n</code></pre> <p>Prevention: Always use mkcert for local development to avoid browser warnings</p>"},{"location":"troubleshooting/ssl-certificates/#2-certificate-validation-errors","title":"2. Certificate Validation Errors","text":"<p>Problem: OpenSSL reports certificate validation failures</p> <p>Error Messages:</p> <pre><code>certificate verify failed: self signed certificate\nSSL routines:tls_process_server_certificate:certificate verify failed\n</code></pre> <p>Root Cause: Self-signed certificates don't have valid certificate chain</p> <p>Attempted Solutions (That Failed):</p> <ul> <li>Adding <code>-k</code> flag to curl commands (bypasses verification but doesn't solve browser issues)</li> <li>Manually importing certificates to browser (temporary fix, not scalable)</li> </ul> <p>Resolution: Use mkcert for properly signed certificates with valid certificate chain</p> <pre><code># Verify mkcert is installed and working\nmkcert -install\n\n# Check certificate authority status\nmkcert -CAROOT\n\n# Regenerate certificates\nichrisbirch ssl-manager generate dev\n\n# Verify certificate information\nichrisbirch ssl-manager info dev\n</code></pre> <p>Prevention: Always prefer mkcert over OpenSSL for local development certificates</p>"},{"location":"troubleshooting/ssl-certificates/#3-certificate-domain-mismatch","title":"3. Certificate Domain Mismatch","text":"<p>Problem: Certificate domain doesn't match requested domain</p> <p>Error Messages:</p> <pre><code>certificate is valid for *.example.com, not api.docker.localhost\nERR_CERT_COMMON_NAME_INVALID\n</code></pre> <p>Root Cause: Certificate Subject Alternative Names (SANs) don't include the requested domain</p> <p>Resolution: Regenerate certificates with proper domain coverage</p> <pre><code># Check which domains are covered by current certificate\nichrisbirch ssl-manager info dev\n\n# Regenerate with proper SANs (automatically includes all required domains)\nichrisbirch ssl-manager generate dev\n</code></pre> <p>Prevention: The ssl-manager script automatically includes all required domains when generating certificates</p>"},{"location":"troubleshooting/ssl-certificates/#4-certificate-expiration","title":"4. Certificate Expiration","text":"<p>Problem: Certificate has expired or is nearing expiration</p> <p>Error Messages:</p> <pre><code>certificate has expired\nSSL_ERROR_EXPIRED_CERT\n</code></pre> <p>Root Cause: Certificate validity period has passed</p> <p>Resolution: Regenerate certificates</p> <pre><code># Check certificate expiration\nichrisbirch ssl-manager info dev\n\n# Regenerate certificates (mkcert certificates last 2+ years)\nichrisbirch ssl-manager generate dev\n\n# Restart to use new certificates\nichrisbirch dev restart\n</code></pre> <p>Prevention: mkcert certificates have much longer validity (2+ years) compared to OpenSSL (365 days)</p>"},{"location":"troubleshooting/ssl-certificates/#5-mkcert-not-found","title":"5. mkcert Not Found","text":"<p>Problem: mkcert command not found when trying to generate certificates</p> <p>Error Messages:</p> <pre><code>mkcert: command not found\n[INFO] mkcert not found, falling back to OpenSSL\n</code></pre> <p>Root Cause: mkcert not installed on the system</p> <p>Resolution: Install mkcert</p> <pre><code># macOS installation\nbrew install mkcert\n\n# Linux installation\ncurl -JLO \"https://dl.filippo.io/mkcert/latest?for=linux/amd64\"\nchmod +x mkcert-v*-linux-amd64\nsudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert\n\n# Windows installation (using Chocolatey)\nchoco install mkcert\n\n# Verify installation\nmkcert -version\n\n# Install the local CA\nmkcert -install\n</code></pre> <p>Prevention: Install mkcert as part of initial development environment setup</p>"},{"location":"troubleshooting/ssl-certificates/#6-certificate-authority-not-trusted","title":"6. Certificate Authority Not Trusted","text":"<p>Problem: mkcert generates certificates but browsers still show warnings</p> <p>Error Messages:</p> <pre><code>NET::ERR_CERT_AUTHORITY_INVALID\nThe certificate is not trusted because the issuer certificate is unknown\n</code></pre> <p>Root Cause: mkcert local CA not installed in system trust stores</p> <p>Resolution: Install mkcert CA properly</p> <pre><code># Install the local Certificate Authority\nmkcert -install\n\n# Verify CA installation\nmkcert -CAROOT\n\n# Check if CA is in system trust store (macOS)\nsecurity find-certificate -c \"mkcert\" -p /Library/Keychains/System.keychain\n\n# Restart browser after CA installation\n# Chrome/Safari: Restart completely\n# Firefox: Restart or clear certificate cache\n</code></pre> <p>Prevention: Always run <code>mkcert -install</code> after installing mkcert</p>"},{"location":"troubleshooting/ssl-certificates/#7-wrong-certificate-file-permissions","title":"7. Wrong Certificate File Permissions","text":"<p>Problem: Traefik can't read certificate files</p> <p>Error Messages:</p> <pre><code>unable to load X509 key pair: open /etc/traefik/certs/dev.key: permission denied\n</code></pre> <p>Root Cause: Certificate files have incorrect permissions</p> <p>Resolution: Fix file permissions</p> <pre><code># Check current permissions\nls -la deploy-containers/traefik/certs/\n\n# Fix permissions (if needed)\nchmod 644 deploy-containers/traefik/certs/*.crt\nchmod 600 deploy-containers/traefik/certs/*.key\n\n# Restart Traefik\nichrisbirch dev restart\n</code></pre> <p>Prevention: The ssl-manager script sets proper permissions automatically</p>"},{"location":"troubleshooting/ssl-certificates/#8-certificate-file-missing","title":"8. Certificate File Missing","text":"<p>Problem: Traefik can't find certificate files</p> <p>Error Messages:</p> <pre><code>unable to load X509 key pair: open /etc/traefik/certs/dev.crt: no such file or directory\n</code></pre> <p>Root Cause: Certificate files not generated or in wrong location</p> <p>Resolution: Generate certificates</p> <pre><code># Check if certificates exist\nls -la deploy-containers/traefik/certs/\n\n# Generate missing certificates\nichrisbirch ssl-manager generate dev\n\n# Verify certificate files\nichrisbirch ssl-manager validate dev\n\n# Restart environment\nichrisbirch dev restart\n</code></pre> <p>Prevention: Always run certificate generation before starting environments</p>"},{"location":"troubleshooting/ssl-certificates/#advanced-certificate-debugging","title":"\ud83d\udd27 Advanced Certificate Debugging","text":""},{"location":"troubleshooting/ssl-certificates/#openssl-certificate-testing","title":"OpenSSL Certificate Testing","text":"<pre><code># Test certificate with OpenSSL\nopenssl s_client -connect api.docker.localhost:443 -servername api.docker.localhost\n\n# Check certificate details\nopenssl x509 -in deploy-containers/traefik/certs/dev.crt -text -noout\n\n# Verify certificate-key pair match\nopenssl x509 -noout -modulus -in deploy-containers/traefik/certs/dev.crt | openssl md5\nopenssl rsa -noout -modulus -in deploy-containers/traefik/certs/dev.key | openssl md5\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#browser-certificate-cache","title":"Browser Certificate Cache","text":"<pre><code># Clear Chrome certificate cache (macOS)\n# Chrome -&gt; Settings -&gt; Privacy and security -&gt; Clear browsing data -&gt; Advanced -&gt; Cached images and files\n\n# Clear Safari certificate cache (macOS)\n# Safari -&gt; Develop -&gt; Empty Caches\n\n# Clear Firefox certificate cache\n# Firefox -&gt; Settings -&gt; Privacy &amp; Security -&gt; Certificates -&gt; Clear\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#system-certificate-store-debugging","title":"System Certificate Store Debugging","text":"<pre><code># macOS: Check system certificate store\nsecurity find-certificate -c \"mkcert\" /Library/Keychains/System.keychain\n\n# Linux: Check certificate store\nls /usr/local/share/ca-certificates/\n\n# Windows: Check certificate store\ncertmgr.msc  # Run this to open Certificate Manager\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#ssl-manager-tool-usage","title":"\ud83d\udee0\ufe0f SSL Manager Tool Usage","text":""},{"location":"troubleshooting/ssl-certificates/#generate-certificates","title":"Generate Certificates","text":"<pre><code># Generate certificates for development (prefers mkcert)\nichrisbirch ssl-manager generate dev\n\n# Generate certificates for all environments\nichrisbirch ssl-manager generate all\n\n# Force OpenSSL generation (for testing)\nFORCE_OPENSSL=1 ichrisbirch ssl-manager generate dev\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#validate-certificates","title":"Validate Certificates","text":"<pre><code># Validate specific environment certificates\nichrisbirch ssl-manager validate dev\n\n# Validate all environment certificates\nichrisbirch ssl-manager validate all\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#certificate-information","title":"Certificate Information","text":"<pre><code># Show detailed certificate information\nichrisbirch ssl-manager info dev\n\n# Show certificate information for all environments\nichrisbirch ssl-manager info all\n</code></pre>"},{"location":"troubleshooting/ssl-certificates/#best-practices","title":"\ud83d\ude80 Best Practices","text":""},{"location":"troubleshooting/ssl-certificates/#development-environment-setup","title":"Development Environment Setup","text":"<ol> <li>Install mkcert first: Always install mkcert before generating certificates</li> <li>Install CA: Run <code>mkcert -install</code> to trust the local Certificate Authority</li> <li>Generate certificates: Use <code>ichrisbirch ssl-manager generate dev</code></li> <li>Verify browser trust: Test https://api.docker.localhost/ in browser</li> <li>Document for team: Ensure all developers follow the same setup</li> </ol>"},{"location":"troubleshooting/ssl-certificates/#certificate-management","title":"Certificate Management","text":"<ol> <li>Use mkcert for development: Avoid OpenSSL self-signed certificates</li> <li>Long validity periods: mkcert certificates last 2+ years</li> <li>Environment-specific certificates: Use separate certificates for dev/testing/prod</li> <li>Regular validation: Use <code>ssl-manager validate</code> to check certificate health</li> <li>Backup certificates: Keep certificate backups for team sharing</li> </ol>"},{"location":"troubleshooting/ssl-certificates/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":"<ol> <li>Check certificate existence: <code>ls deploy-containers/traefik/certs/</code></li> <li>Validate certificates: <code>ichrisbirch ssl-manager validate dev</code></li> <li>Check certificate info: <code>ichrisbirch ssl-manager info dev</code></li> <li>Test browser access: Visit https://api.docker.localhost/</li> <li>Check Traefik logs: <code>ichrisbirch dev logs traefik</code></li> <li>Regenerate if needed: <code>ichrisbirch ssl-manager generate dev</code></li> </ol>"},{"location":"troubleshooting/ssl-certificates/#security-considerations","title":"Security Considerations","text":"<ol> <li>Keep CA private: Don't share mkcert CA root key</li> <li>Environment isolation: Use separate certificates for each environment</li> <li>Regular updates: Update mkcert periodically</li> <li>Production certificates: Use proper CA-signed certificates for production</li> <li>Key protection: Protect private key files with appropriate permissions</li> </ol>"},{"location":"troubleshooting/ssl-certificates/#mkcert-benefits-over-openssl","title":"\ud83c\udf1f mkcert Benefits Over OpenSSL","text":""},{"location":"troubleshooting/ssl-certificates/#technical-advantages","title":"Technical Advantages","text":"Feature mkcert OpenSSL Self-Signed Browser Trust \u2705 Trusted automatically \u274c Requires manual trust Certificate Warnings \u2705 No warnings \u274c Security warnings Subject Alternative Names \u2705 Automatic wildcard + specific \u26a0\ufe0f Manual configuration Validity Period \u2705 2+ years \u26a0\ufe0f 365 days typical Cross-Platform \u2705 Works on all platforms \u26a0\ufe0f Platform-specific trust Team Sharing \u2705 Same CA for all developers \u274c Individual trust required"},{"location":"troubleshooting/ssl-certificates/#user-experience-improvements","title":"User Experience Improvements","text":"<ol> <li>No security warnings: Browsers trust mkcert certificates immediately</li> <li>Professional development: Development environment matches production behavior</li> <li>Team consistency: All developers use the same trusted certificates</li> <li>Faster onboarding: New developers don't need to manually trust certificates</li> <li>Better testing: Can test SSL-specific features without bypassing verification</li> </ol> <p>The SSL certificate strategy using mkcert provides a professional, secure foundation for local development with excellent browser compatibility and team collaboration benefits.</p>"},{"location":"troubleshooting/testing-issues/","title":"Testing Environment Troubleshooting","text":"<p>This document addresses issues encountered when setting up and running tests in the iChrisBirch project, particularly with Docker-based testing environments and database configuration.</p>"},{"location":"troubleshooting/testing-issues/#recent-critical-issues","title":"Recent Critical Issues","text":""},{"location":"troubleshooting/testing-issues/#docker-network-conflicts-during-testing","title":"Docker Network Conflicts During Testing","text":"<p>Problem: Test runs fail with Docker networking errors like:</p> <pre><code>Error response from daemon: failed to set up container networking: network [network-id] not found\n</code></pre> <p>Error Messages:</p> <pre><code>Error response from daemon: failed to set up container networking: network 0c3cd93080df5b4a0c4308df29659328a317c458853c34c473470e35abf5f31a not found\n\u2717 Tests failed (exit code: 1)\n</code></pre> <p>Root Cause: Previous test runs left orphaned Docker containers, networks, and volumes that weren't properly cleaned up. The original cleanup in the CLI script was:</p> <pre><code>docker-compose -f docker-compose.yml -f docker-compose.test.yml down -v &gt;/dev/null 2&gt;&amp;1\n</code></pre> <p>This silently suppressed cleanup errors, so failed cleanup operations went unnoticed, leading to resource conflicts on subsequent test runs.</p> <p>Attempted Solutions (That Failed):</p> <ul> <li>Running <code>docker system prune</code> manually between tests - temporary fix but not automated</li> <li>Checking for specific container names only - missed containers with dynamic names  </li> <li>Using docker-compose down without additional cleanup - didn't handle edge cases where compose cleanup failed</li> </ul> <p>Resolution: Implemented container reuse strategy with database reset:</p> <p>Test run command (<code>cli/ichrisbirch:208-259</code>):</p> <pre><code>function test-run() {\n  cd \"$ICHRISBIRCH_HOME\" || exit\n  source \"$ICHRISBIRCH_HOME/.venv/bin/activate\"\n  export ENVIRONMENT=testing\n\n  # Check if containers are already running and healthy\n  local containers_ready=false\n  if $COMPOSE_TEST ps --status running -q 2&gt;/dev/null | grep -q .; then\n    echo \"Test containers already running, checking health...\"\n    if $COMPOSE_TEST ps | grep -q \"healthy\"; then\n      containers_ready=true\n      echo \"Containers healthy, reusing them\"\n    else\n      echo \"Containers unhealthy, restarting...\"\n      $COMPOSE_TEST down --volumes 2&gt;/dev/null || true\n    fi\n  fi\n\n  if [ \"$containers_ready\" = false ]; then\n    ensure-proxy-network\n    echo \"Starting test containers...\"\n    $COMPOSE_TEST up -d\n    sleep 15\n  fi\n\n  # Always reset database to ensure clean state\n  echo \"Initializing test database...\"\n  ENVIRONMENT=testing uv run python -m ichrisbirch.database.initialization \\\n    --env testing --db-host localhost --db-port 5434\n\n  # Run pytest\n  uv run pytest \"$@\"\n\n  # Leave containers running for fast iteration\n  echo \"Containers left running for fast iteration.\"\n  echo \"Stop them with: icb testing stop\"\n}\n</code></pre> <p>Key improvements:</p> <ol> <li>Container reuse: Reuses healthy containers instead of recreating them each run</li> <li>Health checking: Verifies container health before reusing</li> <li>Database reset: Always reinitializes database for clean test state</li> <li>Fast iteration: Leaves containers running after tests for quick re-runs</li> <li>Unhealthy recovery: Restarts containers if they're unhealthy</li> </ol> <p>Prevention: The container reuse approach prevents network conflicts because containers are not constantly being created and destroyed. The database reset ensures clean state without the overhead of container recreation.</p> <p>Manual cleanup when needed:</p> <pre><code># Stop and remove all test containers and volumes\n./cli/ichrisbirch testing stop\n\n# Full Docker cleanup if issues persist\ndocker compose -f docker-compose.yml -f docker-compose.test.yml \\\n  --project-name icb-test down -v --remove-orphans\ndocker network prune -f\n</code></pre>"},{"location":"troubleshooting/testing-issues/#test-database-setup-issues","title":"Test Database Setup Issues","text":""},{"location":"troubleshooting/testing-issues/#schema-creation-failures","title":"Schema Creation Failures","text":"<p>Problem: Tests fail because the database schema doesn't exist.</p> <p>Error Messages:</p> <pre><code>psycopg2.errors.InvalidSchemaName: schema \"ichrisbirch_test\" does not exist\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \"users\" does not exist\n</code></pre> <p>Root Cause: Test database doesn't have the required schema or tables created.</p> <p>Resolution:</p> <ol> <li>Ensure schema environment variable is set:</li> </ol> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    environment:\n      - POSTGRES_DB_SCHEMA=ichrisbirch_test\n      - DATABASE_URL=postgresql://ichrisbirch:password@postgres:5432/ichrisbirch_test\n</code></pre> <ol> <li>Run Alembic migrations in test setup:</li> </ol> <pre><code># In your test configuration\nfrom alembic import command\nfrom alembic.config import Config\nfrom ichrisbirch.database import get_sqlalchemy_session\n\ndef setup_test_database():\n    # Run migrations to create schema\n    alembic_cfg = Config(\"alembic.ini\")\n    command.upgrade(alembic_cfg, \"head\")\n</code></pre> <ol> <li>Verify database connection:</li> </ol> <pre><code># Test database connectivity\ndocker-compose -f docker-compose.test.yml run test-runner uv run python -c \"\nfrom ichrisbirch.config import settings\nprint(f'Database URL: {settings.database_url}')\n\"\n</code></pre>"},{"location":"troubleshooting/testing-issues/#test-data-isolation-issues","title":"Test Data Isolation Issues","text":"<p>Problem: Tests interfere with each other due to shared test data.</p> <p>Symptoms:</p> <ul> <li>Tests pass individually but fail when run together</li> <li>Inconsistent test results</li> <li>Foreign key constraint violations</li> </ul> <p>Resolution:</p> <p>Use proper test fixtures with cleanup:</p> <pre><code>@pytest.fixture(autouse=True)\ndef insert_testing_data():\n    \"\"\"Setup test data before each test module.\"\"\"\n    from ichrisbirch.database.testing import insert_test_data, delete_test_data\n\n    # Setup\n    insert_test_data('users', 'habits', 'habitcategories')\n\n    yield\n\n    # Teardown\n    delete_test_data('habits', 'habitcategories', 'users')\n</code></pre>"},{"location":"troubleshooting/testing-issues/#docker-test-environment-issues","title":"Docker Test Environment Issues","text":""},{"location":"troubleshooting/testing-issues/#container-startup-problems","title":"Container Startup Problems","text":"<p>Problem: Test containers fail to start or exit immediately.</p> <p>Common Causes:</p> <ol> <li>Missing test dependencies:</li> </ol> <pre><code># Ensure test dependencies are installed\nFROM builder as development\nCOPY . .\nRUN uv sync --frozen --group dev --group test\n</code></pre> <ol> <li>Database service not ready:</li> </ol> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ichrisbirch\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n      start_period: 10s\n</code></pre>"},{"location":"troubleshooting/testing-issues/#test-command-execution-issues","title":"Test Command Execution Issues","text":"<p>Problem: Pytest command not found or fails to execute.</p> <p>Error: <code>/app/.venv/bin/pytest: No such file or directory</code></p> <p>Cause: Virtual environment issues or missing test runner.</p> <p>Resolution:</p> <p>Use UV to run tests:</p> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    command: [\"uv\", \"run\", \"pytest\", \"-vv\", \"--cov=ichrisbirch\"]\n</code></pre> <p>Verify pytest is available:</p> <pre><code>docker-compose -f docker-compose.test.yml run test-runner uv run which pytest\n</code></pre>"},{"location":"troubleshooting/testing-issues/#coverage-and-reporting-issues","title":"Coverage and Reporting Issues","text":""},{"location":"troubleshooting/testing-issues/#missing-coverage-reports","title":"Missing Coverage Reports","text":"<p>Problem: Coverage reports are not generated or are empty.</p> <p>Common Issues:</p> <ol> <li>Missing coverage configuration:</li> </ol> <pre><code># pyproject.toml\n[tool.coverage.run]\nsource = [\"ichrisbirch\"]\nomit = [\n    \"*/tests/*\",\n    \"*/venv/*\",\n    \"*/__pycache__/*\"\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\"\n]\n</code></pre> <ol> <li>Coverage data location issues:</li> </ol> <pre><code># Ensure coverage data is in correct location\ndocker-compose -f docker-compose.test.yml run test-runner uv run coverage report\n</code></pre>"},{"location":"troubleshooting/testing-issues/#html-coverage-reports","title":"HTML Coverage Reports","text":"<p>Problem: HTML coverage reports are not accessible.</p> <p>Resolution:</p> <p>Mount volume to access reports from host:</p> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    volumes:\n      - ./test-results:/app/test-results\n    command: [\"uv\", \"run\", \"pytest\", \"--cov=ichrisbirch\", \"--cov-report=html:test-results/coverage\"]\n</code></pre>"},{"location":"troubleshooting/testing-issues/#performance-and-timeout-issues","title":"Performance and Timeout Issues","text":""},{"location":"troubleshooting/testing-issues/#slow-test-execution","title":"Slow Test Execution","text":"<p>Problem: Tests take too long to run, causing timeouts.</p> <p>Common Causes:</p> <ol> <li>Database connection overhead</li> <li>Inefficient test data setup</li> <li>Missing test database optimization</li> </ol> <p>Optimizations:</p> <pre><code># Use database transactions for faster rollback\n@pytest.fixture(autouse=True)\ndef db_transaction():\n    \"\"\"Use transaction rollback instead of DELETE for cleanup.\"\"\"\n    from ichrisbirch.database import get_sqlalchemy_session\n\n    with get_sqlalchemy_session() as session:\n        transaction = session.begin()\n        yield session\n        transaction.rollback()\n</code></pre>"},{"location":"troubleshooting/testing-issues/#memory-issues","title":"Memory Issues","text":"<p>Problem: Tests fail due to memory limitations.</p> <p>Resolution:</p> <p>Set appropriate resource limits:</p> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n        reservations:\n          memory: 512M\n</code></pre>"},{"location":"troubleshooting/testing-issues/#environment-specific-issues","title":"Environment-Specific Issues","text":""},{"location":"troubleshooting/testing-issues/#local-vs-container-differences","title":"Local vs Container Differences","text":"<p>Problem: Tests pass locally but fail in containers.</p> <p>Common Causes:</p> <ol> <li>Path differences: Absolute vs relative paths</li> <li>Environment variables: Missing in container</li> <li>File permissions: User/group differences</li> </ol> <p>Resolution:</p> <ol> <li>Standardize paths:</li> </ol> <pre><code># Use pathlib for cross-platform compatibility\nfrom pathlib import Path\n\nBASE_DIR = Path(__file__).parent.parent\nTEST_DATA_DIR = BASE_DIR / \"tests\" / \"data\"\n</code></pre> <ol> <li>Document required environment variables:</li> </ol> <pre><code># .env.test\nPOSTGRES_DB_SCHEMA=ichrisbirch_test\nDATABASE_URL=postgresql://ichrisbirch:password@postgres:5432/ichrisbirch_test\nAPI_URL=http://api:8000\nFLASK_ENV=testing\n</code></pre>"},{"location":"troubleshooting/testing-issues/#cicd-pipeline-issues","title":"CI/CD Pipeline Issues","text":"<p>Problem: Tests pass locally but fail in GitHub Actions.</p> <p>Common Issues:</p> <ol> <li>Service dependencies not ready</li> <li>Different Python/dependency versions</li> <li>Missing environment setup</li> </ol> <p>Resolution:</p> <pre><code># .github/workflows/test.yml\nname: Test\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up test environment\n        run: |\n          cp .env.test.example .env.test\n          docker-compose -f docker-compose.test.yml up -d postgres\n\n      - name: Wait for database\n        run: |\n          docker-compose -f docker-compose.test.yml run --rm test-runner \\\n            sh -c 'until pg_isready -h postgres -p 5432; do sleep 1; done'\n\n      - name: Run tests\n        run: docker-compose -f docker-compose.test.yml up test-runner\n</code></pre>"},{"location":"troubleshooting/testing-issues/#debugging-test-issues","title":"Debugging Test Issues","text":""},{"location":"troubleshooting/testing-issues/#test-failure-diagnosis","title":"Test Failure Diagnosis","text":"<p>Steps to debug failing tests:</p> <ol> <li>Run single test in isolation:</li> </ol> <pre><code>docker-compose -f docker-compose.test.yml run test-runner \\\n  uv run pytest tests/specific_test.py::TestClass::test_method -vv\n</code></pre> <ol> <li>Check database state:</li> </ol> <pre><code>docker-compose -f docker-compose.test.yml run test-runner \\\n  uv run python -c \"\nfrom ichrisbirch.database import get_sqlalchemy_session\nwith get_sqlalchemy_session() as session:\n    result = session.execute('SELECT COUNT(*) FROM users')\n    print(f'Users count: {result.scalar()}')\n\"\n</code></pre> <ol> <li>Examine test logs:</li> </ol> <pre><code>docker-compose -f docker-compose.test.yml logs test-runner\ndocker-compose -f docker-compose.test.yml logs postgres\n</code></pre>"},{"location":"troubleshooting/testing-issues/#database-connection-debugging","title":"Database Connection Debugging","text":"<p>Test database connectivity:</p> <pre><code># debug_db.py\nfrom ichrisbirch.config import settings\nfrom ichrisbirch.database import get_sqlalchemy_session\n\nprint(f\"Database URL: {settings.database_url}\")\n\ntry:\n    with get_sqlalchemy_session() as session:\n        result = session.execute(\"SELECT version()\")\n        print(f\"PostgreSQL version: {result.scalar()}\")\n        print(\"Database connection successful!\")\nexcept Exception as e:\n    print(f\"Database connection failed: {e}\")\n</code></pre> <p>Run with:</p> <pre><code>docker-compose -f docker-compose.test.yml run test-runner uv run python debug_db.py\n</code></pre>"},{"location":"troubleshooting/testing-issues/#prevention-strategies","title":"Prevention Strategies","text":""},{"location":"troubleshooting/testing-issues/#test-environment-validation","title":"Test Environment Validation","text":"<p>Create a test environment validation script:</p> <pre><code># validate_test_env.py\nimport sys\nfrom ichrisbirch.config import settings\nfrom ichrisbirch.database import get_sqlalchemy_session\n\ndef validate_test_environment():\n    \"\"\"Validate that test environment is properly configured.\"\"\"\n    errors = []\n\n    # Check required environment variables\n    if not settings.database_url:\n        errors.append(\"DATABASE_URL not set\")\n\n    if \"test\" not in settings.database_url:\n        errors.append(\"DATABASE_URL should contain 'test'\")\n\n    # Test database connection\n    try:\n        with get_sqlalchemy_session() as session:\n            session.execute(\"SELECT 1\")\n    except Exception as e:\n        errors.append(f\"Database connection failed: {e}\")\n\n    if errors:\n        print(\"Test environment validation failed:\")\n        for error in errors:\n            print(f\"  - {error}\")\n        sys.exit(1)\n    else:\n        print(\"Test environment validation passed!\")\n\nif __name__ == \"__main__\":\n    validate_test_environment()\n</code></pre>"},{"location":"troubleshooting/testing-issues/#automated-test-environment-setup","title":"Automated Test Environment Setup","text":"<pre><code>#!/bin/bash\n# scripts/setup_test_env.sh\n\nset -e\n\necho \"Setting up test environment...\"\n\n# Copy test environment file\ncp .env.test.example .env.test\n\n# Build test images\ndocker-compose -f docker-compose.test.yml build\n\n# Start database\ndocker-compose -f docker-compose.test.yml up -d postgres\n\n# Wait for database to be ready\necho \"Waiting for database...\"\ndocker-compose -f docker-compose.test.yml run --rm test-runner \\\n  sh -c 'until pg_isready -h postgres -p 5432; do sleep 1; done'\n\n# Run migrations\ndocker-compose -f docker-compose.test.yml run --rm test-runner \\\n  uv run alembic upgrade head\n\n# Validate environment\ndocker-compose -f docker-compose.test.yml run --rm test-runner \\\n  uv run python validate_test_env.py\n\necho \"Test environment setup complete!\"\n</code></pre>"},{"location":"troubleshooting/testing-issues/#common-error-messages-and-solutions","title":"Common Error Messages and Solutions","text":"Error Message Cause Solution <code>schema \"ichrisbirch_test\" does not exist</code> Missing schema environment variable Set <code>POSTGRES_DB_SCHEMA=ichrisbirch_test</code> <code>relation \"users\" does not exist</code> Database migrations not run Run <code>alembic upgrade head</code> <code>pytest: not found</code> Missing test dependencies Install with <code>uv sync --group test</code> <code>Connection refused</code> Database not ready Add health check and depends_on <code>permission denied</code> File permission issues Set proper user/group in Docker <code>No module named 'ichrisbirch'</code> Package not installed Run <code>uv sync</code> to install package <code>error: Failed to spawn: 'pytest'</code> Test dependencies not installed in Docker build Add <code>--group test</code> to Dockerfile <code>uv sync</code> <code>service has neither an image nor a build context</code> Missing build directive in compose file Add build context to service definition <code>ModuleNotFoundError: No module named 'tests.utils.environment'</code> Incorrect import path Fix import path to correct module location"},{"location":"troubleshooting/testing-issues/#recent-critical-issues-july-2025","title":"Recent Critical Issues (July 2025)","text":""},{"location":"troubleshooting/testing-issues/#test-dependencies-missing-in-docker-build","title":"Test Dependencies Missing in Docker Build","text":"<p>Problem: Docker-based test runner fails with <code>error: Failed to spawn: 'pytest'</code> even though pytest is defined in pyproject.toml dependency groups.</p> <p>Error Messages:</p> <pre><code>test-runner-1  | error: Failed to spawn: `pytest`\ntest-runner-1  |   Caused by: No such file or directory (os error 2)\ntest-runner-1 exited with code 2\n</code></pre> <p>Root Cause: The Dockerfile development stage was not installing test dependency groups. The <code>uv sync --frozen</code> command only installs main dependencies, not test dependencies.</p> <p>Attempted Solutions (That Failed):</p> <ul> <li>Manually running <code>uv sync --group test</code> in running container worked, but didn't persist in built image</li> <li>Rebuilding without <code>--no-cache</code> didn't fix the issue</li> <li>Installing pytest directly didn't address the underlying group installation problem</li> </ul> <p>Resolution:</p> <p>Update the Dockerfile development stage to include dependency groups:</p> <pre><code># Before (broken):\nRUN uv venv &amp;&amp; \\\n    uv sync --frozen &amp;&amp; \\\n    chown -R appuser:appuser /app/.venv /app\n\n# After (working):\nRUN uv venv &amp;&amp; \\\n    uv sync --frozen --group dev --group test &amp;&amp; \\\n    chown -R appuser:appuser /app/.venv /app\n</code></pre> <p>Then rebuild with <code>--no-cache</code>:</p> <pre><code>docker-compose -f docker-compose.test.yml build --no-cache test-runner\n</code></pre> <p>Prevention: Always include <code>--group dev --group test</code> flags when installing dependencies in development/test Docker stages.</p>"},{"location":"troubleshooting/testing-issues/#missing-build-context-in-docker-compose-services","title":"Missing Build Context in Docker Compose Services","text":"<p>Problem: Docker Compose fails with \"service has neither an image nor a build context specified\" for scheduler service.</p> <p>Error Messages:</p> <pre><code>service \"scheduler\" has neither an image nor a build context specified: invalid compose project\n</code></pre> <p>Root Cause: The test compose file was overriding the scheduler service configuration but omitted the required <code>build</code> directive that was present in the base compose file.</p> <p>Resolution:</p> <p>Add build context to the problematic service in <code>docker-compose.test.yml</code>:</p> <pre><code># Before (broken):\nscheduler:\n  command: .venv/bin/python -m ichrisbirch.wsgi_scheduler\n  environment:\n    - ENVIRONMENT=testing\n    # ... other config\n\n# After (working):\nscheduler:\n  build:\n    context: .\n    target: development\n  command: .venv/bin/python -m ichrisbirch.wsgi_scheduler\n  environment:\n    - ENVIRONMENT=testing\n    # ... other config\n</code></pre> <p>Prevention: When overriding services in compose override files, ensure all required directives (build, image, etc.) are included.</p>"},{"location":"troubleshooting/testing-issues/#incorrect-module-import-paths","title":"Incorrect Module Import Paths","text":"<p>Problem: Test runner fails with <code>ModuleNotFoundError</code> for test utility modules.</p> <p>Error Messages:</p> <pre><code>ImportError while loading conftest '/app/tests/conftest.py'.\ntests/conftest.py:26: in &lt;module&gt;\n    from tests.utils.environment import TestEnvironment\nE   ModuleNotFoundError: No module named 'tests.utils.environment'\n</code></pre> <p>Root Cause: Import statement referenced wrong module path. The actual module was at <code>tests/environment.py</code>, not <code>tests/utils/environment.py</code>.</p> <p>Resolution:</p> <p>Fix the import path in <code>tests/conftest.py</code>:</p> <pre><code># Before (broken):\nfrom tests.utils.environment import TestEnvironment\n\n# After (working):\nfrom tests.environment import TestEnvironment\n</code></pre> <p>Prevention:</p> <ul> <li>Use IDE auto-completion for imports</li> <li>Verify module structure before adding imports</li> <li>Add import validation to pre-commit hooks</li> </ul>"},{"location":"troubleshooting/testing-issues/#docker-network-conflicts","title":"Docker Network Conflicts","text":"<p>Problem: Intermittent networking failures with \"network not found\" errors during test execution.</p> <p>Error Messages:</p> <pre><code>Error response from daemon: failed to set up container networking: network 29c640af9598b69b0e85acbdc4c59293e181060841e68af175b51c13cd045f79 not found\n</code></pre> <p>Root Cause: Orphaned Docker networks and containers from previous failed runs interfering with new test executions.</p> <p>Resolution:</p> <p>Comprehensive Docker cleanup:</p> <pre><code># Stop all test containers\ndocker-compose -f docker-compose.yml -f docker-compose.test.yml down --remove-orphans\n\n# Clean up containers, networks, and build cache\ndocker container prune -f\ndocker network prune -f\ndocker system prune -f  # Use with caution - removes unused images\n</code></pre> <p>Prevention:</p> <ul> <li>Always use <code>--remove-orphans</code> flag when stopping compose</li> <li>Regular cleanup of Docker resources</li> <li>Add cleanup commands to test scripts</li> </ul>"},{"location":"troubleshooting/testing-issues/#complete-resolution-workflow","title":"Complete Resolution Workflow","text":"<p>For comprehensive testing issues, follow this workflow:</p> <ol> <li>Clean Docker environment:</li> </ol> <pre><code>docker-compose -f docker-compose.yml -f docker-compose.test.yml down -v --remove-orphans\ndocker network prune -f\n</code></pre> <ol> <li>Verify and fix Dockerfile:</li> </ol> <pre><code># Check development stage includes test groups\ngrep -A 5 \"uv sync\" Dockerfile\n# Should show: uv sync --frozen --group dev --group test\n</code></pre> <ol> <li>Verify compose service definitions:</li> </ol> <pre><code># Check all services have build or image specified\ndocker-compose -f docker-compose.yml -f docker-compose.test.yml config\n</code></pre> <ol> <li>Fix import paths:</li> </ol> <pre><code># Verify module exists\nfind tests/ -name \"*.py\" | grep -E \"(environment|conftest)\"\n</code></pre> <ol> <li>Rebuild and test:</li> </ol> <pre><code>docker-compose -f docker-compose.test.yml build --no-cache test-runner\nichrisbirch test\n</code></pre>"},{"location":"troubleshooting/testing-issues/#test-checklist","title":"Test Checklist","text":"<p>Before running tests, verify:</p> <ul> <li> Test database is running and accessible</li> <li> Schema exists and migrations are current</li> <li> Test dependencies are installed</li> <li> Environment variables are set correctly</li> <li> Test data fixtures are working</li> <li> Coverage configuration is correct</li> <li> Resource limits are appropriate for test load</li> </ul>"}]}