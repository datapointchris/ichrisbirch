{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"iChrisBirch Documentation","text":"<p>Welcome to the iChrisBirch application documentation.</p>"},{"location":"#architecture","title":"Architecture","text":"<ul> <li>Project Layout - Overall project structure and organization</li> <li>Configuration - Environment and settings management</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Developer Setup - Getting started with development</li> <li>Testing - Testing strategy and guidelines</li> <li>Adding New Apps - How to add new features</li> </ul>"},{"location":"#docker-deployment","title":"Docker &amp; Deployment","text":"<ul> <li>Docker Development - Docker Compose development workflow</li> <li>Docker Documentation - Comprehensive Docker containerization guide</li> <li>Docker Architecture - Multi-stage Docker builds and container strategy</li> <li>Docker Compose - Service orchestration across environments</li> <li>Docker Quick Reference - Common commands and troubleshooting</li> </ul>"},{"location":"#devops","title":"DevOps","text":"<ul> <li>CI/CD - Continuous integration and deployment</li> <li>AWS - Cloud infrastructure and services</li> <li>Terraform - Infrastructure as code</li> <li>Backups - Data backup and recovery</li> </ul>"},{"location":"#api","title":"API","text":"<ul> <li>API Documentation - Backend API reference</li> <li>Authentication Architecture - Modern API key authentication system</li> <li>Authentication Strategies - Auth implementation details</li> </ul>"},{"location":"#frontend","title":"Frontend","text":"<ul> <li>CSS - Styling guidelines and architecture</li> <li>CSS BEM - BEM methodology for CSS</li> <li>HTML5 Semantic - Semantic HTML structure</li> </ul>"},{"location":"#tools-utilities","title":"Tools &amp; Utilities","text":"<ul> <li>Alembic - Database migrations</li> <li>Scheduler - Background job processing</li> <li>Documentation Tools - Docs generation and maintenance</li> <li>Troubleshooting - Common issues and solutions</li> </ul> <p>Docs here</p>"},{"location":"AWS/","title":"AWS","text":""},{"location":"AWS/#iam","title":"IAM","text":""},{"location":"AWS/#groups","title":"Groups","text":"<p><code>admin</code> - Administrator access <code>developer</code> - Access to services</p> <ul> <li>GROUP: <code>admin</code></li> <li>POLICY: <code>AWSAdministratorAccess</code></li> <li>GROUP: <code>developer</code> group has POLICIES</li> <li>POLICY: <code>AllowPassRoleS3DatabaseBackups</code><ul> <li>Allow to assume the <code>S3DatabaseBackups</code> ROLE</li> <li>ROLE: <code>S3DatabaseBackups</code> - S3 Full Access</li> </ul> </li> <li>POLICY: <code>AWSKeyManagementServiceUser</code><ul> <li>Allow to view and use all KMS keys</li> <li>Allow grant to AWSResources (like S3)</li> </ul> </li> <li>POLICY: <code>AmazonRDSFullAccess</code></li> <li>POLICY: <code>AmazonS3FullAccess</code></li> <li>POLICY: <code>AmazonDynamoDBFullAccess</code></li> <li>ROLE: <code>AWSTrustedAdvisorRole</code></li> </ul> <p></p>"},{"location":"AWS/#ec2","title":"EC2","text":""},{"location":"AWS/#ichrisbirch-instances","title":"ichrisbirch instances","text":"<p>US East 1 Security Group: ichrisbirch-sg Key name: ichrisbirch-webserver Ubuntu 22.04</p>"},{"location":"add_new_app/","title":"Adding A New Application","text":"<p>For this document example we will be creating a new app called <code>Items</code></p>  db table <code>items</code>  sqlalchemy model <code>Item</code>  pydantic schema <code>Item</code>  app endpoint <code>/items</code>  api endpoint <code>/items/</code>"},{"location":"add_new_app/#sqlalchemy-model","title":"Sqlalchemy Model","text":"<p> Import new models into <code>ichrisbirch/alembic/env.py</code> </p> <p> Import new models into <code>ichrisbirch/models/__init__.py</code> For easy reference from the module level.</p> <pre><code>from ichrisbirch import models\n\nitem = models.Item(**data)\n</code></pre>"},{"location":"add_new_app/#pydantic-schema","title":"Pydantic Schema","text":"<p> Import new schemas into <code>ichrisbirch/schemas/__init__.py</code> For easy reference from the module level.</p> <pre><code>from ichrisbirch import schemas\n\nitem = schemas.ItemCreate(**data)\n</code></pre>"},{"location":"add_new_app/#application-blueprint","title":"Application Blueprint","text":""},{"location":"add_new_app/#app-routes","title":"App Routes","text":""},{"location":"add_new_app/#application-blueprint-to-app-factory","title":"Application Blueprint to App Factory","text":""},{"location":"add_new_app/#api-router","title":"API Router","text":""},{"location":"add_new_app/#api-endpoints","title":"API Endpoints","text":"<p> Import in <code>ichrisbirch/schemas/__init__.py</code> For easy reference from the module level.</p>"},{"location":"add_new_app/#api-router-to-api-factory","title":"API Router to API Factory","text":""},{"location":"add_new_app/#html-basehtml-and-indexhtml","title":"HTML base.html and index.html","text":""},{"location":"add_new_app/#navigation-link","title":"Navigation Link","text":"<p> Add link to navigation in <code>ichrisbirch/app/templates/base.html</code></p>"},{"location":"add_new_app/#stylesheet","title":"Stylesheet","text":""},{"location":"add_new_app/#tests","title":"Tests","text":""},{"location":"add_new_app/#testing-data","title":"Testing Data","text":"<p>:material-test: Add testing data into <code>tests/testing_data</code>  Import the test data in <code>tests/testing_data/__init__.py</code>  Add testing data to <code>tests/conftest.py/get_test_data()</code></p>"},{"location":"add_new_app/#api-endpoints-tests","title":"API Endpoints Tests","text":""},{"location":"add_new_app/#app-routes-tests","title":"App Routes Tests","text":""},{"location":"add_new_app/#frontend-tests","title":"Frontend Tests","text":""},{"location":"alembic/","title":"Alembic Revision","text":"<p>Run in <code>ichrisbirch/ichrisbirch</code> (where <code>alembic.ini</code> is located)</p>"},{"location":"alembic/#first-run","title":"First Run","text":"<pre><code>export ENVIRONMENT='development' # or 'production'\nalembic revision --autogenerate -m 'Create initial tables'\nalembic upgrade head\n</code></pre>"},{"location":"alembic/#subsequent-runs","title":"Subsequent Runs","text":"<ol> <li> <p>Make the changes to the models and schemas</p> </li> <li> <p>Run a revision to pickup changes in code <code>alembic revision --autogenerate -m 'Add notes field to tasks table'</code></p> <p>Note If this doesn't work perfectly, you must edit the revision file</p> </li> <li> <p>Run the upgrade in the environments</p> </li> </ol> <p>Locally</p> <pre><code>export ENVIRONMENT='development'\nalembic upgrade head\n</code></pre> <p>EC2</p> <pre><code>export ENVIRONMENT='production'\nalembic upgrade head\n</code></pre>"},{"location":"alembic/#troubleshooting","title":"Troubleshooting","text":"<p>Error Alembic is not able to upgrade to the latest because the revisions got out of sync.  </p> <p>Solution Find the last revision that was successfully run (manually by inspecting the database) and then run: <code>alembic stamp &lt;revision&gt;</code> to set the current revision to the last successful one. Then run the upgrade again: <code>alembic upgrade head</code></p>"},{"location":"alembic/#sqlalchemy-create_all-vs-alembic-upgrade","title":"sqlalchemy create_all vs alembic upgrade","text":"<p><code>SQLAlchemy</code> and <code>Alembic</code> are two powerful tools in the Python ecosystem used for database handling and migrations, respectively. They are often used together in projects to manage database schemas and perform database operations. Understanding the difference between <code>SQLAlchemy</code>'s <code>create_all</code> method and <code>Alembic</code>'s <code>upgrade</code> function is crucial for effectively managing database schema changes and migrations.</p>"},{"location":"alembic/#sqlalchemycreate_all","title":"<code>SQLAlchemy.create_all</code>","text":"<p><code>SQLAlchemy</code> is an SQL toolkit and Object-Relational Mapping (ORM) library for Python. It provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access.</p> <ul> <li>What it does: The <code>create_all</code> method in <code>SQLAlchemy</code> is used to create all tables that have been defined in your SQLAlchemy models but don't yet exist in the database. It doesn't consider the current state of the database schema. Instead, it blindly attempts to create all the tables (and associated schema elements like indexes) based on the models you've defined. If a table already exists, it simply skips the creation for that table.</li> <li>Usage scenario: <code>create_all</code> is particularly useful in simple projects or during the initial setup of a project's database where you are starting with an empty database and want to construct the schema based on your models' definitions.</li> </ul>"},{"location":"alembic/#alembicupgrade","title":"<code>Alembic.upgrade</code>","text":"<p><code>Alembic</code> is a lightweight database migration tool for usage with <code>SQLAlchemy</code>. It allows you to manage changes to your database schema over time, enabling versioning of your database similarly to how you version your source code.</p> <ul> <li>What it does: The <code>upgrade</code> function in Alembic applies one or more migrations (changes) to the database schema, moving it to a new version. These migrations are written as scripts which define how to apply a change (e.g., add a table, alter a column) and how to revert it. The <code>upgrade</code> command considers the current version of your database and applies all new migrations in sequence up to the latest version or to a specified version.</li> <li>Usage scenario: <code>Alembic.upgrade</code> is used in iterative development and production environments where the state of the database schema evolves over time. It ensures that schema changes are applied in a controlled and versioned manner, allowing for smooth transitions across different versions of your schema as your application grows and changes.</li> </ul>"},{"location":"alembic/#key-differences","title":"Key Differences","text":"<ul> <li>Version control: Alembic allows for version-controlled schema changes, making it possible to migrate your database schema forwards or backwards as needed. <code>SQLAlchemy.create_all</code> does not consider versions of your schema.</li> <li>Sensitivity to existing schema: <code>create_all</code> essentially ignores the current schema state (it won't modify or delete existing tables), while Alembic <code>upgrade</code> scripts can be tailored to alter the current schema precisely and incrementally.</li> <li>Purpose and scope: <code>create_all</code> is a more blunt instrument, best suited for initial schema creation. Alembic, with its <code>upgrade</code> (and corresponding <code>downgrade</code>) commands, supports a more nuanced and controlled approach to database schema evolution.</li> </ul> <p>In summary, while <code>SQLAlchemy.create_all</code> is useful for initial schema creation in simple scenarios, <code>Alembic.upgrade</code> provides a robust framework for managing schema changes over time in a version-controlled, incremental, and reversible manner. For complex projects and in production environments, integrating Alembic for migration management alongside SQLAlchemy for ORM capabilities is considered best practice.</p>"},{"location":"alembic/#how-to-deal-with-database-that-has-got-out-of-sync-with-alembic-revisions-and-alembic-report-target-database-is-not-up-to-date-how-to-find-what-version-of-the-revision-the-database-matches","title":"How to deal with database that has got out of sync with alembic revisions and alembic report Target database is not up to date.  How to find what version of the revision the database matches","text":"<p>When your database schema has gotten out of sync with Alembic revisions, the message \"Target database is not up to date\" typically indicates Alembic detects mismatches between the expected schema version (from your migration scripts) and the current state of your database. Handling this scenario involves a few steps to identify the disparity and resolve it. Here's how you can approach this situation:</p>"},{"location":"alembic/#1-identify-current-database-version","title":"1. Identify Current Database Version","text":"<p>First, check the current schema version of your database. Alembic uses a table (<code>alembic_version</code> by default) to track the current revision of the schema in your database.</p> <p>You can manually check this table:</p> <pre><code>SELECT * FROM alembic_version;\n</code></pre> <p>Or use Alembic's <code>current</code> command:</p> <pre><code>alembic current\n</code></pre> <p>This command displays the current revision that the database is on.</p>"},{"location":"alembic/#2-compare-with-alembic-revision-history","title":"2. Compare with Alembic Revision History","text":"<p>Next, list all the revisions known to Alembic to see where the current database version stands in relation to the migration history.</p> <p>Run the following command to show your migrations history:</p> <pre><code>alembic history\n</code></pre> <p>This command will print a list of revisions. Find where the revision from your database fits within this list. This will inform you whether the database is ahead, behind, or has diverged (if the current revision doesn't exist in your migration chain).</p>"},{"location":"alembic/#3-identify-divergences-or-missing-revisions","title":"3. Identify Divergences or Missing Revisions","text":"<p>If the database's current revision doesn't exist in the migration history from <code>alembic history</code>, it suggests that the database might have applied a revision that has since been deleted or was created from a different branch of your code.</p> <p>In cases where the database is behind, and simply applying newer migrations is required, you can proceed to use <code>alembic upgrade</code> with the target revision you want to apply.</p> <p>However, if the database is ahead or has diverged, you need to assess how to reconcile the differences.</p>"},{"location":"alembic/#4-resolving-divergences","title":"4. Resolving Divergences","text":""},{"location":"alembic/#option-a-manually-adjust-the-databaseschema","title":"Option A: Manually adjust the database/schema","text":"<p>If possible, manually adjust the database schema to match a specific version in your Alembic history, and use <code>alembic stamp</code> to mark it as such, without applying migrations. This can be necessary if the database was manually altered.</p> <pre><code>alembic stamp &lt;revision_id&gt;\n</code></pre>"},{"location":"alembic/#option-b-generate-a-new-migration","title":"Option B: Generate a new migration","text":"<p>You can attempt to autogenerate a new migration that takes into account the current state of the database and the desired state as per your models.</p> <p>Run:</p> <pre><code>alembic revision --autogenerate -m \"Fixing divergence\"\n</code></pre> <p>Review the generated script closely, as it may not be perfect and could require manual adjustments.</p>"},{"location":"alembic/#option-c-revert-to-a-known-good-state","title":"Option C: Revert to a known good state","text":"<p>If all else fails and you have the ability to reset your database to a known good state (usually viable only in development or with available backups), you can revert the database back to a specific revision and reapply migrations from there.</p>"},{"location":"alembic/#5-apply-missing-migrations","title":"5. Apply Missing Migrations","text":"<p>After resolving discrepancies, ensure that your database is at the correct revision and that all necessary migrations are applied.</p> <pre><code>alembic upgrade head\n</code></pre>"},{"location":"alembic/#caution","title":"Caution","text":"<p>Dealing with schema mismatches can lead to data loss or corruption if not handled carefully. Always back up your database before attempting to resolve discrepancies and thoroughly test any changes in a safe environment prior to application in production.</p> <p>Navigating database schema issues can be challenging, but by systematically checking the current state, comparing it with your migration history, and carefully applying corrections, you can resolve discrepancies and get your project back on track.</p>"},{"location":"alembic/#continue","title":"continue","text":""},{"location":"alembic/#6-validate-schema-after-adjustments","title":"6. Validate Schema After Adjustments","text":"<p>After applying the necessary adjustments and migrations, it\u2019s essential to validate that the database schema now matches the expectations of your application and Alembic migrations. Conduct thorough testing to ensure that:</p> <ul> <li>All expected tables, columns, indexes, and other database objects are correctly defined.</li> <li>The application can successfully interact with the database without encountering schema-related errors.</li> <li>Running <code>alembic current</code> reports the correct and expected revision.</li> </ul>"},{"location":"alembic/#7-consider-future-prevention-strategies","title":"7. Consider Future Prevention Strategies","text":"<p>To avoid future discrepancies between your database schema and Alembic revisions, consider implementing strategies that ensure better synchronization and tracking:</p> <ul> <li>Continuous Integration (CI) Checks: Implement CI pipelines that run tests against a database that is always migrated from scratch using current Alembic migrations. This helps catch issues where migrations do not match the application models or are missing.</li> <li>Code Review Processes: Incorporate migration script reviews as part of your development process to catch potential issues early on.</li> <li>Restricted Database Access: Limit who can manually alter the database schema. Ideally, all changes should go through Alembic migrations to ensure that the schema evolution is recorded and versioned.</li> <li>Documenting Manual Changes: In the unavoidable scenario where manual database changes are made, document these changes meticulously. Consider creating corresponding Alembic migrations, even if they are marked as already applied, to ensure the migration history remains an accurate record of the schema's evolution.</li> </ul>"},{"location":"alembic/#8-additional-tools-and-practices","title":"8. Additional Tools and Practices","text":"<ul> <li>Alembic Autogenerate Revisions: While the <code>--autogenerate</code> feature is powerful, it should not be blindly trusted. Always review the generated migration scripts to ensure they accurately represent the desired schema changes and do not inadvertently drop or alter objects.</li> <li>Model Comparison Extensions: For complex projects, consider using or developing tools that help compare the SQLAlchemy models directly against the actual database schema, identifying discrepancies without relying solely on Alembic's version history.</li> <li>Regular Audits: Schedule regular audits of your database schema versus your models and migrations. This proactive approach can help identify issues before they become problematic.</li> <li>Environment Parity: Aim for parity between your development, staging, and production environments in terms of how migrations are applied and managed, reducing the risk of discrepancies arising from differences in how environments are handled.</li> </ul>"},{"location":"alembic/#conclusion","title":"Conclusion","text":"<p>Getting a database schema back in sync with Alembic revisions involves careful diagnosis, proper tool usage, and strategic resolution of discrepancies. It requires a clear understanding of your current schema state, how it deviates from the expected state, and the steps needed to safely reconcile these differences. Implementing preventative strategies and maintaining meticulous records of changes are key to minimizing future synchronization issues, ensuring that your database schema evolution remains manageable, trackable, and aligned with your application's requirements.</p>"},{"location":"alembic/#how-does-alembic-determine-the-order-of-migrations","title":"How Does Alembic Determine the Order of Migrations","text":"<p>Alembic determines the order in which to apply migrations using a couple of key concepts: revision identifiers and down_revision attributes within the migration scripts. These elements create a directed acyclic graph (DAG) of migrations, establishing a clear lineage or path through your migration history. Here's how these components work together to manage migration order:</p>"},{"location":"alembic/#revision-identifiers","title":"Revision Identifiers","text":"<p>Each Alembic migration script is assigned a unique revision identifier (often a hash) when the migration is generated. This identifier uniquely distinguishes each migration in the series of changes made over time.</p>"},{"location":"alembic/#down-revision-attribute","title":"Down Revision Attribute","text":"<p>Within each migration script, there's an attribute named <code>down_revision</code>. This attribute specifies the identifier of the migration that directly precedes the current one in the migration history. The <code>down_revision</code> effectively points back to the migration's parent in the version history tree.</p> <p>For the very first migration in a project, <code>down_revision</code> will be <code>None</code>, indicating that there is no parent migration (i.e., it's the root of the migration tree).</p>"},{"location":"alembic/#upgrade-and-downgrade-sequences","title":"Upgrade and Downgrade Sequences","text":"<p>Given these two components, Alembic constructs a sequence of migrations:</p> <ul> <li> <p>Upgrade: To migrate forward, Alembic starts from the earliest migration whose <code>down_revision</code> is <code>None</code> and follows the chain of <code>revision</code> to <code>down_revision</code> links, applying each migration in turn until it reaches the specified target migration or the latest migration if no target is specified.</p> </li> <li> <p>Downgrade: For migrating backward, Alembic reverses the process, using the current revision as a starting point and following the chain of <code>down_revision</code> values in reverse to apply the <code>downgrade()</code> operations defined in each migration script, until it reaches the specified target revision.</p> </li> </ul>"},{"location":"alembic/#handling-branches","title":"Handling Branches","text":"<p>Alembic also supports branching in migrations. When branches are present, there may be multiple migration scripts with the same <code>down_revision</code>. In this scenario, Alembic uses a \"merge\" migration to bring the divergent branches back into a single linear path. The merge migration specifies multiple <code>down_revision</code> values, identifying each of the branch tips that it reconciles.</p> <p>When applying migrations:</p> <ol> <li> <p>Linear Migrations: In simple, linear migrations, Alembic applies migrations in the straightforward sequence dictated by the single parent-child (<code>down_revision</code> to <code>revision</code>) relationships.</p> </li> <li> <p>Branched Migrations: In branched scenarios, Alembic will apply migrations from each branch as required, until it encounters a merge point. At the merge point, it ensures that all required branches are up to date before applying the merge migration, thus reconciling the branches and continuing forward in a linear fashion from there.</p> </li> </ol>"},{"location":"alembic/#version-table","title":"Version Table","text":"<p>Alembic tracks the current version of the database schema in the <code>alembic_version</code> table, recording which migrations have been applied. This table is crucial for determining the starting point for any migration operation, be it an upgrade or downgrade.</p> <p>In summary, Alembic determines the order of migrations through a combination of unique revision identifiers, parent-child (down_revision) relationships creating a logical sequence, and support for merging branched histories. This structure allows Alembic to manage complex migrations histories with precision and ensure the database schema evolves coherently with the application's requirements.</p>"},{"location":"alembic/#removing-a-revision-that-has-not-been-applied-to-the-database","title":"Removing a Revision That Has Not Been Applied to the Database","text":"<p>If you have an Alembic revision that hasn't been applied to any database yet and you wish to remove it, the process is fairly straightforward since you only need to deal with the revision script(s) in your migrations folder. Here's how you can do it:</p>"},{"location":"alembic/#steps-to-remove-an-unapplied-alembic-revision","title":"Steps to Remove an Unapplied Alembic Revision","text":"<ol> <li> <p>Locate the Revision File: In your project, navigate to the <code>versions</code> directory within your Alembic migrations folder. This folder contains all the revision scripts generated by Alembic.</p> </li> <li> <p>Identify the Revision Script: Each file in the <code>versions</code> directory corresponds to a specific revision. The filename usually starts with the revision ID (a sequence of letters and numbers generated by Alembic) followed by an underscore and a brief description of the migration, e.g., <code>ae1027a6acf_migration_description.py</code>. Identify the script file for the revision you wish to remove. Make sure this is the correct revision by opening the file and verifying its contents, including the revision ID, the <code>down_revision</code>, and the changes it introduces.</p> </li> <li> <p>Delete the Revision File: Simply delete the identified Python script file from the <code>versions</code> directory. This removes the revision from your migrations history, as far as Alembic is concerned.</p> </li> <li> <p>Check if Downstream Revisions Exist: If the revision you're removing has \"child\" revisions (i.e., revisions that list it as their <code>down_revision</code>), you will need to decide how to handle those. You cannot simply delete a revision if later revisions depend on it without risking inconsistencies in your migration path. If such downstream revisions exist, consider the following options:</p> </li> <li>Delete the Downstream Revisions Too: If the downstream revisions also haven't been applied and aren't necessary, you can delete them as well.</li> <li> <p>Rebase the Downstream Revisions: If the downstream revisions need to be kept, you may need to edit their <code>down_revision</code> attributes to reflect the removal of the parent revision. This might involve setting their <code>down_revision</code> to the removed revision's parent or to a new merge revision if the history is more complex.</p> </li> <li> <p>Regenerate Dependency Graph (Optional): If you modified the <code>down_revision</code> of any subsequent migrations, or if you're not sure about the consistency of your migration scripts, you might want to regenerate the Alembic dependency graph. However, this is more about verifying that your revisions are consistent and there are no \u201corphaned\u201d migrations. Alembic doesn't automatically generate a visual graph, but you can check consistency by running <code>alembic history</code> to make sure it outputs a coherent history from your base revision to the head, without any missing links.</p> </li> <li> <p>Update Database Schema Manually if Necessary: If the deleted migration or any of its downstream migrations had been applied to any other environment's database (development, staging, etc.), you'll need to manually adjust those database schemas and possibly the <code>alembic_version</code> table to ensure consistency. This step applies only if the migration was mistakenly said to be unapplied when, in fact, it had been applied somewhere.</p> </li> </ol>"},{"location":"alembic/#delete-caution","title":"Delete Caution","text":"<ul> <li>Be extra careful to ensure that the migration has indeed not been applied to any environment. Removing applied migrations can lead to inconsistencies and errors.</li> <li>Always have a backup of your database and current migration scripts before deleting or modifying them.</li> <li>Remember to communicate with your team about any changes to the migration scripts, especially if other developers might have applied the deleted migration in their local environment.</li> </ul> <p>In summary, removing an unapplied Alembic revision is as simple as deleting its script file from the <code>versions</code> directory, but care should be taken to handle dependency and consistency issues that might arise from doing so.</p>"},{"location":"authentication-architecture/","title":"Authentication Architecture Migration - Complete","text":""},{"location":"authentication-architecture/#overview","title":"Overview","text":"<p>The iChrisBirch project has successfully migrated from database-based service accounts to modern API key authentication following industry best practices.</p>"},{"location":"authentication-architecture/#migration-summary","title":"Migration Summary","text":""},{"location":"authentication-architecture/#previous-architecture-deprecated","title":"Previous Architecture (Deprecated)","text":"<ul> <li>Database service accounts: <code>APIServiceAccount</code> class managing database users</li> <li>Mixed authentication patterns: Inconsistent auth across different modules  </li> <li>Test infrastructure issues: Broken imports and dependencies</li> <li>Circular dependencies: Service account creation caused recursive loops</li> </ul>"},{"location":"authentication-architecture/#current-architecture","title":"Current Architecture \u2705","text":"<p>Modern API Key Authentication:</p> <ul> <li>Internal Service Auth: <code>X-Internal-Service</code> + <code>X-Service-Key</code> headers</li> <li>User Authentication: JWT tokens, OAuth2, and application headers</li> <li>Consistent client usage: <code>LoggingAPIClient</code> throughout codebase</li> <li>Professional logging: Extensive debugging capabilities preserved</li> </ul>"},{"location":"authentication-architecture/#authentication-methods","title":"Authentication Methods","text":""},{"location":"authentication-architecture/#internal-service-authentication","title":"Internal Service Authentication","text":"<p>Used for service-to-service communication:</p> <pre><code># Headers-based authentication\nheaders = {\n    'X-Internal-Service': 'scheduler',\n    'X-Service-Key': settings.auth.internal_service_key\n}\n\n# Modern client usage\nwith logging_internal_service_client() as client:\n    users = client.resource('users', schemas.User)\n    user = users.get_by_email(email)\n</code></pre>"},{"location":"authentication-architecture/#user-authentication","title":"User Authentication","text":"<p>Multiple methods supported:</p> <ul> <li>JWT Tokens: Standard Bearer token authentication</li> <li>Application Headers: <code>X-User-ID</code> + <code>X-Application-ID</code></li> <li>OAuth2: External authentication provider support</li> </ul>"},{"location":"authentication-architecture/#factory-functions","title":"Factory Functions","text":"<p>Standardized client creation:</p> <pre><code>from ichrisbirch.api.client.logging_client import (\n    logging_internal_service_client,\n    logging_user_client,\n    logging_flask_session_client\n)\n\n# Internal service operations\nwith logging_internal_service_client() as client:\n    # Service-to-service calls\n\n# User-specific operations  \nwith logging_user_client(user_token) as client:\n    # User-specific API calls\n\n# Flask session operations\nwith logging_flask_session_client() as client:\n    # Flask login integration\n</code></pre>"},{"location":"authentication-architecture/#key-components","title":"Key Components","text":""},{"location":"authentication-architecture/#loggingapiclient","title":"LoggingAPIClient","text":"<p>Modern replacement for QueryAPI with identical interface:</p> <ul> <li>Extensive logging: Environment info, auth details, request/response logging</li> <li>Custom methods preserved: <code>get_generic()</code>, <code>post_action()</code> functionality</li> <li>Context management: Proper session lifecycle management</li> <li>Error handling: Comprehensive exception handling and logging</li> </ul>"},{"location":"authentication-architecture/#fastapi-endpoints","title":"FastAPI Endpoints","text":"<p>Authentication dependencies:</p> <ul> <li><code>CurrentUser</code>: Requires valid user authentication</li> <li><code>AdminUser</code>: Requires admin user authentication  </li> <li><code>AdminOrInternalServiceAccess</code>: Allows admin users OR internal service auth</li> <li>Internal service verification: <code>verify_internal_service()</code> dependency</li> </ul>"},{"location":"authentication-architecture/#flask-integration","title":"Flask Integration","text":"<p>Seamless integration with Flask login system:</p> <ul> <li>User loading: Internal service client for user lookups</li> <li>Session management: Proper Flask session integration</li> <li>Login/logout: Standard Flask-Login patterns preserved</li> </ul>"},{"location":"authentication-architecture/#configuration","title":"Configuration","text":""},{"location":"authentication-architecture/#environment-variables","title":"Environment Variables","text":"<p>Required settings in all environments:</p> <pre><code># Internal service authentication\nAUTH_INTERNAL_SERVICE_KEY=&lt;secure-api-key&gt;\n\n# API endpoints  \nAPI_URL=http://localhost:8000  # or appropriate URL for environment\n</code></pre>"},{"location":"authentication-architecture/#settings-structure","title":"Settings Structure","text":"<pre><code># ichrisbirch/config.py\nclass AuthSettings:\n    def __init__(self):\n        self.internal_service_key = os.environ['AUTH_INTERNAL_SERVICE_KEY']\n        # Other auth settings...\n\nclass Settings:\n    def __init__(self):\n        self.auth = AuthSettings()\n        self.api_url = os.environ['API_URL']\n</code></pre>"},{"location":"authentication-architecture/#usage-patterns","title":"Usage Patterns","text":""},{"location":"authentication-architecture/#chat-authentication","title":"Chat Authentication","text":"<pre><code># ichrisbirch/chat/auth.py\nwith logging_internal_service_client() as client:\n    users = client.resource('users', schemas.User)\n    if user_data := users.get_generic(['email', username]):\n        user = models.User(**user_data)\n        return user\n</code></pre>"},{"location":"authentication-architecture/#flask-login-integration","title":"Flask Login Integration","text":"<pre><code># ichrisbirch/app/login.py\ndef get_users_api():\n    return logging_internal_service_client(base_url=settings.api_url)\n\n@login_manager.user_loader  \ndef load_user(alternative_id):\n    with get_users_api() as client:\n        users = client.resource('users', schemas.User)\n        return users.get_generic(['alt', alternative_id])\n</code></pre>"},{"location":"authentication-architecture/#scheduler-jobs","title":"Scheduler Jobs","text":"<pre><code># Service jobs use internal authentication\nwith logging_internal_service_client() as client:\n    habits = client.resource('habits', schemas.Habit)\n    active_habits = habits.list(params={'current': True})\n</code></pre>"},{"location":"authentication-architecture/#security-features","title":"Security Features","text":""},{"location":"authentication-architecture/#internal-service-protection","title":"Internal Service Protection","text":"<ul> <li>API key validation: Cryptographically secure key verification</li> <li>Service identification: Named service identification in headers</li> <li>Request logging: Comprehensive audit trail for service calls</li> <li>Rate limiting ready: Infrastructure prepared for rate limiting</li> </ul>"},{"location":"authentication-architecture/#user-access-controls","title":"User Access Controls","text":"<ul> <li>Role-based access: Admin vs regular user permissions</li> <li>Own-data restrictions: Users can only access their own data</li> <li>Cross-user prevention: Prevents unauthorized access to other users' data</li> <li>Admin overrides: Admin users can access all data when appropriate</li> </ul>"},{"location":"authentication-architecture/#benefits-achieved","title":"Benefits Achieved","text":""},{"location":"authentication-architecture/#technical-improvements","title":"Technical Improvements","text":"<ul> <li>\u2705 Industry standard authentication: API keys instead of database users</li> <li>\u2705 Consistent patterns: Same authentication method across all modules</li> <li>\u2705 Clean architecture: No circular dependencies or global state issues</li> <li>\u2705 Professional logging: Enhanced debugging and monitoring capabilities</li> </ul>"},{"location":"authentication-architecture/#operational-benefits","title":"Operational Benefits","text":"<ul> <li>\u2705 Simplified deployment: No database user creation required</li> <li>\u2705 Better security: API keys are more secure than database accounts</li> <li>\u2705 Easier maintenance: Single authentication configuration point</li> <li>\u2705 Scalability ready: Authentication works across distributed systems</li> </ul>"},{"location":"authentication-architecture/#developer-experience","title":"Developer Experience","text":"<ul> <li>\u2705 Clear patterns: Consistent client usage across all code</li> <li>\u2705 Better testing: Clean test patterns without database dependencies</li> <li>\u2705 Enhanced debugging: Extensive logging preserved and improved</li> <li>\u2705 Modern tooling: Following current industry best practices</li> </ul>"},{"location":"authentication-architecture/#migration-complete","title":"Migration Complete","text":"<p>All components successfully migrated:</p> <ul> <li>Chat authentication: \u2705 Using modern internal service client</li> <li>Flask login system: \u2705 Integrated with LoggingAPIClient  </li> <li>API endpoints: \u2705 Support internal service and user authentication</li> <li>Test infrastructure: \u2705 Clean test patterns without service account dependencies</li> <li>Configuration: \u2705 Simplified environment variable configuration</li> </ul> <p>The authentication architecture is now modern, secure, and follows industry best practices while maintaining all existing functionality and debugging capabilities.</p>"},{"location":"backups/","title":"Backups","text":"<ol> <li>Install AWS-CLI</li> <li><code>aws configure</code> - Use credentials for running the script</li> </ol> <p>Note</p> <p>There are more thorough instructions in the script: <code>/scripts/postgres-snapshot-to-s3.sh --help</code></p> <p>Postgres Backup: Location: <code>/scripts/postgres-snapshot-to-s3.sh</code></p> <p>MongDB Backup: https://www.cloudsavvyit.com/6059/how-to-set-up-automated-mongodb-backups-to-s3/</p>"},{"location":"cicd/","title":"CICD","text":""},{"location":"cicd/#deploy-docsyml","title":"<code>deploy-docs.yml</code>","text":"<p>Description:</p> <p>Build the docs with mkdocs and run gh-deploy to publish them to github pages with the <code>pages-build-deployment</code> workflow.</p>"},{"location":"cicd/#python-cicdyml","title":"<code>python-cicd.yml</code>","text":"<p>Description:</p>"},{"location":"cicd/#key-for-github-actions-to-access-ec2-instance","title":"Key for Github Actions to Access EC2 Instance","text":"<p><code>ICHRISBIRCH_KEY</code></p> <p>To generate this value locally: <code>cat ichrisbirch-webserver.pem | pbcopy</code> Simply paste into the secret on Github</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#ichrisbirchichrisbirchconfigpy","title":"<code>ichrisbirch/ichrisbirch/config.py</code>","text":"<p>In the Config class, we're setting the env_file based on the ENVIRONMENT variable. If ENVIRONMENT is not recognized, env_file will be None. When env_file is set, pydantic will automatically try to load the variables from the specified file.</p> <p>Also note that since pydantic automatically converts environment variables to their corresponding data types, we don't need to use Optional or Union in our field definitions anymore.</p>"},{"location":"configuration/#flake-8","title":"Flake 8","text":"<p><code>.flake8</code> cannot be loaded from <code>pyproject.toml</code></p>"},{"location":"css/","title":"CSS Notes","text":""},{"location":"css/#flex","title":"Flex","text":"<p>The display: flex and display: inline-flex properties in CSS are used to create a flex container and make its children flex items. The difference between them lies in how the flex container behaves in relation to other elements.</p> <p>display: flex: This makes the container a block-level flex container. A block-level element takes up the full width of its parent element, and it starts and ends with a new line. So, a flex container with display: flex will take up the full width of its parent and will not allow other elements to sit next to it on the same line.</p> <p>display: inline-flex: This makes the container an inline-level flex container. An inline-level element only takes up as much width as it needs, and it does not start or end with a new line. So, a flex container with display: inline-flex will only be as wide as necessary to contain its items, and it will allow other elements to sit next to it on the same line.</p>"},{"location":"css_bem/","title":"CSS BEM","text":"<p>HTML5 semantic elements help structure the content of web pages in a way that is meaningful for both browsers and developers. BEM (Block, Element, Modifier) is a methodology that aims to create reusable components and code sharing in front-end development. It stands for Block, Element, Modifier and provides a way for developers to name their CSS classes in a strict, understandable, and informative way, significantly improving code maintainability and readability.</p> <p>Here\u2019s an example of a medium complexity page using HTML5 semantic tags combined with BEM CSS naming conventions. This example includes a basic layout with a header, navigation, a main content area with an article and sidebar, and a footer.</p>"},{"location":"css_bem/#example-html-structure","title":"Example HTML Structure","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Example Page with HTML5 and BEM&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;header class=\"header\"&gt;\n    &lt;h1 class=\"header__title\"&gt;My Website&lt;/h1&gt;\n    &lt;nav class=\"nav\"&gt;\n        &lt;ul class=\"nav__list\"&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#home\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n            &lt;li class=\"nav__item\"&gt;&lt;a class=\"nav__link\" href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/nav&gt;\n&lt;/header&gt;\n\n&lt;main class=\"main\"&gt;\n    &lt;article class=\"article\"&gt;\n        &lt;h2 class=\"article__title\"&gt;Blog Post Title&lt;/h2&gt;\n        &lt;p class=\"article__meta\"&gt;Posted on &lt;time datetime=\"2023-04-01\"&gt;April 1, 2023&lt;/time&gt;&lt;/p&gt;\n        &lt;div class=\"article__content\"&gt;\n            &lt;p&gt;This is a blog post. It describes something interesting.&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/article&gt;\n\n    &lt;aside class=\"sidebar\"&gt;\n        &lt;div class=\"sidebar__section\"&gt;\n            &lt;h2 class=\"sidebar__title\"&gt;About Me&lt;/h2&gt;\n            &lt;p&gt;I am a web developer...&lt;/p&gt;\n        &lt;/div&gt;\n        &lt;div class=\"sidebar__section\"&gt;\n            &lt;h2 class=\"sidebar__title\"&gt;Archives&lt;/h2&gt;\n            &lt;ul class=\"sidebar__list\"&gt;\n                &lt;li class=\"sidebar__item\"&gt;March 2023&lt;/li&gt;\n                &lt;li class=\"sidebar__item\"&gt;February 2023&lt;/li&gt;\n                &lt;li class=\"sidebar__item\"&gt;January 2023&lt;/li&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n    &lt;/aside&gt;\n&lt;/main&gt;\n\n&lt;footer class=\"footer\"&gt;\n    &lt;p class=\"footer__text\"&gt;\u00a9 2023 My Website&lt;/p&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"css_bem/#example-css-using-bem","title":"Example CSS Using BEM","text":"<pre><code>.header {\n    background-color: #f0f0f0;\n    padding: 20px 0;\n}\n\n.header__title {\n    margin: 0;\n    padding: 0 20px;\n}\n\n.nav {\n    background-color: #333;\n}\n\n.nav__list {\n    list-style: none;\n    display: flex;\n    justify-content: center;\n    padding: 0;\n}\n\n.nav__item {\n    margin: 0 10px;\n}\n\n.nav__link {\n    color: white;\n    text-decoration: none;\n}\n\n.main {\n    display: flex;\n    margin: 20px;\n}\n\n.article {\n    flex: 3;\n}\n\n.article__title {\n    color: #333;\n}\n\n.article__meta {\n    font-style: italic;\n}\n\n.sidebar {\n    flex: 1;\n    padding-left: 20px;\n}\n\n.sidebar__title {\n    font-size: 20px;\n}\n\n.footer {\n    background-color: #333;\n    color: white;\n    text-align: center;\n    padding: 10px 0;\n}\n</code></pre>"},{"location":"css_bem/#bem-explanation","title":"BEM Explanation","text":"<ul> <li> <p>Block: Standalone entity that is meaningful on its own (e.g., <code>header</code>, <code>nav</code>, <code>article</code>, <code>sidebar</code>, <code>footer</code>). Blocks can be nested inside each other but should remain independent.</p> </li> <li> <p>Element: A part of a block that has no standalone meaning and is semantically tied to its block (e.g., <code>header__title</code>, <code>nav__link</code>, <code>article__title</code>). Elements are always part of a block, not another element.</p> </li> <li> <p>Modifier: Flags on blocks or elements used to change appearance, behavior, or state (e.g., <code>nav__link--active</code>, although not shown in the example above, would represent an active state of the navigation link).</p> </li> </ul> <p>BEM's naming convention makes the structure of HTML/CSS clear and understandable at a glance, provides a strong contract for developers on a project, and helps avoid CSS naming conflicts by using unique names based on the block-element hierarchy.</p>"},{"location":"developer_setup/","title":"Developer Setup","text":"<ul> <li>1. Programs to install</li> <li>2. Set up git-secret</li> <li>3. Setup the project</li> <li>4. Run the project</li> <li>5. Connecting to the Running Project</li> <li>5.1. App</li> <li>5.2. API</li> <li>6. Links and Notes</li> </ul>"},{"location":"developer_setup/#1-programs-to-install","title":"1. Programs to install","text":"<p>poetry git-secret Docker Desktop</p>"},{"location":"developer_setup/#2-set-up-git-secret","title":"2. Set up git-secret","text":""},{"location":"developer_setup/#3-setup-the-project","title":"3. Setup the project","text":"<pre><code>git clone https://github.com/datapointchris/ichrisbirch.git\n\ncd ichrisbirch/\n\ngit secret reveal\n\npoetry install\n\nsource .venv/bin/activate\n\nexport ENVIRONMENT=development\n\npre-commit install\n\n# Make sure Docker is running\n\npytest\n</code></pre>"},{"location":"developer_setup/#4-run-the-project","title":"4. Run the project","text":"<p>TODO</p> <p>This doesn't work!  I need to figure out another way to run it locally.   Right now it is relying on using local NGINX and Supervisor.</p> <pre><code># App and API are separate applications.\n# App is a flask app that runs the frontend\n# API is FastAPI running the API backend that the frontend connects to\n\n# Run these in separate shells for log separation\n# poetry run python ichrisbirch/runapidev.py\n# poetry run python ichrisbirch/runappdev.py\n</code></pre>"},{"location":"developer_setup/#5-connecting-to-the-running-project","title":"5. Connecting to the Running Project","text":""},{"location":"developer_setup/#51-app","title":"5.1. App","text":"<p>http://127.0.0.1:6000</p>"},{"location":"developer_setup/#52-api","title":"5.2. API","text":"<p>http://127.0.0.1:6200</p>"},{"location":"developer_setup/#6-links-and-notes","title":"6. Links and Notes","text":"<p>GitHub - github/scripts-to-rule-them-all: Set of boilerplate scripts describing the normalized script pattern that GitHub uses in its projects.</p>"},{"location":"docker-development/","title":"Docker Development Environment","text":""},{"location":"docker-development/#overview","title":"Overview","text":"<p>The iChrisBirch project uses Docker Compose for containerized development with separate configurations for development, testing, and production environments.</p>"},{"location":"docker-development/#architecture","title":"Architecture","text":""},{"location":"docker-development/#service-configuration","title":"Service Configuration","text":"<p>Development Environment (<code>docker-compose.dev.yml</code>):</p> <ul> <li>Nginx: Port 80/443 (reverse proxy)</li> <li>Flask App: Port 8000 (internal), auto-reload enabled</li> <li>FastAPI Backend: Port 8000 (internal), auto-reload enabled  </li> <li>PostgreSQL: Port 5432 (internal/external), persistent volumes</li> <li>Redis: Port 6379 (internal/external), persistent volumes</li> <li>Chat Service: Port 8505 (internal), auto-reload enabled</li> </ul> <p>Test Environment (<code>docker-compose.test.yml</code>):</p> <ul> <li>PostgreSQL: Port 5434 (external) \u2192 5432 (internal), tmpfs for speed</li> <li>Redis: Port 6380 (external) \u2192 6379 (internal), tmpfs for speed  </li> <li>FastAPI Backend: Port 8001 (external) \u2192 8000 (internal), isolated test database</li> <li>Nginx: Disabled for testing</li> </ul> <p>Production Environment (<code>docker-compose.yml</code>):</p> <ul> <li>All services: Standard internal ports with production optimizations</li> <li>SSL/TLS: Ready for certificate mounting</li> <li>Health checks: Comprehensive monitoring</li> <li>Resource limits: Production-ready constraints</li> </ul>"},{"location":"docker-development/#cli-command-reference","title":"CLI Command Reference","text":""},{"location":"docker-development/#development-commands","title":"Development Commands","text":"<pre><code>ichrisbirch dev start     # Start all development services\nichrisbirch dev stop      # Stop and remove containers  \nichrisbirch dev restart   # Restart existing containers\nichrisbirch dev rebuild   # Rebuild images and restart\nichrisbirch dev logs      # View live container logs\nichrisbirch dev status    # Show container status\n</code></pre> <p>Technical Details:</p> <ul> <li><code>restart</code>: Fast recovery from service crashes (no image rebuild)</li> <li><code>rebuild</code>: Full image rebuild after code/dependency changes</li> <li><code>logs</code>: Shows Docker infrastructure logs, not application logs</li> </ul>"},{"location":"docker-development/#test-commands","title":"Test Commands","text":"<pre><code>ichrisbirch test          # Run pytest in containerized environment\nichrisbirch test logs     # Run tests with timestamped log output\n</code></pre> <p>Test Infrastructure:</p> <ul> <li>Isolated database on port 5434 with tmpfs for performance</li> <li>Redis on port 6380 with tmpfs storage</li> <li>Services terminate automatically when tests complete</li> <li>Cleanup via <code>docker-compose down -v</code> removes test containers/volumes</li> </ul>"},{"location":"docker-development/#production-commands","title":"Production Commands","text":"<pre><code>ichrisbirch prod status    # Check production service status\nichrisbirch prod apihealth # HTTP health check for API service\nichrisbirch prod logs      # View production application logs\n</code></pre>"},{"location":"docker-development/#docker-compose-configuration","title":"Docker Compose Configuration","text":""},{"location":"docker-development/#file-structure","title":"File Structure","text":"<ul> <li><code>docker-compose.yml</code>: Base production configuration</li> <li><code>docker-compose.dev.yml</code>: Development overrides with debugging</li> <li><code>docker-compose.test.yml</code>: Test environment with performance optimizations</li> </ul>"},{"location":"docker-development/#environment-variables","title":"Environment Variables","text":"<p>Each environment uses corresponding environment files:</p> <ul> <li>Development: <code>.dev.env.secret</code> (Git Secret encrypted)</li> <li>Testing: <code>.test.env.secret</code> (Git Secret encrypted)  </li> <li>Production: <code>.prod.env.secret</code> (Git Secret encrypted)</li> </ul>"},{"location":"docker-development/#volume-mounts","title":"Volume Mounts","text":"<p>Development:</p> <ul> <li>Source code mounted for live editing</li> <li>Persistent database and Redis volumes</li> <li>Nginx configuration from <code>deploy/dev/nginx/</code></li> </ul> <p>Testing:</p> <ul> <li>tmpfs mounts for database and Redis (maximum speed)</li> <li>Isolated test data, discarded after tests</li> <li>No source code mounts (clean container environment)</li> </ul> <p>Production:</p> <ul> <li>Named volumes for data persistence</li> <li>SSL certificate mounting ready</li> <li>Optimized configurations from <code>deploy/prod/</code></li> </ul>"},{"location":"docker-development/#logging-architecture","title":"Logging Architecture","text":""},{"location":"docker-development/#application-logging","title":"Application Logging","text":"<ul> <li>Python loggers: Write directly to application log files</li> <li>Log locations: <code>$LOG_DIR</code> environment variable (defaults to <code>./logs</code>)</li> <li>Log files: <code>ichrisbirch.log</code>, <code>app.log</code>, <code>api.log</code>, <code>scheduler.log</code></li> <li>Colored output: CLI provides colored log viewing with <code>ichrisbirch logs</code></li> </ul>"},{"location":"docker-development/#container-logging","title":"Container Logging","text":"<ul> <li>Docker logs: Infrastructure and startup information only</li> <li>JSON file driver: Rotation and size limits configured</li> <li>Service tags: Each service tagged for log identification</li> <li>Access via: <code>docker-compose logs</code> for container-level debugging</li> </ul>"},{"location":"docker-development/#port-configuration","title":"Port Configuration","text":""},{"location":"docker-development/#external-port-mapping","title":"External Port Mapping","text":"<p>Development (accessible from host):</p> <ul> <li>Nginx: localhost:80, localhost:443</li> <li>PostgreSQL: localhost:5432</li> <li>Redis: localhost:6379</li> <li>API/App: Through Nginx reverse proxy</li> </ul> <p>Testing (isolated ports):</p> <ul> <li>PostgreSQL: localhost:5434</li> <li>Redis: localhost:6380</li> <li>FastAPI: localhost:8001</li> </ul> <p>Production:</p> <ul> <li>Nginx: Port 80/443 only (reverse proxy handles internal routing)</li> <li>All other services: Internal container network only</li> </ul>"},{"location":"docker-development/#service-communication","title":"Service Communication","text":"<ul> <li>Internal network: All services communicate via container names</li> <li>Health checks: Services wait for dependencies before starting</li> <li>Service discovery: Automatic via Docker Compose networking</li> </ul>"},{"location":"docker-development/#development-workflow","title":"Development Workflow","text":""},{"location":"docker-development/#starting-development","title":"Starting Development","text":"<pre><code># Clone and setup\ngit clone &lt;repository&gt;\ncd ichrisbirch\ngit secret reveal  # Decrypt environment files\n\n# Start development environment  \nichrisbirch dev start\n\n# View logs\nichrisbirch dev logs\n</code></pre>"},{"location":"docker-development/#running-tests","title":"Running Tests","text":"<pre><code># Run full test suite\nichrisbirch test\n\n# Run with log output\nichrisbirch test logs\n\n# Check test infrastructure\nichrisbirch dev status\n</code></pre>"},{"location":"docker-development/#debugging-services","title":"Debugging Services","text":"<pre><code># Check service status\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml ps\n\n# View specific service logs  \ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml logs api\n\n# Execute commands in running containers\ndocker exec -it ichrisbirch-api-dev /bin/bash\n</code></pre>"},{"location":"docker-development/#production-deployment","title":"Production Deployment","text":""},{"location":"docker-development/#container-optimization","title":"Container Optimization","text":"<ul> <li>Multi-stage builds: Development vs production images</li> <li>Security: Non-root user (<code>appuser</code>) for all services</li> <li>Resource limits: Memory and CPU constraints configured</li> <li>Health monitoring: Comprehensive health check endpoints</li> </ul>"},{"location":"docker-development/#ssltls-configuration","title":"SSL/TLS Configuration","text":"<p>Ready for certificate mounting:</p> <pre><code>volumes:\n  - ./nginx/ssl:/etc/nginx/ssl:ro  # Uncomment for SSL certificates\n</code></pre>"},{"location":"docker-development/#scaling-considerations","title":"Scaling Considerations","text":"<ul> <li>Database: PostgreSQL optimized for production workloads</li> <li>Redis: Configured with appropriate memory limits and eviction policies</li> <li>Application services: Ready for horizontal scaling with load balancer</li> <li>Static files: Nginx optimized for efficient static file serving</li> </ul>"},{"location":"docker-development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker-development/#common-issues","title":"Common Issues","text":"<p>Port conflicts: Development uses standard ports; test uses offset ports to avoid conflicts</p> <p>Container permissions: Services run as <code>appuser</code> (UID 1000) for security</p> <p>Database connections: Services include health checks and retry logic for reliable startup</p> <p>Volume permissions: Ensure <code>$LOG_DIR</code> is writable by user running Docker Compose</p>"},{"location":"docker-development/#debug-commands","title":"Debug Commands","text":"<pre><code># Check all container status\nichrisbirch dev status\n\n# View recent infrastructure logs\nichrisbirch dev logs\n\n# Clean up unused containers/images  \ndocker system prune -f\n\n# Reset development environment\nichrisbirch dev stop &amp;&amp; ichrisbirch dev rebuild\n</code></pre>"},{"location":"docker-development/#performance-optimization","title":"Performance Optimization","text":"<p>Development: Persistent volumes with live code reloading</p> <p>Testing: tmpfs mounts eliminate I/O bottlenecks for test database/Redis</p> <p>Production: Optimized PostgreSQL and Redis configurations for production workloads</p>"},{"location":"docker-development/#migration-from-legacy-setup","title":"Migration from Legacy Setup","text":""},{"location":"docker-development/#key-changes","title":"Key Changes","text":"<ul> <li>Containerized services: All services now run in Docker containers</li> <li>Environment isolation: Separate configurations for dev/test/prod</li> <li>Modern authentication: API key-based internal service authentication</li> <li>Health monitoring: Comprehensive health checks and monitoring</li> <li>Simplified deployment: Single command deployment with Docker Compose</li> </ul>"},{"location":"docker-development/#compatibility","title":"Compatibility","text":"<ul> <li>Existing workflows: CLI commands maintain same interface</li> <li>Log viewing: Same colored log output with enhanced container support</li> <li>Development experience: Hot reloading and debugging capabilities preserved</li> <li>Production deployment: Enhanced reliability and monitoring capabilities</li> </ul>"},{"location":"documentation/","title":"Documentation","text":"<p>Built with Material for MkDocs</p> <p>Config file: <code>mkdocs.yml</code> Directory: <code>docs/</code> Docs build pipeline: <code>.github/workflows/deploy-docs.yml</code></p> <p>Docs are built using mkdocs automatically on push with the above pipeline, which triggers the <code>pages-build-deployment</code> Github workflow that publishes them to a <code>gh-pages</code> branch and publishes them to <code>datapointchris.github.io/ichrisbirch</code> from that branch.</p> <p>Refer to CICD for a description of the pipeline.</p>"},{"location":"documentation/#github-settings","title":"Github Settings","text":""},{"location":"documentation/#build-and-deployment","title":"Build and deployment","text":"<p>Source: Deploy from a branch</p> <p>Branch: gh-pages / (root)</p> <p>Note</p> <p>Even though this project is using Github Actions to publish the branch, the action is actually using <code>gh-deploy</code> so the source is NOT Github Actions, but rather \"Deploy from a branch\" that <code>gh-deploy</code> sets up.</p> <p>You might have to push the build once the first time to get the <code>gh-pages</code> branch to show up. This is the branch to use, not master, since part of the deploy script used <code>gh-deploy</code> which builds the <code>gh-pages</code> branch.</p> <p>In this branch, the root of the folder is the built docs, NOT /docs, because we are not building from the master branch where the docs live in /docs.</p>"},{"location":"documentation/#custom-domain","title":"Custom Domain","text":"<p>Custom domain: docs.ichrisbirch.com</p> <p>Refer to Domain Names for setting up the subdomain.</p> <p>Note</p> <p>CNAME record needs to be set up before adding the custom domain, or the lookup will fail. CNAME record goes in the <code>/docs</code> folder because that folder is built as the root on the <code>gh-pages</code> branch that is set up with that file when hosting.</p>"},{"location":"documentation/#diagrams","title":"Diagrams","text":"<p>Online FlowChart &amp; Diagrams Editor - Mermaid Live Editor</p> <p>GitHub - mingrammer/diagrams: Diagram as Code for prototyping cloud system architectures</p>"},{"location":"documentation/#todo-read-these-things","title":"TODO: Read these things","text":"<p>Vale.sh - A linter for prose</p> <p>Python's doctest: Document and Test Your Code at Once \u2013 Real Python</p> <p>Awesome documentation example for small project: Documentation \u2014 pypdf 3.5.1 documentation</p> <p>A Guide to Writing Your First Software Documentation \u2014 SitePointSitePoint</p> <p>How to Write Documentation For Your Next Software Development Project</p> <p>Software Documentation Best Practices [With Examples] helpjuice-logo-0307896d1acd18c6a7f52c4256467fb6ca1007315c373af21357496e9ceb49e2</p> <p>Software Documentation Types and Best Practices | by AltexSoft Inc | Prototypr</p> <p>Prepare the documentation for successful software project development</p> <p>How to Write Technical Documentation With Empathy | by Edward Huang | Jan, 2023 | Better Programming</p>"},{"location":"documentation_tools/","title":"Documentation Tools","text":"<p>This directory contains tools and utilities for generating and maintaining the project's documentation.</p>"},{"location":"documentation_tools/#diagram-generator","title":"Diagram Generator","text":"<p>The diagram generator tools located in the <code>tools/docs/diagram_generator/</code> directory help create and maintain visual representations of the project architecture.</p>"},{"location":"documentation_tools/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>tools/docs/diagram_generator/</code>: Tools for generating diagrams used in documentation</li> <li><code>analyzers/</code>: Code that analyzes the project structure to generate diagram data</li> <li><code>renderers/</code>: Code that renders diagrams from analyzed data</li> <li><code>generate_diagrams.py</code>: Main script to generate all diagrams</li> <li><code>mkdocs_diagrams_plugin.py</code>: MkDocs plugin to generate diagrams during site build</li> <li><code>mkdocs_code_sync_plugin.py</code>: MkDocs plugin to sync code examples with actual code</li> </ul>"},{"location":"documentation_tools/#usage","title":"Usage","text":""},{"location":"documentation_tools/#generate-diagrams","title":"Generate Diagrams","text":"<p>To generate all diagrams for the documentation, run:</p> <pre><code>python -m tools.docs.diagram_generator.generate_diagrams\n</code></pre> <p>This will:</p> <ol> <li>Analyze the project structure</li> <li>Generate SVG diagrams for:</li> <li>Test fixtures</li> <li>AWS infrastructure</li> <li>Testing architecture</li> </ol> <p>All generated diagrams are saved in the <code>docs/images/generated/</code> directory.</p>"},{"location":"documentation_tools/#mkdocs-integration","title":"MkDocs Integration","text":"<p>The diagram generator is integrated with MkDocs via plugins defined in <code>mkdocs.yml</code>. When building the documentation site, these plugins:</p> <ol> <li>Automatically regenerate diagrams before building the site</li> <li>Synchronize code examples with actual project code</li> </ol>"},{"location":"documentation_tools/#development","title":"Development","text":"<p>When adding new types of diagrams:</p> <ol> <li>Create a new renderer in the <code>tools/docs/diagram_generator/renderers/</code> directory</li> <li>Add the renderer to <code>generate_diagrams.py</code></li> <li>Update the documentation to reference the new diagrams</li> </ol>"},{"location":"domain_names/","title":"Domain Names","text":"<p>Hosted in <code>AWS Route 53</code> There are 3 hosted zones, one for the top level domain and one for each subdomain.</p>"},{"location":"domain_names/#ichrisbirchcom-hosted-zone","title":"ichrisbirch.com Hosted Zone","text":"<p>This is referred to as the <code>Apex</code> domain, or top level domain.</p> <p>There are 5 records in this hosted zone:</p> Record Name Type Description Value ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Apex Nameservers ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS api.ichrisbirch.com <code>NS</code> Nameservers from hosted zone <code>NS</code> record AWS API Nameservers docs.ichrisbirch.com <code>CNAME</code> Re-direct from Github Pages to docs subdomain datapointchris.github.io \\www.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver EC2 IP (Elastic IP)"},{"location":"domain_names/#apiichrisbirchcom-hosted-zone","title":"api.ichrisbirch.com Hosted Zone","text":"<p>There are 3 records in this hosted zone:</p> Record Name Type Description Value api.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver EC2 IP (Elastic IP) api.ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Api Nameservers api.ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS"},{"location":"domain_names/#docsichrisbirchcom-hosted-zone","title":"docs.ichrisbirch.com Hosted Zone","text":"<p>There are 3 records in this hosted zone:</p> Record Name Type Description Value docs.ichrisbirch.com <code>A</code> Points to the EC2 IP of the webserver Github Servers docs.ichrisbirch.com <code>NS</code> Created automatically with the hosted zone AWS Docs Nameservers docs.ichrisbirch.com <code>SOA</code> Created automtaically with the hosted zone AWS DNS <p>Refer to the Documentation and CICD pages for setting up Github Pages with this subdomain.</p> <p>Use <code>dig {address}</code> to see if the domain looks set up correctly.</p> <p>Danger</p> <p>If the DNS seems to not be updating and the records are not working, be wary of the <code>Browser Cache</code> and history!! <code>Safari</code> kept the old <code>IP</code> in the cache until restart, the only way to see the update was to use a <code>Private Window</code> Check the cache before troubleshooting.</p>"},{"location":"domain_names/#apex-ichrisbirchcom-wwwichrisbirchcom","title":"Apex - <code>ichrisbirch.com ( www.ichrisbirch.com )</code>","text":"<p>There should be a <code>nameserver</code> record created with the <code>hosted zone</code>, and the <code>soa</code> is created automatically. The <code>A</code> record should point to the elastic IP if assigned, or public ip of the instance or load balancer and the name should have subdomain <code>www</code>.  I believe because the domain <code>ichrisbirch.com</code> is the apex, it doesn't need an <code>A</code> record.</p>"},{"location":"domain_names/#apiichrisbirchcom","title":"<code>api.ichrisbirch.com</code>","text":"<p>There should be a <code>nameserver</code> record created with the <code>hosted zone</code> and <code>soa</code>, same as the Apex. The <code>A</code> record should point to the elastic IP.</p> <p>There should be another <code>NS</code> record with the <code>API Nameservers</code> that is attached to the Apex zone. This allows the Apex to know how to discover the subdomain.</p>"},{"location":"domain_names/#docsichrisbirchcom","title":"<code>docs.ichrisbirch.com</code>","text":"<p>The docs are slightly different because they are hosted by <code>github</code>, being served with <code>mkdocs</code>, so they are not sitting on the server like the app (www) and api (api) are. There should be the similar <code>NS</code> and <code>soa</code> records created.  </p> <p>The <code>A</code> record points to the <code>github</code> (I think) hosts where the docs are hosted: <pre><code>\"185.199.108.153\",\n\"185.199.109.153\",\n\"185.199.110.153\",\n\"185.199.111.153\"\n</code></pre> The <code>CNAME</code> record lives at the <code>Apex</code> level, in place of the <code>NS</code> records like the <code>api</code> uses. Since the <code>CNAME</code> is an alias, it is used to alias <code>docs.ichrisbirch.com</code> =&gt; <code>datapointchris.github.io</code></p>"},{"location":"domain_names/#reference","title":"Reference","text":""},{"location":"domain_names/#1-ns-name-server-records","title":"1. NS (Name Server) Records","text":"<p>Purpose: NS records specify the authoritative name servers for a domain. These servers hold the DNS records for the domain. Function: Direct traffic by telling DNS resolvers which nameserver(s) to ask for the specific domain\u2019s information. Usage: Found at the domain's DNS zone and commonly points to multiple nameservers to provide redundancy. Example:</p> <pre><code>example.com. IN NS ns1.example.com.\nexample.com. IN NS ns2.example.com.\n</code></pre>"},{"location":"domain_names/#2-soa-start-of-authority-records","title":"2. SOA (Start of Authority) Records","text":"<p>Purpose: SOA records provide essential information about the DNS zone of a domain, including the primary nameserver, the admin\u2019s contact email, and timing information for zone transfers. Function: Defines the authoritative server and sets the rules for DNS caching, zone transfers, and DNS record updates. Usage: Should be the first record in a DNS zone file, as it contains critical operational data. Example:</p> <pre><code>example.com. IN SOA ns1.example.com. hostmaster.example.com. (\n              2023010101 ; Serial\n              7200       ; Refresh\n              3600       ; Retry\n              1209600    ; Expire\n              3600       ; Minimum TTL\n              )\n</code></pre>"},{"location":"domain_names/#3-a-address-records","title":"3. A (Address) Records","text":"<p>Purpose: A records map a domain or subdomain to an IPv4 address. Function: Translates the human-readable domain names to numerical IP addresses that computers use. Usage: Essential for pointing a domain or subdomain to a web server\u2019s IP address. Example:</p> <p><code>www.example.com. IN A 192.0.2.1</code></p>"},{"location":"domain_names/#4-cname-canonical-name-records","title":"4. CNAME (Canonical Name) Records","text":"<p>Purpose: CNAME records alias one domain name to another. Function: Points one domain/subdomain to another domain/subdomain, allowing management of multiple addresses by changing a single target address. Usage: Useful for pointing multiple subdomains to a single canonical name and to reduce redundancy in DNS management. Example:</p> <p><code>blog.example.com. IN CNAME www.example.com.</code></p>"},{"location":"domain_names/#how-they-relate-to-domains-and-subdomains","title":"How They Relate to Domains and Subdomains","text":"<p>NS Records: Define which servers are authoritative for the domain's DNS records. If you have subdomains, the NS records for the main domain affects them unless specifically overridden.</p> <p>SOA Records: Hold administrative information and control parameters for the DNS zone; they are vital for overall DNS zone health and updates.</p> <p>A Records: Directly tie domain names and subdomains to specific IP addresses. Different subdomains can be mapped to different IPs using A records.</p> <p>CNAME Records: Allow you to point subdomains (or even the root domain if needed) to other domain names, simplifying DNS management. For example, blog.example.com can point to <code>www.example.com</code>, which has an A record, thereby inheriting its IP indirectly.</p> <p>Example Application: For a domain example.com:</p> <ul> <li>NS Records: Point to ns1.example.com and ns2.example.com.</li> <li>SOA Record: Contains administrative details for the DNS zone, like contact info and timing settings.</li> <li>A Record: Points <code>www.example.com</code> to 192.0.2.1.</li> <li>CNAME Record: Points blog.example.com to <code>www.example.com</code>, which has the A record for the actual IP.</li> </ul>"},{"location":"domain_names/#aws-documentation","title":"AWS Documentation","text":"<p>Routing traffic for subdomains - Amazon Route 53Routing traffic for subdomains - Amazon Route 53</p>"},{"location":"git_secret/","title":"git-secret","text":"<p>Danger</p> <p>It seems that if <code>gpg</code> is updated then it causes some or all of the keys to need to be re-imported with <code>git secret</code> Also if there is a mismatch between <code>gpg</code> versions between computers or cicd and macos then it can create an issue. Overall, this is not the best way of handling secrets :sadface:</p>"},{"location":"git_secret/#making-a-secret","title":"Making a Secret","text":"<pre><code># for new repository\ngit secret init\n\n# This user has to have a public GPG key on THIS computer\ngit secret tell ichrisbirch@gmail.com\n\ngit secret add .env\ngit secret hide\n\ngit commit -am 'build: add secret .env file'\n</code></pre>"},{"location":"git_secret/#using-git-secret-with-ec2-instance","title":"Using git-secret with EC2 instance","text":""},{"location":"git_secret/#make-gpg-key-for-ec2-instance-on-local-machine","title":"Make gpg key for EC2 instance on local machine","text":"<pre><code>gpg --gen-key\n# Real name: iChrisBirch EC2\n# Email address: ec2@ichrisbirch.com\n\n# Export and upload keys to EC2 Instance\ngpg --export --armor ec2@ichrisbirch.com &gt; ec2-public.key\ngpg --export-secret-key --armor ec2@ichrisbirch.com &gt; ec2-private.key\nscp -i ~/.ssh/ichrisbirch-webserver.pem ec2-public.key ubuntu@ichrisbirch:~\nscp -i ~/.ssh/ichrisbirch-webserver.pem ec2-private.key ubuntu@ichrisbirch:~\n\n# Project Directory\ngit secret tell ec2@ichrisbirch.com\n# to re-encrypt them with the new authorized user\ngit secret reveal\ngit secret hide\ngit add .\ngit commit -m 'ops: Update secrets with new authorized user'\ngit push\n</code></pre>"},{"location":"git_secret/#import-gpg-key-on-ec2-instance","title":"Import gpg key on EC2 Instance","text":"<pre><code># Import keys\ngpg --import ec2-public.key\ngpg --import ec2-private.key\n\n# Project Directory\ngit pull\ngit secret reveal\n</code></pre>"},{"location":"git_secret/#make-a-gpg-key-for-cicd","title":"Make a gpg key for CICD","text":""},{"location":"git_secret/#make-a-new-key-locally","title":"Make a new key locally","text":"<pre><code># Generate new key, no passphrase\ngpg --gen-key\n# Export the secret key as one line, multiline not allowed\ngpg --armor --export-secret-key datapointchris@github.com | tr '\\n' ',' &gt; cicd-gpg-key.gpg\n# In the repository, make sure to add the new identity to allowed:\ngit secret tell datapointchris@github.com\ngit secret hide\n</code></pre>"},{"location":"git_secret/#add-the-key-to-the-cicd-environment-secrets","title":"Add the key to the CICD environment secrets","text":""},{"location":"git_secret/#add-run-step-to-cicd-workflow","title":"Add Run Step to CICD workflow","text":"<pre><code>- name: \"git-secret Reveal .env files\"\n  run: |\n    # Import private key and avoid the \"Inappropriate ioctl for device\" error\n    echo {% raw %}${{ secrets.CICD_GPG_KEY }}{% endraw %} | tr ',' '\\n' | gpg --batch --yes --pinentry-mode loopback --import\n    git secret reveal\n</code></pre>"},{"location":"git_secret/#expired-gpg-key","title":"Expired GPG key","text":"<p><code>git-secret: warning: at least one key for email(s) is revoked, expired, or otherwise invalid: ichrisbirch@gmail.com</code></p> <p>Expired keys need to have their expiry date extended, which requires the following steps:</p> <pre><code># List keys and subkey(s)\ngpg --list-secret-keys --verbose --with-subkey-fingerprints\n\n&gt;&gt;&gt; sec   ed25519 2022-04-19 [SC] [expired: 2024-04-18]\n&gt;&gt;&gt;       B98C7D8073BB87...\n&gt;&gt;&gt; uid           [ultimate] Chris Birch &lt;ichrisbirch@gmail.com&gt;\n&gt;&gt;&gt; ssb   cv25519 2022-04-19 [E] [expired: 2024-04-18]\n&gt;&gt;&gt;       2E418AB946A0ECA...\n\n# Set new expiry date for primary key and subkey(s)\n# NOTE: MUST put the primary key first, expire date, subkeys after in the same command\ngpg --quick-set-expire B98C7D8073BB87... 1y 2E418AB946A0ECA...\n\n# Check that the keys are no longer expired\ngpg --list-secret-keys --verbose --with-subkey-fingerprints\n\n&gt;&gt;&gt; sec   ed25519 2022-04-19 [SC] [expires: 2025-04-19]\n&gt;&gt;&gt;       B98C7D8073BB87...\n&gt;&gt;&gt; uid           [ultimate] Chris Birch &lt;ichrisbirch@gmail.com&gt;\n&gt;&gt;&gt; ssb   cv25519 2022-04-19 [E] [expires: 2025-04-19]\n&gt;&gt;&gt;       2E418AB946A0ECA...\n\n# Remove the expired email address for git-secret\ngit secret removeperson ichrisbirch@gmail.com\n\n&gt;&gt;&gt; git-secret: removed keys.\n&gt;&gt;&gt; git-secret: now [ichrisbirch@gmail.com] do not have an access to the repository.\n&gt;&gt;&gt; git-secret: make sure to hide the existing secrets again.\n\n# Add the email address as authorized viewer\ngit secret tell ichrisbirch@gmail.com\n\ngit-secret: done. ichrisbirch@gmail.com added as user(s) who know the secret.\n\n# Hide the secrets again\ngit secret hide\n\n&gt;&gt;&gt; git-secret: done. 3 of 3 files are hidden.\n\n# Check status to see that they are hidden\ngit status\n\n&gt;&gt;&gt;        modified:   .dev.env.secret\n&gt;&gt;&gt;        modified:   .gitsecret/keys/pubring.kbx\n&gt;&gt;&gt;        modified:   .gitsecret/keys/pubring.kbx~\n&gt;&gt;&gt;        modified:   .prod.env.secret\n&gt;&gt;&gt;        modified:   .test.env.secret\n</code></pre>"},{"location":"html5_semantic/","title":"Semantic HTML","text":"<p>HTML5 introduced a set of semantic elements that provide meaningful information about the content they wrap, making web pages more readable for both developers and machines (like search engines or screen readers). Below is an example of a basic web page utilizing several common HTML5 semantic tags, along with explanations of when and why each tag is used.</p>"},{"location":"html5_semantic/#example-html5-page-with-semantic-tags","title":"Example HTML5 Page with Semantic Tags","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Example HTML5 Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;header&gt;\n    &lt;h1&gt;My Website&lt;/h1&gt;\n    &lt;nav&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;a href=\"#home\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/nav&gt;\n&lt;/header&gt;\n\n&lt;section id=\"home\"&gt;\n    &lt;h2&gt;Welcome to My Website&lt;/h2&gt;\n    &lt;p&gt;This is a paragraph explaining what my website is about.&lt;/p&gt;\n&lt;/section&gt;\n\n&lt;article&gt;\n    &lt;h2&gt;Blog Post Title&lt;/h2&gt;\n    &lt;p&gt;Posted on &lt;time datetime=\"2023-04-01\"&gt;April 1, 2023&lt;/time&gt;&lt;/p&gt;\n    &lt;p&gt;This is a blog post. It contains interesting content about a certain topic.&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;aside&gt;\n    &lt;h2&gt;About Me&lt;/h2&gt;\n    &lt;p&gt;This section provides information about the website owner or related links.&lt;/p&gt;\n&lt;/aside&gt;\n\n&lt;footer&gt;\n    &lt;p&gt;Contact information and copyright notice goes here.&lt;/p&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"html5_semantic/#explanations-of-semantic-tags","title":"Explanations of Semantic Tags","text":""},{"location":"html5_semantic/#header","title":"<code>&lt;header&gt;</code>","text":"<ul> <li>When to use: For introductory content or navigation links at the top of a section or page.</li> <li>Why: Helps identify the top part of a page or section, often containing the website's logo, navigation links, or titles.</li> </ul>"},{"location":"html5_semantic/#nav","title":"<code>&lt;nav&gt;</code>","text":"<ul> <li>When to use: For navigation links.</li> <li>Why: Indicates a section with navigation links to other pages or parts of the page. Search engines and screen readers can identify the navigation structure of a site more easily.</li> </ul>"},{"location":"html5_semantic/#section","title":"<code>&lt;section&gt;</code>","text":"<ul> <li>When to use: For a thematic grouping of content, typically with a heading.</li> <li>Why: Organizes the page content into thematic groups for easier understanding and navigation. Each <code>&lt;section&gt;</code> should ideally represent a standalone part of the page.</li> </ul>"},{"location":"html5_semantic/#article","title":"<code>&lt;article&gt;</code>","text":"<ul> <li>When to use: For self-contained, independent pieces of content that could be distributed and reused, like blog posts or news articles.</li> <li>Why: Marks content as being a complete, self-contained piece of the page's content. It's important for syndication and when separating document sections that could stand alone or be reused.</li> </ul>"},{"location":"html5_semantic/#aside","title":"<code>&lt;aside&gt;</code>","text":"<ul> <li>When to use: For tangentially related content to the main content, such as sidebars.</li> <li>Why: Differentiates side content from the main content, making it clear that it's supplementary. Good for sidebars, advertising, or content that complements the main content.</li> </ul>"},{"location":"html5_semantic/#footer","title":"<code>&lt;footer&gt;</code>","text":"<ul> <li>When to use: For footer content at the bottom of a section or page.</li> <li>Why: Contains information about the author, copyright notices, contact information, etc. It marks the end of a section or document.</li> </ul>"},{"location":"html5_semantic/#time","title":"<code>&lt;time&gt;</code>","text":"<ul> <li>When to use: To represent a specific period (a date or time).</li> <li>Why: Provides a standard way to encode dates and times in HTML, making it easier for machines to interpret datetime values in a human-readable format.</li> </ul> <p>These semantic elements are crucial for creating a well-structured and accessible webpage, improving both user experience and SEO.</p>"},{"location":"logging-configuration/","title":"LOG_DIR Configuration Guide","text":""},{"location":"logging-configuration/#overview","title":"Overview","text":"<p>The logging system now seamlessly handles log directory configuration across different environments using an environment variable approach with intelligent fallbacks.</p>"},{"location":"logging-configuration/#how-it-works","title":"How It Works","text":""},{"location":"logging-configuration/#1-python-logger-ichrisbirchloggerpy","title":"1. Python Logger (ichrisbirch/logger.py)","text":"<p>The logger now uses a <code>get_log_base_location()</code> function with this priority order:</p> <pre><code>def get_log_base_location():\n    # 1. Check for explicit LOG_DIR environment variable (Docker/manual override)\n    if log_dir := os.environ.get('LOG_DIR'):\n        return log_dir\n\n    # 2. Fall back to platform detection for bare metal deployments\n    if platform.system() == 'Darwin':\n        return '/usr/local/var/log/ichrisbirch'\n\n    # 3. Default Linux location\n    return '/var/log/ichrisbirch'\n</code></pre>"},{"location":"logging-configuration/#2-docker-compose-configuration","title":"2. Docker Compose Configuration","text":"<p>All services now pass <code>LOG_DIR</code> as an environment variable to containers:</p> <pre><code>environment:\n  - LOG_DIR=${LOG_DIR:-/var/log/ichrisbirch}\n</code></pre> <p>Volume mounts use the same variable:</p> <pre><code>volumes:\n  - ${LOG_DIR:-/var/log/ichrisbirch}:/var/log/ichrisbirch\n</code></pre>"},{"location":"logging-configuration/#3-cli-integration","title":"3. CLI Integration","text":"<p>The CLI already sets <code>LOG_DIR</code> based on platform:</p> <pre><code>if [[ $(uname) == \"Darwin\" ]]; then\n    LOG_DIR=\"/usr/local/var/log/ichrisbirch\"\nelse\n    LOG_DIR=\"/var/log/ichrisbirch\"\nfi\n</code></pre>"},{"location":"logging-configuration/#environment-files","title":"Environment Files","text":""},{"location":"logging-configuration/#development-devenvexample","title":"Development (.dev.env.example)","text":"<pre><code># Development on macOS (host directory)\nLOG_DIR=/usr/local/var/log/ichrisbirch\n</code></pre>"},{"location":"logging-configuration/#production-prodenvexample","title":"Production (.prod.env.example)","text":"<pre><code># Production (standard Linux location)\nLOG_DIR=/var/log/ichrisbirch\n</code></pre>"},{"location":"logging-configuration/#usage-scenarios","title":"Usage Scenarios","text":""},{"location":"logging-configuration/#1-development-on-macos-with-docker","title":"1. Development on macOS with Docker","text":"<pre><code># CLI automatically sets LOG_DIR=/usr/local/var/log/ichrisbirch\nichrisbirch dev start\n# \u2705 Logs go to /usr/local/var/log/ichrisbirch on host\n# \u2705 Python logger detects LOG_DIR env var and uses it\n</code></pre>"},{"location":"logging-configuration/#2-production-linux-with-docker","title":"2. Production Linux with Docker","text":"<pre><code># CLI automatically sets LOG_DIR=/var/log/ichrisbirch\nichrisbirch prod start  \n# \u2705 Logs go to /var/log/ichrisbirch on host\n# \u2705 Python logger detects LOG_DIR env var and uses it\n</code></pre>"},{"location":"logging-configuration/#3-bare-metal-development-no-docker","title":"3. Bare Metal Development (no Docker)","text":"<pre><code># Python logger falls back to platform detection\n# \u2705 macOS: /usr/local/var/log/ichrisbirch\n# \u2705 Linux: /var/log/ichrisbirch\n</code></pre>"},{"location":"logging-configuration/#4-custom-override","title":"4. Custom Override","text":"<pre><code>export LOG_DIR=/custom/log/path\nichrisbirch dev start\n# \u2705 Logs go to /custom/log/path\n</code></pre>"},{"location":"logging-configuration/#benefits","title":"Benefits","text":"<ol> <li>Seamless: No manual configuration needed</li> <li>Consistent: Same log location on host and in container</li> <li>Flexible: Easy to override for testing or custom deployments</li> <li>Backward Compatible: Works with existing bare metal deployments</li> <li>Industry Standard: Environment variable approach is widely used</li> </ol>"},{"location":"logging-configuration/#migration","title":"Migration","text":"<p>No migration needed! The new system:</p> <ul> <li>\u2705 Automatically works with existing CLI usage</li> <li>\u2705 Maintains platform detection for bare metal</li> <li>\u2705 Adds Docker environment variable support</li> <li>\u2705 Preserves all existing log file locations</li> </ul> <p>Your existing setup will work unchanged, with improved Docker integration.</p>"},{"location":"project_layout/","title":"Project Layout","text":""},{"location":"project_layout/#environment-files","title":"Environment Files","text":"<p>Location: <code>/</code> <code>.dev.env</code> <code>.test.env</code> <code>.prod.env</code></p>"},{"location":"project_layout/#project-configuration","title":"Project Configuration","text":"<p>Location: <code>/</code> <code>config.py</code> - Config classes for environments</p>"},{"location":"scheduler/","title":"Scheduler","text":""},{"location":"scheduler/#apscheduler","title":"APScheduler","text":"<p>The scheduler is run in it's own <code>wsgi</code> application managed by <code>supervisor</code>. The scheduler is using the standard blocking scheduler since it is in its own process. Workers need to be set to 1 for <code>gunicorn</code> in order to not start multiple instances of the scheduler. Technically the scheduler could be run as part of the API since the tasks are related to the API, but the API will be changing to async in the future which would require a different scheduler, and the jobs may not always be only related to the API.</p> <p>The jobs are located in the <code>jobs.py</code> file in the <code>/scheduler</code> directory.</p>"},{"location":"scheduler/#current-jobs","title":"Current Jobs","text":"<p><code>decrease_task_priority</code> - Decreases the priority of all tasks by 1 every 24 hours.</p> <p><code>check_and_run_autotasks</code> - Checks if any autotasks need to be run based on their schedule and runs them if so.</p> <p><code>backup_database</code> - Backs up the postgres database to S3 every 3 days.</p>"},{"location":"terraform/","title":"Terraform","text":""},{"location":"terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"terraform/#terraform-state-is-locked","title":"Terraform State is Locked","text":"<p>Github Runners sometimes lock the state.</p> <p>Locally: <code>terraform plan</code> -&gt; This will give you an ID of the lock. <code>terraform force-unlock $LOCK_ID</code></p>"},{"location":"terraform/#terraform-plan-backend-configuration-changed","title":"terraform plan -&gt; backend configuration changed","text":"<p>Reference: Confusing error message when terraform backend is changed - Terraform - HashiCorp Discuss</p> <pre><code>\u279c terraform plan\n\u2577\n\u2502 Error: Backend initialization required: please run \"terraform init\"\n\u2502\n\u2502 Reason: Backend configuration block has changed\n\u2502\n\u2502 The \"backend\" is the interface that Terraform uses to store state,\n\u2502 perform operations, etc. If this message is showing up, it means that the\n\u2502 Terraform configuration you're using is using a custom configuration for\n\u2502 the Terraform backend.\n\u2502\n\u2502 Changes to backend configurations require reinitialization. This allows\n\u2502 Terraform to set up the new configuration, copy existing state, etc. Please run\n\u2502 \"terraform init\" with either the \"-reconfigure\" or \"-migrate-state\" flags to\n\u2502 use the current configuration.\n\u2502\n\u2502 If the change reason above is incorrect, please verify your configuration\n\u2502 hasn't changed and try again. At this point, no changes to your existing\n\u2502 configuration or state have been made.\n\u2575\n\n\n\u279c terraform init\nInitializing the backend...\n\u2577\n\u2502 Error: Backend configuration changed\n\u2502\n\u2502 A change in the backend configuration has been detected, which may require migrating existing state.\n\u2502\n\u2502 If you wish to attempt automatic migration of the state, use \"terraform init -migrate-state\".\n\u2502 If you wish to store the current configuration with no changes to the state, use \"terraform init -reconfigure\".\n\n\n\u279c terraform init -migrate-state\nInitializing the backend...\nBackend configuration changed!\n\nTerraform has detected that the configuration specified for the backend\nhas changed. Terraform will now check for existing state in the backends.\n\n\u2577\n\u2502 Error: Failed to decode current backend config\n\u2502\n\u2502 The backend configuration created by the most recent run of \"terraform init\" could not be decoded: unsupported attribute \"assume_role_duration_seconds\". The configuration may have been initialized by an earlier version that\n\u2502 used an incompatible configuration structure. Run \"terraform init -reconfigure\" to force re-initialization of the backend.\n\u2575\n\n\n\u279c terraform init -reconfigure\nInitializing the backend...\n\nSuccessfully configured the backend \"s3\"! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing provider plugins...\n- Reusing previous version of hashicorp/aws from the dependency lock file\n- Reusing previous version of hashicorp/tls from the dependency lock file\n- Using previously-installed hashicorp/aws v5.67.0\n- Using previously-installed hashicorp/tls v4.0.6\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>This was caused by an upgrade to terraform, a new version had different configuration options.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#flask","title":"Flask","text":"<p>Error</p> <p>Blank pages loading, but no errors.</p> <p>Solution</p> <p>Try a different port Sometimes the port is busy or used, but does not give a 'port in use' error</p>"},{"location":"troubleshooting/#poetry","title":"Poetry","text":"<p>Error</p> <p>ModuleNotFoundError: No module named 'cachecontrol' when running poetry:</p> <p>Solution</p> <p><code>sudo apt install python3-cachecontrol</code></p>"},{"location":"troubleshooting/#supervisor","title":"Supervisor","text":"<p>Error</p> <p>supervisor.sock no such file</p> <p>Solution</p> <p>make sure directories and files for logs are created.</p> <p>Error</p> <p>BACKOFF can't find command... that is pointing to .venv</p> <p>Solution</p> <p>Prod: Check that the project is installed Dev: Check the symlink isn't broken</p> <p>Error</p> <pre><code>error: &lt;class 'FileNotFoundError'&gt;, [Errno 2] No such file or directory: file: /usr/local/Cellar/supervisor/4.2.5/libexec/lib/python3.11/site-packages/supervisor/xmlrpc.py line: 55\n</code></pre> <p>Solution</p> <p>Start and run supervisor with homebrew: <code>brew services start supervisor</code></p> <p>Error</p> <pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/var/www/ichrisbirch/ichrisbirch/NoneNone/pylogger.log'\n</code></pre> <p>Solution</p> <p>The environment file has not been loaded. Most likely you need to run <code>git secret reveal</code> This happens when the project has been cloned for the first time or directory has been deleted or the env files might have changed.</p>"},{"location":"troubleshooting/#nginx","title":"NGINX","text":"<p>Error</p> <p>bind() to 0.0.0.0:80 failed (98: Address already in use)</p> <p>Solution</p> <p><code>sudo pkill -f nginx &amp; wait $!</code> <code>sudo systemctl start nginx</code></p> <p>Error</p> <p>DEV bind() to 127.0.0.1:80 failed (13: Permission denied)</p> <p>Solution</p> <p>NGINX is not running as root.  It does not run reliably with homebrew. Use <code>sudo nginx -s reload</code> instead of homebrew.</p>"},{"location":"troubleshooting/#api-postgres","title":"API Postgres","text":"<p>Error</p> <p>[error] 94580#0: *18 kevent() reported that connect() failed (61: Connection refused) while connecting to upstream, client: 127.0.0.1, server: api.localhost, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:4200/\", host: \"api.macmini.local</p> <p>Solution</p> <p>DB cannot connect.  Postgres string was built wrong, corrected by adding a test to check config is loaded properly.</p> <p>Error</p> <p>Local changes were working but nothing that connected to prod postgres.</p> <p><code>api.ichrisbirch.com/tasks/</code> - 502 Bad Gateway <code>api.ichrisbirch.com</code> Success redirect to <code>/docs</code> <code>ichrisbirch.com</code> redirects to www in browser but error with requests <code>www.ichrisbirch.com/tasks/</code> - Internal Server Error Can connect to prod server with DBeaver Verified that the connection info is the same. Seems that the API is not connecting to postgres instance</p> <p>api.macmini.local WORKING api.macmini.local/ WORKING api.macmini.local/docs WORKING api.macmini.local/tasks WORKING api.macmini.local/tasks/1 WORKING api.macmini.local/tasks/completed</p> <p>ichrisbirch.com WORKING api.ichrisbirch.com/ WORKING api.ichrisbirch.com/docs ERROR api.ichrisbirch.com/tasks ERROR api.ichrisbirch.com/tasks/1 ERROR api.ichrisbirch.com/tasks/completed</p> <p>Solution</p> <p>The issue was resolved by modifying the security group of the postgres instance to allow the ec2 instance to connect by allowing it's security group.</p>"},{"location":"troubleshooting/#pytest","title":"Pytest","text":"<p>Error</p> <p>E       assert 307 == 200 E        +  where 307 = &lt;Response [307]&gt;.status_code</p> <p>Solution</p> <p>The trailing <code>/</code> is missing from the endpoint being called in the test, resulting in a 307 Temporary Redirect To fix: <code>/endpoint</code> --&gt; <code>/endpoint/</code></p>"},{"location":"troubleshooting/#alembic","title":"Alembic","text":"<p>Error</p> <p>Alembic is not able to upgrade to the latest because the revisions got out of sync.</p> <p>Solution</p> <p>Find the last revision that was successfully run (manually by inspecting the database) and then run: <code>alembic stamp &lt;revision&gt;</code> to set the current revision to the last successful one. Then run the upgrade again: <code>alembic upgrade head</code></p>"},{"location":"troubleshooting/#fastapi","title":"FastAPI","text":"<p>Error</p> <p>Request Error: Client error '405 Method Not Allowed' for url xxx</p> <p>Solution</p> <p>Make sure that the <code>id</code> is being passed correctly for routes like <code>/endpoint/{id}/</code> The error will not say <code>id</code> is not found, it will give a 405 error because the url is not correct</p> <p>Error</p> <p><code>PATCH</code> endpoint giving: 422 Unprocessable Entity: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"id\"],\"msg\":\"Field required\"</p> <p>Solution</p> <p><code>PATCH</code> endpoints require the ID in the endpoint, but also the ID must be passed in the payload for the model so it can update the record in the DB by ID.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#fastapi-crud-endpoints","title":"FastAPI Crud Endpoints","text":"<p>Order matters with endpoints, dynamic routes <code>route/endpoint/{id}</code> are last. They even have to be after other endpoints:</p> <p><code>route/</code> <code>route/endpoint/</code> <code>route/endpoint/extension</code> The two below don't matter the order, only that they are after all of the endpoints that do not take in a path variable. <code>route/{id}</code> <code>route/endpoint/{id}</code> </p>"},{"location":"api/#endpoint-structure","title":"Endpoint Structure","text":""},{"location":"api/#get-vs-post-vs-put-for-updating-resources","title":"GET vs POST vs PUT for Updating Resources","text":"<p>In RESTful API design, it's common to use a POST or PUT request when you want to update a resource.</p> <p>A <code>GET</code> request should be idempotent, meaning that making the same request multiple times should have the same effect as making it once. In your case, marking a task as complete changes the state of the task, so a <code>GET</code> request would not be appropriate.</p> <p>Between <code>POST</code> and <code>PUT</code>, the choice depends on whether you consider marking a task as complete to be a partial update of the task or a creation of a new state for the task.</p> <p>If you consider it to be a partial update, you should use a <code>PUT</code> request. If you consider it to be a creation of a new state, you should use a <code>POST</code> request.</p> <p>In your current implementation, you're using a <code>POST</code> request, which is perfectly fine. If you wanted to use a PUT request, you could change the decorator to <code>@router.put</code> and the route to something like <code>/task/{task_id}/complete/</code>.</p>"},{"location":"api/#endpoint-resource-vs-action-order","title":"Endpoint Resource vs Action Order","text":"<p>The choice between <code>/tasks/{task_id}/complete/</code> and <code>/tasks/complete/{task_id}/</code> is largely a matter of personal preference and the conventions you've established in your project. However, the most common and RESTful way to design the endpoint would be <code>/tasks/{task_id}/complete/</code>.</p> <p>This is because in REST, URLs are used to represent resources, and the components of the URL are used to form a hierarchy of resources. In this case, the task with a specific task_id is the resource, and complete is an action on that resource. So, it makes sense to structure the URL as <code>/tasks/{task_id}/complete/</code>, where complete is a sub-resource of the task.</p> <p>This structure also has the advantage of being consistent with the other endpoints in your code, which use the structure <code>/tasks/{task_id}/</code>.</p>"},{"location":"api/#put-vs-patch","title":"PUT vs PATCH","text":"<p>In RESTful API design, PUT and PATCH are both used to update a resource, but they are used in slightly different ways:</p> <p>PUT is used to update a resource with a complete new version. It's idempotent, meaning that making the same PUT request multiple times will have the same effect as making it once. If you PUT a resource and then PUT it again with the same data, the second request will have no effect.</p> <p>PATCH, on the other hand, is used to update a resource with a partial update. It's not idempotent by nature, meaning that making the same PATCH request multiple times may have different effects. For example, if you PATCH a resource to increment a counter, making the same PATCH request again will increment the counter again.</p> <p>In RESTful API design, a successful <code>PATCH</code> request typically returns a <code>200 OK</code> status code along with the updated resource. This allows the client to see the changes that were made, which might be different from the changes that were requested if some of the changes couldn't be applied.</p> <p>However, if the <code>PATCH</code> request doesn't return the updated resource, it should return a <code>204 No Content</code> status code to indicate that the request was successful but there's no representation to return (i.e., no body).</p>"},{"location":"api/#good-api-design","title":"Good API Design","text":""},{"location":"api/authentication_strategies/","title":"API Authentication Strategies","text":"<p>This document outlines the different authentication methods for various types of API access.</p>"},{"location":"api/authentication_strategies/#authentication-hierarchy","title":"Authentication Hierarchy","text":""},{"location":"api/authentication_strategies/#1-internal-service-authentication","title":"1. Internal Service Authentication","text":"<p>Who: Your Flask app, background jobs, internal services Purpose: Trusted components of your application ecosystem Implementation: Shared service key</p> <pre><code>from ichrisbirch.api.client import internal_service_client\n\n# Your Flask app calling your API\nclient = internal_service_client(\"flask-frontend\")\nusers = client.resource('users', UserModel)\nuser = users.list(username=\"john_doe\")[0]  # Check username for login\n</code></pre>"},{"location":"api/authentication_strategies/#2-developer-api-keys","title":"2. Developer API Keys","text":"<p>Who: External developers building their own frontends Purpose: Third-party application access with controlled permissions Implementation: Individual API keys with scoping</p> <pre><code># Custom provider for external developers\nclass DeveloperAPIKeyProvider(CredentialProvider):\n    def __init__(self, api_key: str, app_name: str = \"external-app\"):\n        self.api_key = api_key\n        self.app_name = app_name\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\n            \"X-API-Key\": self.api_key,\n            \"X-App-Name\": self.app_name\n        }\n\n    def is_available(self) -&gt; bool:\n        return bool(self.api_key)\n\n# External developer usage\nclient = APIClient(credential_provider=DeveloperAPIKeyProvider(\"dev_abc123\"))\n</code></pre>"},{"location":"api/authentication_strategies/#3-user-token-authentication","title":"3. User Token Authentication","text":"<p>Who: End users of external applications Purpose: User-scoped access through external frontends Implementation: OAuth 2.0 or JWT tokens</p> <pre><code># For user-specific operations\nclass UserBearerTokenProvider(CredentialProvider):\n    def __init__(self, user_token: str):\n        self.user_token = user_token\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\"Authorization\": f\"Bearer {self.user_token}\"}\n\n    def is_available(self) -&gt; bool:\n        return bool(self.user_token)\n\n# External app on behalf of user\nclient = APIClient(credential_provider=UserBearerTokenProvider(\"user_jwt_token\"))\n</code></pre>"},{"location":"api/authentication_strategies/#api-access-scenarios","title":"API Access Scenarios","text":""},{"location":"api/authentication_strategies/#scenario-1-your-flask-app-current","title":"Scenario 1: Your Flask App (Current)","text":"<pre><code>@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form['username']\n    password = request.form['password']\n\n    # Check credentials via API\n    with internal_service_client(\"flask-frontend\") as client:\n        users = client.resource('users', UserModel)\n\n        # Find user by username\n        user_list = users.list(username=username)\n        if not user_list:\n            return \"Invalid username\", 401\n\n        user = user_list[0]\n\n        # Verify password via API action\n        auth_result = users.action('verify_password', {\n            'user_id': user.id,\n            'password': password\n        })\n\n        if auth_result['valid']:\n            session['user_id'] = user.id\n            return redirect('/dashboard')\n        else:\n            return \"Invalid password\", 401\n</code></pre>"},{"location":"api/authentication_strategies/#scenario-2-external-developer-building-mobile-app","title":"Scenario 2: External Developer Building Mobile App","text":"<pre><code># Mobile app developer gets API key: \"dev_mobile_app_xyz789\"\n# They build a React Native app\n\n// In their mobile app\nconst apiKey = \"dev_mobile_app_xyz789\";\n\n// Login endpoint for their users\nasync function loginUser(username, password) {\n    const response = await fetch('/api/auth/login', {\n        method: 'POST',\n        headers: {\n            'X-API-Key': apiKey,\n            'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ username, password })\n    });\n\n    if (response.ok) {\n        const { user_token } = await response.json();\n        // Store user token for subsequent requests\n        return user_token;\n    }\n}\n\n// Accessing user data\nasync function getUserTasks(userToken) {\n    const response = await fetch('/api/tasks', {\n        headers: {\n            'Authorization': `Bearer ${userToken}`,\n            'X-API-Key': apiKey\n        }\n    });\n    return response.json();\n}\n</code></pre>"},{"location":"api/authentication_strategies/#scenario-3-external-web-app-developer","title":"Scenario 3: External Web App Developer","text":"<pre><code># Python web developer gets API key: \"dev_web_portal_abc456\"\n# They build a Django app that integrates with your API\n\nfrom your_api_client import APIClient, DeveloperAPIKeyProvider, UserBearerTokenProvider\n\nclass ExternalTaskService:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def authenticate_user(self, username: str, password: str) -&gt; Optional[str]:\n        \"\"\"Authenticate user and return JWT token\"\"\"\n        provider = DeveloperAPIKeyProvider(self.api_key, \"external-web-app\")\n\n        with APIClient(credential_provider=provider) as client:\n            auth = client.resource('auth', AuthModel)\n            result = auth.action('login', {\n                'username': username,\n                'password': password\n            })\n\n            return result.get('token') if result['success'] else None\n\n    def get_user_tasks(self, user_token: str) -&gt; List[dict]:\n        \"\"\"Get tasks for authenticated user\"\"\"\n        user_provider = UserBearerTokenProvider(user_token)\n\n        with APIClient(credential_provider=user_provider) as client:\n            tasks = client.resource('tasks', TaskModel)\n            return tasks.list()\n</code></pre>"},{"location":"api/authentication_strategies/#backend-api-implementation","title":"Backend API Implementation","text":"<p>You'd need to add authentication middleware to your FastAPI backend:</p> <pre><code># In your FastAPI app\nfrom fastapi import HTTPException, Depends, Header\nfrom typing import Optional\n\nasync def get_api_key(x_api_key: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract API key from headers\"\"\"\n    return x_api_key\n\nasync def get_user_token(authorization: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract user token from Authorization header\"\"\"\n    if authorization and authorization.startswith(\"Bearer \"):\n        return authorization[7:]  # Remove \"Bearer \" prefix\n    return None\n\nasync def get_internal_service(x_internal_service: Optional[str] = Header(None)) -&gt; Optional[str]:\n    \"\"\"Extract internal service auth\"\"\"\n    return x_internal_service\n\n@app.middleware(\"http\")\nasync def auth_middleware(request: Request, call_next):\n    \"\"\"Authentication middleware to validate requests\"\"\"\n\n    # Internal service requests\n    if request.headers.get(\"X-Internal-Service\"):\n        # Validate service key\n        service_key = request.headers.get(\"Authorization\", \"\").replace(\"Service \", \"\")\n        if not validate_service_key(service_key):\n            raise HTTPException(401, \"Invalid service key\")\n\n    # Developer API key requests\n    elif request.headers.get(\"X-API-Key\"):\n        api_key = request.headers.get(\"X-API-Key\")\n        if not validate_developer_key(api_key):\n            raise HTTPException(401, \"Invalid API key\")\n\n    # User token requests\n    elif request.headers.get(\"Authorization\", \"\").startswith(\"Bearer \"):\n        token = request.headers.get(\"Authorization\")[7:]\n        if not validate_user_token(token):\n            raise HTTPException(401, \"Invalid user token\")\n\n    else:\n        raise HTTPException(401, \"Authentication required\")\n\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"api/authentication_strategies/#developer-onboarding-process","title":"Developer Onboarding Process","text":""},{"location":"api/authentication_strategies/#for-external-developers","title":"For External Developers","text":"<ol> <li>Registration: Developer signs up on your developer portal</li> <li>API Key Generation: System generates unique API key</li> <li>Documentation: Provide API docs and client library</li> <li>Rate Limiting: Apply limits based on their plan</li> <li>Monitoring: Track usage and provide analytics</li> </ol> <pre><code># Developer management endpoints\n@router.post(\"/developer/register\")\nasync def register_developer(developer_info: DeveloperRegistration):\n    \"\"\"Register new developer and generate API key\"\"\"\n    api_key = generate_api_key()\n\n    # Store in database\n    dev_record = DeveloperAccount(\n        name=developer_info.name,\n        email=developer_info.email,\n        api_key=api_key,\n        rate_limit=1000,  # requests per hour\n        scopes=[\"read:tasks\", \"read:users\"]  # limited permissions\n    )\n\n    return {\"api_key\": api_key, \"documentation\": \"/docs/api\"}\n\n@router.get(\"/developer/usage\")\nasync def get_usage_stats(api_key: str = Depends(get_api_key)):\n    \"\"\"Get API usage statistics for developer\"\"\"\n    return get_developer_usage(api_key)\n</code></pre>"},{"location":"api/authentication_strategies/#summary","title":"Summary","text":"<ul> <li>Internal Service Auth: Your Flask app and internal services (current implementation)</li> <li>Developer API Keys: External developers building applications</li> <li>User Tokens: End users of those external applications</li> </ul> <p>The key insight is that you have three layers of authentication:</p> <ol> <li>Service-level (your internal components)</li> <li>Application-level (external developers)  </li> <li>User-level (end users through external apps)</li> </ol> <p>Would you like me to help implement any of these authentication providers or show you how to set up the developer registration system?</p>"},{"location":"api/client/","title":"API Client Architecture","text":"<p>The ichrisbirch API Client provides a modern, session-based architecture for interacting with the FastAPI backend. This architecture follows industry patterns from libraries like boto3 and Stripe, providing flexible authentication, resource management, and request handling.</p>"},{"location":"api/client/#overview","title":"Overview","text":"<p>The API client consists of four main components:</p> <ol> <li>Credential Providers (<code>auth.py</code>) - Pluggable authentication strategies</li> <li>Session Management (<code>session.py</code>) - Persistent configuration and HTTP client management</li> <li>Resource Clients (<code>resource.py</code>) - Generic CRUD operations for API resources</li> <li>Main API Client (<code>api.py</code>) - High-level interface with factory methods</li> </ol>"},{"location":"api/client/#quick-start","title":"Quick Start","text":""},{"location":"api/client/#basic-usage","title":"Basic Usage","text":"<pre><code>from ichrisbirch.api.client import APIClient\n\n# Context-aware client (automatically detects Flask context)\nclient = APIClient()\n\n# Get a resource client for tasks\ntasks = client.resource('tasks', TaskModel)\n\n# CRUD operations\ntask = tasks.get(123)\nall_tasks = tasks.list()\nnew_task = tasks.create({'title': 'New Task', 'description': 'Task description'})\nupdated_task = tasks.update(123, {'status': 'completed'})\ntasks.delete(123)\n\n# Custom actions\nresult = tasks.action('bulk_complete', {'task_ids': [1, 2, 3]})\n\n# Direct API requests\nresponse = client.request('GET', '/custom/endpoint')\n</code></pre>"},{"location":"api/client/#authentication-patterns","title":"Authentication Patterns","text":"<pre><code>from ichrisbirch.api.client import (\n    internal_service_client,\n    user_client,\n    flask_session_client,\n    default_client\n)\n\n# Internal service authentication\nclient = internal_service_client('flask-frontend')\n\n# User authentication\nclient = user_client('user123')\n\n# Flask session authentication\nclient = flask_session_client()\n\n# Default context-aware authentication\nclient = default_client()\n</code></pre>"},{"location":"api/client/#architecture-principles","title":"Architecture Principles","text":""},{"location":"api/client/#session-based-design","title":"Session-Based Design","text":"<p>Following the boto3 pattern, the client uses sessions to manage:</p> <ul> <li>HTTP client lifecycle</li> <li>Authentication state</li> <li>Base URL and default headers</li> <li>Request/response handling</li> </ul>"},{"location":"api/client/#pluggable-authentication","title":"Pluggable Authentication","text":"<p>Credential providers allow different authentication strategies:</p> <ul> <li>InternalServiceProvider: Service-to-service authentication</li> <li>UserTokenProvider: User-based token authentication  </li> <li>FlaskSessionProvider: Flask session-based authentication</li> </ul>"},{"location":"api/client/#generic-resource-pattern","title":"Generic Resource Pattern","text":"<p>Instead of specific factory methods for each resource type, the client uses a generic <code>resource()</code> method that works with any Pydantic model:</p> <pre><code># Generic pattern (preferred)\ntasks = client.resource('tasks', TaskModel)\nusers = client.resource('users', UserModel)\n\n# Avoids specific factories like:\n# tasks = client.tasks()  # Not implemented\n# users = client.users()  # Not implemented\n</code></pre>"},{"location":"api/client/#context-aware-defaults","title":"Context-Aware Defaults","text":"<p>The client automatically detects context and chooses appropriate authentication:</p> <ul> <li>Inside Flask request context: Uses Flask session</li> <li>Outside Flask context: Uses internal service authentication</li> <li>No defensive defaults - fails fast on misconfiguration</li> </ul>"},{"location":"api/client/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Credential Providers - Authentication strategies and implementation</li> <li>Session Management - Session lifecycle and configuration</li> <li>Resource Clients - CRUD operations and custom actions</li> <li>Migration Guide - Migrating from QueryAPI to the new client</li> <li>Usage Examples - Common patterns and use cases</li> </ul>"},{"location":"api/client/#design-benefits","title":"Design Benefits","text":"<ol> <li>Industry Standard Pattern: Follows established patterns from boto3, Stripe, etc.</li> <li>Flexible Authentication: Pluggable providers support various auth strategies</li> <li>Type Safety: Full Pydantic model integration with proper typing</li> <li>Resource Agnostic: Generic client works with any API endpoint</li> <li>Context Awareness: Automatically adapts to Flask vs non-Flask environments</li> <li>Session Management: Proper HTTP client lifecycle management</li> <li>Extensible: Easy to add new credential providers or custom endpoints</li> </ol>"},{"location":"api/client/#future-considerations","title":"Future Considerations","text":"<ul> <li>Async Support: Could add async versions of all methods</li> <li>Caching: Could add response caching at the session level</li> <li>Retry Logic: Could add automatic retry with exponential backoff</li> <li>Rate Limiting: Could add client-side rate limiting</li> <li>Response Streaming: Could add support for streaming responses</li> </ul>"},{"location":"api/client/auth/","title":"Credential Providers","text":"<p>Credential providers are pluggable authentication strategies that supply credentials for API requests. They follow a simple interface and can be easily extended for different authentication scenarios.</p>"},{"location":"api/client/auth/#credentialprovider-interface","title":"CredentialProvider Interface","text":"<p>All credential providers implement the abstract <code>CredentialProvider</code> class:</p> <pre><code>class CredentialProvider(ABC):\n    \"\"\"Abstract base for credential providers.\"\"\"\n\n    @abstractmethod\n    def get_credentials(self) -&gt; Dict[str, str]:\n        \"\"\"Return headers to be added to requests.\"\"\"\n        pass\n\n    @abstractmethod\n    def is_available(self) -&gt; bool:\n        \"\"\"Check if credentials are available.\"\"\"\n        pass\n</code></pre>"},{"location":"api/client/auth/#built-in-providers","title":"Built-in Providers","text":""},{"location":"api/client/auth/#internalserviceprovider","title":"InternalServiceProvider","text":"<p>Used for service-to-service authentication within the ichrisbirch ecosystem.</p> <pre><code>from ichrisbirch.api.client.auth import InternalServiceProvider\n\nprovider = InternalServiceProvider(service_name=\"flask-frontend\")\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"Service flask-frontend &lt;service_key&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>Flask app calling FastAPI endpoints</li> <li>Background jobs accessing the API</li> <li>Internal microservice communication</li> </ul> <p>Configuration:</p> <ul> <li>Requires <code>AUTH_INTERNAL_SERVICE_KEY</code> environment variable</li> <li>Service name identifies the calling service</li> </ul>"},{"location":"api/client/auth/#usertokenprovider","title":"UserTokenProvider","text":"<p>Used for user-based authentication with JWT tokens or similar.</p> <pre><code>from ichrisbirch.api.client.auth import UserTokenProvider\n\nprovider = UserTokenProvider(user_id=\"user123\", app_id=\"web-app\")\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"Bearer &lt;user_token&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>API calls on behalf of a specific user</li> <li>Background tasks with user context</li> <li>Service calls that need user permissions</li> </ul> <p>Implementation Notes:</p> <ul> <li>Token retrieval should be implemented based on your token storage</li> <li>Could integrate with JWT libraries, database lookups, or external auth services</li> <li>May include token refresh logic</li> </ul>"},{"location":"api/client/auth/#flasksessionprovider","title":"FlaskSessionProvider","text":"<p>Uses Flask session data for authentication in web request contexts.</p> <pre><code>from ichrisbirch.api.client.auth import FlaskSessionProvider\n\nprovider = FlaskSessionProvider()\ncredentials = provider.get_credentials()\n# Returns: {\"Authorization\": \"User &lt;user_id&gt;\", \"X-App-ID\": \"&lt;app_id&gt;\"}\n</code></pre> <p>Use Cases:</p> <ul> <li>Web requests where user is already authenticated</li> <li>Form submissions and AJAX calls</li> <li>Any Flask route that needs to call the API</li> </ul> <p>Behavior:</p> <ul> <li>Only available within Flask request context</li> <li>Extracts user_id and app_id from session</li> <li>Returns empty dict if session data is missing</li> </ul>"},{"location":"api/client/auth/#default-provider-selection","title":"Default Provider Selection","text":"<p>The <code>APISession</code> automatically selects an appropriate provider when none is explicitly provided:</p> <pre><code>def _default_provider(self) -&gt; CredentialProvider:\n    \"\"\"Select appropriate provider based on context.\"\"\"\n    if has_request_context():\n        return FlaskSessionProvider()\n    else:\n        return InternalServiceProvider()\n</code></pre> <p>This provides context-aware authentication:</p> <ul> <li>In Flask requests: Uses session-based auth</li> <li>Outside Flask: Uses internal service auth</li> </ul>"},{"location":"api/client/auth/#custom-providers","title":"Custom Providers","text":"<p>You can create custom credential providers for specific authentication needs:</p> <pre><code>class APIKeyProvider(CredentialProvider):\n    \"\"\"API key based authentication.\"\"\"\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return {\"X-API-Key\": self.api_key}\n\n    def is_available(self) -&gt; bool:\n        return bool(self.api_key)\n\n# Usage\nprovider = APIKeyProvider(\"your-api-key\")\nclient = APIClient(credential_provider=provider)\n</code></pre>"},{"location":"api/client/auth/#provider-selection-guide","title":"Provider Selection Guide","text":"Context Recommended Provider Use Case Flask request with user session <code>FlaskSessionProvider</code> Web app user actions Flask request without session <code>InternalServiceProvider</code> Internal API calls Background job with user context <code>UserTokenProvider</code> User-specific tasks Background job system task <code>InternalServiceProvider</code> System maintenance External script/tool <code>APIKeyProvider</code> (custom) CLI tools, scripts Testing <code>InternalServiceProvider</code> Test automation"},{"location":"api/client/auth/#error-handling","title":"Error Handling","text":"<p>Credential providers follow a fail-fast approach:</p> <ul> <li><code>is_available()</code> returns <code>False</code> when credentials cannot be obtained</li> <li><code>get_credentials()</code> may raise exceptions for configuration errors</li> <li>No defensive defaults - missing configuration should fail clearly</li> </ul> <p>This ensures authentication problems are detected early rather than silently failing with unclear errors.</p>"},{"location":"api/client/auth/#security-considerations","title":"Security Considerations","text":"<ol> <li>Service Keys: Store in environment variables, not code</li> <li>User Tokens: Implement secure token storage and refresh logic</li> <li>Session Security: Ensure Flask sessions are properly secured</li> <li>Credential Rotation: Design providers to support key/token rotation</li> <li>Logging: Avoid logging credential values in plaintext</li> </ol>"},{"location":"api/client/auth/#testing","title":"Testing","text":"<p>Mock credential providers for testing:</p> <pre><code>class MockCredentialProvider(CredentialProvider):\n    def __init__(self, credentials: Dict[str, str]):\n        self._credentials = credentials\n\n    def get_credentials(self) -&gt; Dict[str, str]:\n        return self._credentials\n\n    def is_available(self) -&gt; bool:\n        return True\n\n# Test usage\nmock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\nclient = APIClient(credential_provider=mock_provider)\n</code></pre>"},{"location":"api/client/examples/","title":"API Client Usage Examples","text":"<p>This document provides practical examples of using the new API client for common scenarios in the ichrisbirch application.</p>"},{"location":"api/client/examples/#basic-setup","title":"Basic Setup","text":"<pre><code>from ichrisbirch.api.client import APIClient, internal_service_client, user_client\nfrom ichrisbirch.models import TaskModel, UserModel, ProjectModel\n</code></pre>"},{"location":"api/client/examples/#flask-web-application-examples","title":"Flask Web Application Examples","text":""},{"location":"api/client/examples/#user-dashboard","title":"User Dashboard","text":"<pre><code>@app.route('/dashboard')\n@login_required\ndef dashboard():\n    \"\"\"User dashboard showing their tasks and projects.\"\"\"\n    # Uses Flask session automatically\n    with APIClient() as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        projects_client = client.resource('projects', ProjectModel)\n\n        # Get user's active tasks\n        active_tasks = tasks_client.list(\n            assigned_to=session['user_id'],\n            status='active',\n            limit=10\n        )\n\n        # Get user's projects\n        projects = projects_client.list(\n            owner_id=session['user_id'],\n            include_task_counts=True\n        )\n\n        return render_template('dashboard.html',\n                             tasks=active_tasks,\n                             projects=projects)\n</code></pre>"},{"location":"api/client/examples/#task-management","title":"Task Management","text":"<pre><code>@app.route('/tasks/&lt;int:task_id&gt;/complete', methods=['POST'])\n@login_required\ndef complete_task(task_id):\n    \"\"\"Mark a task as completed.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        try:\n            # Use custom action for completion\n            result = tasks.action('complete', {\n                'task_id': task_id,\n                'completion_note': request.form.get('note', '')\n            })\n\n            flash('Task completed successfully', 'success')\n            return redirect(url_for('dashboard'))\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                flash('Task not found', 'error')\n            elif e.response.status_code == 403:\n                flash('Not authorized to complete this task', 'error')\n            else:\n                flash('Error completing task', 'error')\n            return redirect(url_for('dashboard'))\n</code></pre>"},{"location":"api/client/examples/#crud-operations","title":"CRUD Operations","text":"<pre><code>@app.route('/projects', methods=['GET', 'POST'])\n@login_required\ndef projects():\n    \"\"\"List projects or create new project.\"\"\"\n    with APIClient() as client:\n        projects_client = client.resource('projects', ProjectModel)\n\n        if request.method == 'POST':\n            # Create new project\n            project_data = {\n                'name': request.form['name'],\n                'description': request.form['description'],\n                'owner_id': session['user_id']\n            }\n\n            try:\n                new_project = projects_client.create(project_data)\n                flash(f'Project \"{new_project.name}\" created', 'success')\n                return redirect(url_for('projects'))\n\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == 422:\n                    errors = e.response.json()\n                    flash(f'Validation errors: {errors}', 'error')\n                else:\n                    flash('Error creating project', 'error')\n\n        # List projects\n        user_projects = projects_client.list(owner_id=session['user_id'])\n        return render_template('projects.html', projects=user_projects)\n</code></pre>"},{"location":"api/client/examples/#background-job-examples","title":"Background Job Examples","text":""},{"location":"api/client/examples/#data-processing-job","title":"Data Processing Job","text":"<pre><code>def process_daily_reports():\n    \"\"\"Background job to generate daily reports.\"\"\"\n    # Use internal service authentication\n    with internal_service_client('report-processor') as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        reports_client = client.resource('reports', ReportModel)\n\n        # Get completed tasks from yesterday\n        yesterday = datetime.now() - timedelta(days=1)\n        completed_tasks = tasks_client.list(\n            status='completed',\n            completed_after=yesterday.isoformat(),\n            limit=1000\n        )\n\n        # Generate report\n        report_data = {\n            'date': yesterday.date().isoformat(),\n            'total_completed': len(completed_tasks),\n            'completion_rate': calculate_completion_rate(completed_tasks),\n            'top_performers': get_top_performers(completed_tasks)\n        }\n\n        # Create report\n        report = reports_client.create(report_data)\n        logger.info(f\"Daily report created: {report.id}\")\n\n        # Send notifications\n        notify_managers(report)\n</code></pre>"},{"location":"api/client/examples/#bulk-operations","title":"Bulk Operations","text":"<pre><code>def archive_old_tasks():\n    \"\"\"Archive tasks older than 90 days.\"\"\"\n    with internal_service_client('maintenance') as client:\n        tasks_client = client.resource('tasks', TaskModel)\n\n        # Find old completed tasks\n        cutoff_date = datetime.now() - timedelta(days=90)\n        old_tasks = tasks_client.list(\n            status='completed',\n            completed_before=cutoff_date.isoformat(),\n            limit=500  # Process in batches\n        )\n\n        if old_tasks:\n            # Use bulk action\n            task_ids = [task.id for task in old_tasks]\n            result = tasks_client.action('bulk_archive', {\n                'task_ids': task_ids,\n                'archive_reason': 'automatic_cleanup'\n            })\n\n            logger.info(f\"Archived {result['archived_count']} old tasks\")\n</code></pre>"},{"location":"api/client/examples/#service-integration-examples","title":"Service Integration Examples","text":""},{"location":"api/client/examples/#external-api-integration","title":"External API Integration","text":"<pre><code>class ExternalSyncService:\n    \"\"\"Service to sync data with external API.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('external-sync')\n        self.tasks = self.client.resource('tasks', TaskModel)\n        self.users = self.client.resource('users', UserModel)\n\n    def sync_user_tasks(self, external_user_id: str):\n        \"\"\"Sync tasks for a user from external system.\"\"\"\n        try:\n            # Get external tasks\n            external_tasks = self.get_external_tasks(external_user_id)\n\n            # Find corresponding internal user\n            user = self.users.list(external_id=external_user_id)[0]\n\n            for ext_task in external_tasks:\n                # Check if task already exists\n                existing = self.tasks.list(external_id=ext_task['id'])\n\n                task_data = {\n                    'title': ext_task['title'],\n                    'description': ext_task['description'],\n                    'assigned_to': user.id,\n                    'external_id': ext_task['id'],\n                    'external_updated_at': ext_task['updated_at']\n                }\n\n                if existing:\n                    # Update existing task\n                    self.tasks.update(existing[0].id, task_data)\n                else:\n                    # Create new task\n                    self.tasks.create(task_data)\n\n        except Exception as e:\n            logger.error(f\"Error syncing tasks for user {external_user_id}: {e}\")\n            raise\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#notification-service","title":"Notification Service","text":"<pre><code>class NotificationService:\n    \"\"\"Service for sending notifications based on API events.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('notification-service')\n        self.notifications = self.client.resource('notifications', NotificationModel)\n        self.users = self.client.resource('users', UserModel)\n\n    def notify_task_assignment(self, task_id: int, assigned_user_id: str):\n        \"\"\"Send notification when task is assigned.\"\"\"\n        try:\n            # Get task details\n            tasks = self.client.resource('tasks', TaskModel)\n            task = tasks.get(task_id)\n\n            if not task:\n                logger.warning(f\"Task {task_id} not found for notification\")\n                return\n\n            # Get user details\n            user = self.users.get(assigned_user_id)\n\n            # Create notification\n            notification_data = {\n                'user_id': assigned_user_id,\n                'type': 'task_assignment',\n                'title': 'New Task Assigned',\n                'message': f'You have been assigned task: {task.title}',\n                'related_object_type': 'task',\n                'related_object_id': task_id,\n                'priority': task.priority\n            }\n\n            notification = self.notifications.create(notification_data)\n\n            # Send via email/SMS if needed\n            if user.notification_preferences.get('email_enabled'):\n                self.send_email_notification(user, notification)\n\n        except Exception as e:\n            logger.error(f\"Error sending task assignment notification: {e}\")\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#testing-examples","title":"Testing Examples","text":""},{"location":"api/client/examples/#unit-testing-with-mocks","title":"Unit Testing with Mocks","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom ichrisbirch.api.client import APIClient\nfrom ichrisbirch.api.client.auth import MockCredentialProvider\n\nclass TestTaskService:\n\n    @pytest.fixture\n    def mock_api_client(self):\n        \"\"\"Mock API client for testing.\"\"\"\n        provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\n        return APIClient(\n            base_url=\"http://test-api:8000\",\n            credential_provider=provider\n        )\n\n    def test_get_user_tasks(self, mock_api_client):\n        \"\"\"Test getting user tasks.\"\"\"\n        # Mock the HTTP response\n        with patch.object(mock_api_client.session.client, 'request') as mock_request:\n            mock_response = Mock()\n            mock_response.json.return_value = [\n                {'id': 1, 'title': 'Test Task', 'assigned_to': 'user123'},\n                {'id': 2, 'title': 'Another Task', 'assigned_to': 'user123'}\n            ]\n            mock_request.return_value = mock_response\n\n            # Test the service\n            tasks = mock_api_client.resource('tasks', TaskModel)\n            user_tasks = tasks.list(assigned_to='user123')\n\n            # Verify the request\n            mock_request.assert_called_once()\n            args, kwargs = mock_request.call_args\n            assert kwargs['method'] == 'GET'\n            assert 'tasks' in kwargs['url']\n            assert kwargs['params'] == {'assigned_to': 'user123'}\n</code></pre>"},{"location":"api/client/examples/#integration-testing","title":"Integration Testing","text":"<pre><code>@pytest.mark.integration\nclass TestAPIClientIntegration:\n    \"\"\"Integration tests with actual API.\"\"\"\n\n    @pytest.fixture\n    def api_client(self):\n        \"\"\"Real API client for integration tests.\"\"\"\n        return APIClient(base_url=os.environ['TEST_API_URL'])\n\n    def test_crud_operations(self, api_client):\n        \"\"\"Test full CRUD cycle.\"\"\"\n        with api_client:\n            tasks = api_client.resource('tasks', TaskModel)\n\n            # Create\n            task_data = {\n                'title': 'Integration Test Task',\n                'description': 'Created by integration test',\n                'priority': 'low'\n            }\n            created_task = tasks.create(task_data)\n            assert created_task.id is not None\n            assert created_task.title == task_data['title']\n\n            # Read\n            fetched_task = tasks.get(created_task.id)\n            assert fetched_task.title == created_task.title\n\n            # Update\n            updated_data = {'status': 'in_progress'}\n            updated_task = tasks.update(created_task.id, updated_data)\n            assert updated_task.status == 'in_progress'\n\n            # Delete\n            tasks.delete(created_task.id)\n\n            # Verify deletion\n            deleted_task = tasks.get(created_task.id)\n            assert deleted_task is None\n</code></pre>"},{"location":"api/client/examples/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/client/examples/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>def robust_task_processing(task_id: int):\n    \"\"\"Process task with comprehensive error handling.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        try:\n            # Get task\n            task = tasks.get(task_id)\n            if not task:\n                logger.warning(f\"Task {task_id} not found\")\n                return {'success': False, 'error': 'Task not found'}\n\n            # Process task\n            result = tasks.action('process', {'task_id': task_id})\n\n            return {'success': True, 'result': result}\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                error_msg = f\"Task {task_id} not found\"\n            elif e.response.status_code == 422:\n                validation_errors = e.response.json()\n                error_msg = f\"Validation errors: {validation_errors}\"\n            elif e.response.status_code == 403:\n                error_msg = \"Not authorized to process this task\"\n            else:\n                error_msg = f\"HTTP error {e.response.status_code}: {e.response.text}\"\n\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except httpx.ConnectError:\n            error_msg = \"Could not connect to API server\"\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except httpx.TimeoutException:\n            error_msg = \"Request timed out\"\n            logger.error(error_msg)\n            return {'success': False, 'error': error_msg}\n\n        except Exception as e:\n            error_msg = f\"Unexpected error: {str(e)}\"\n            logger.exception(error_msg)\n            return {'success': False, 'error': error_msg}\n</code></pre>"},{"location":"api/client/examples/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\nfrom typing import Callable, Any\n\ndef with_retry(func: Callable, max_attempts: int = 3, delay: float = 1.0) -&gt; Any:\n    \"\"\"Execute function with retry logic.\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            return func()\n        except (httpx.ConnectError, httpx.TimeoutException) as e:\n            if attempt == max_attempts - 1:\n                raise e\n\n            logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay}s...\")\n            time.sleep(delay)\n            delay *= 2  # Exponential backoff\n\ndef get_tasks_with_retry():\n    \"\"\"Get tasks with automatic retry.\"\"\"\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        return with_retry(lambda: tasks.list(status='active'))\n</code></pre>"},{"location":"api/client/examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/client/examples/#connection-reuse","title":"Connection Reuse","text":"<pre><code>class TaskProcessor:\n    \"\"\"Long-running task processor that reuses connections.\"\"\"\n\n    def __init__(self):\n        self.client = internal_service_client('task-processor')\n        self.tasks = self.client.resource('tasks', TaskModel)\n        self.notifications = self.client.resource('notifications', NotificationModel)\n\n    def process_batch(self, batch_size: int = 50):\n        \"\"\"Process tasks in batches.\"\"\"\n        offset = 0\n\n        while True:\n            # Get batch of pending tasks\n            pending_tasks = self.tasks.list(\n                status='pending',\n                limit=batch_size,\n                offset=offset\n            )\n\n            if not pending_tasks:\n                break\n\n            # Process each task\n            for task in pending_tasks:\n                try:\n                    self.process_single_task(task)\n                except Exception as e:\n                    logger.error(f\"Error processing task {task.id}: {e}\")\n\n            offset += batch_size\n\n    def process_single_task(self, task: TaskModel):\n        \"\"\"Process a single task.\"\"\"\n        # Update status\n        self.tasks.update(task.id, {'status': 'processing'})\n\n        # Do processing work\n        result = self.do_work(task)\n\n        # Update with result\n        self.tasks.update(task.id, {\n            'status': 'completed',\n            'result': result\n        })\n\n        # Send notification\n        self.notifications.create({\n            'user_id': task.assigned_to,\n            'type': 'task_completed',\n            'message': f'Task \"{task.title}\" completed'\n        })\n\n    def close(self):\n        \"\"\"Clean up resources.\"\"\"\n        self.client.close()\n</code></pre>"},{"location":"api/client/examples/#pagination-handling","title":"Pagination Handling","text":"<pre><code>def get_all_tasks() -&gt; List[TaskModel]:\n    \"\"\"Get all tasks using pagination.\"\"\"\n    all_tasks = []\n\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        page_size = 100\n        offset = 0\n\n        while True:\n            batch = tasks.list(limit=page_size, offset=offset)\n            if not batch:\n                break\n\n            all_tasks.extend(batch)\n            offset += page_size\n\n            # Avoid infinite loops\n            if len(batch) &lt; page_size:\n                break\n\n    return all_tasks\n</code></pre> <p>These examples demonstrate practical usage patterns for the new API client in various scenarios throughout the ichrisbirch application.</p>"},{"location":"api/client/migration/","title":"Migration Guide: QueryAPI to API Client","text":"<p>This guide helps you migrate from the existing <code>QueryAPI</code> class to the new session-based API client architecture.</p>"},{"location":"api/client/migration/#overview-of-changes","title":"Overview of Changes","text":"<p>The new API client provides:</p> <ul> <li>Session-based architecture following industry patterns (boto3, Stripe)</li> <li>Pluggable authentication with credential providers</li> <li>Generic resource pattern instead of specific methods</li> <li>Better type safety with full Pydantic integration</li> <li>Context-aware defaults for Flask vs non-Flask environments</li> </ul>"},{"location":"api/client/migration/#migration-strategy","title":"Migration Strategy","text":""},{"location":"api/client/migration/#phase-1-coexistence","title":"Phase 1: Coexistence","text":"<p>Both systems can coexist during migration:</p> <pre><code># Old QueryAPI (still available)\nfrom ichrisbirch.app.query_api import QueryAPI\nquery_api = QueryAPI(use_internal_auth=True)\n\n# New API Client\nfrom ichrisbirch.api.client import APIClient\nclient = APIClient()\n</code></pre>"},{"location":"api/client/migration/#phase-2-gradual-migration","title":"Phase 2: Gradual Migration","text":"<p>Migrate one endpoint at a time:</p> <pre><code># Before: QueryAPI\ndef get_tasks():\n    query_api = QueryAPI()\n    return query_api.get_generic(\"tasks\")\n\n# After: API Client\ndef get_tasks():\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list()\n</code></pre>"},{"location":"api/client/migration/#phase-3-complete-migration","title":"Phase 3: Complete Migration","text":"<p>Remove QueryAPI usage and update all code to use the new client.</p>"},{"location":"api/client/migration/#common-migration-patterns","title":"Common Migration Patterns","text":""},{"location":"api/client/migration/#basic-get-requests","title":"Basic GET Requests","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\ntasks = query_api.get_generic(\"tasks\")\ntask = query_api.get_generic(\"tasks\", resource_id=123)\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    tasks_client = client.resource('tasks', TaskModel)\n    tasks = tasks_client.list()\n    task = tasks_client.get(123)\n</code></pre>"},{"location":"api/client/migration/#post-actions","title":"POST Actions","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\nresult = query_api.post_action(\"tasks/bulk_complete\", {\n    'task_ids': [1, 2, 3]\n})\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    result = tasks.action('bulk_complete', {\n        'task_ids': [1, 2, 3]\n    })\n</code></pre>"},{"location":"api/client/migration/#custom-endpoints","title":"Custom Endpoints","text":"<p>Before (QueryAPI):</p> <pre><code>query_api = QueryAPI()\ndata = query_api.get_generic(\"custom/endpoint\")\nresult = query_api.post_action(\"custom/action\", {'data': 'value'})\n</code></pre> <p>After (API Client):</p> <pre><code>with APIClient() as client:\n    # Direct requests for truly custom endpoints\n    data = client.request('GET', '/custom/endpoint')\n    result = client.request('POST', '/custom/action', json={'data': 'value'})\n\n    # Or if it belongs to a resource\n    resource = client.resource('custom', CustomModel)\n    data = resource.custom_endpoint('GET', '/endpoint')\n    result = resource.custom_endpoint('POST', '/action', {'data': 'value'})\n</code></pre>"},{"location":"api/client/migration/#authentication-patterns","title":"Authentication Patterns","text":"<p>Before (QueryAPI):</p> <pre><code># Internal service\nquery_api = QueryAPI(use_internal_auth=True)\n\n# User context (automatic)\nquery_api = QueryAPI()  # Uses Flask session\n</code></pre> <p>After (API Client):</p> <pre><code># Internal service\nfrom ichrisbirch.api.client import internal_service_client\nclient = internal_service_client()\n\n# User context (automatic)\nfrom ichrisbirch.api.client import default_client\nclient = default_client()  # Context-aware\n\n# Or explicit Flask session\nfrom ichrisbirch.api.client import flask_session_client\nclient = flask_session_client()\n</code></pre>"},{"location":"api/client/migration/#specific-queryapi-method-migrations","title":"Specific QueryAPI Method Migrations","text":""},{"location":"api/client/migration/#get_generic","title":"get_generic()","text":"<p>QueryAPI Pattern:</p> <pre><code>def get_generic(self, endpoint: str, resource_id: int = None, **params) -&gt; Union[List[Dict], Dict, None]:\n    # Internal implementation\n    return response_data\n</code></pre> <p>API Client Equivalent:</p> <pre><code># For resource operations\ntasks = client.resource('tasks', TaskModel)\nall_tasks = tasks.list(**params)  # GET /tasks\nsingle_task = tasks.get(resource_id, **params)  # GET /tasks/{id}\n\n# For custom endpoints\nresponse = client.request('GET', f'/{endpoint}', params=params)\n</code></pre>"},{"location":"api/client/migration/#post_action","title":"post_action()","text":"<p>QueryAPI Pattern:</p> <pre><code>def post_action(self, endpoint: str, data: Dict) -&gt; Dict:\n    # Internal implementation\n    return response_data\n</code></pre> <p>API Client Equivalent:</p> <pre><code># For resource actions\nresource = client.resource('tasks', TaskModel)\nresult = resource.action('action_name', data)\n\n# For custom endpoints\nresult = client.request('POST', f'/{endpoint}', json=data)\n</code></pre>"},{"location":"api/client/migration/#internal-authentication","title":"Internal Authentication","text":"<p>QueryAPI Pattern:</p> <pre><code>query_api = QueryAPI(use_internal_auth=True)\n</code></pre> <p>API Client Equivalent:</p> <pre><code>from ichrisbirch.api.client import internal_service_client\nclient = internal_service_client(\"flask-frontend\")\n</code></pre>"},{"location":"api/client/migration/#file-by-file-migration-examples","title":"File-by-File Migration Examples","text":""},{"location":"api/client/migration/#flask-route-migration","title":"Flask Route Migration","text":"<p>Before:</p> <pre><code>@app.route('/tasks')\ndef get_tasks():\n    query_api = QueryAPI()\n    tasks = query_api.get_generic(\"tasks\", status=\"active\")\n    return render_template('tasks.html', tasks=tasks)\n</code></pre> <p>After:</p> <pre><code>@app.route('/tasks')\ndef get_tasks():\n    with APIClient() as client:\n        tasks_client = client.resource('tasks', TaskModel)\n        tasks = tasks_client.list(status=\"active\")\n        return render_template('tasks.html', tasks=tasks)\n</code></pre>"},{"location":"api/client/migration/#background-job-migration","title":"Background Job Migration","text":"<p>Before:</p> <pre><code>def process_pending_tasks():\n    query_api = QueryAPI(use_internal_auth=True)\n    pending = query_api.get_generic(\"tasks\", status=\"pending\")\n\n    for task in pending:\n        result = query_api.post_action(f\"tasks/{task['id']}/process\", {})\n        if result['success']:\n            query_api.post_action(f\"tasks/{task['id']}/complete\", {})\n</code></pre> <p>After:</p> <pre><code>def process_pending_tasks():\n    with internal_service_client(\"background-processor\") as client:\n        tasks = client.resource('tasks', TaskModel)\n        pending = tasks.list(status=\"pending\")\n\n        for task in pending:\n            result = tasks.action('process', task_id=task.id)\n            if result['success']:\n                tasks.action('complete', task_id=task.id)\n</code></pre>"},{"location":"api/client/migration/#service-class-migration","title":"Service Class Migration","text":"<p>Before:</p> <pre><code>class TaskService:\n    def __init__(self):\n        self.query_api = QueryAPI()\n\n    def get_user_tasks(self, user_id: str):\n        return self.query_api.get_generic(\"tasks\", assigned_to=user_id)\n\n    def complete_task(self, task_id: int):\n        return self.query_api.post_action(f\"tasks/{task_id}/complete\", {})\n</code></pre> <p>After:</p> <pre><code>class TaskService:\n    def __init__(self):\n        self.client = APIClient()\n        self.tasks = self.client.resource('tasks', TaskModel)\n\n    def get_user_tasks(self, user_id: str):\n        return self.tasks.list(assigned_to=user_id)\n\n    def complete_task(self, task_id: int):\n        return self.tasks.action('complete', task_id=task_id)\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/migration/#testing-migration","title":"Testing Migration","text":""},{"location":"api/client/migration/#mock-queryapi","title":"Mock QueryAPI","text":"<p>Before:</p> <pre><code>def test_get_tasks(monkeypatch):\n    def mock_get_generic(endpoint, **params):\n        return [{'id': 1, 'title': 'Test Task'}]\n\n    monkeypatch.setattr(QueryAPI, 'get_generic', mock_get_generic)\n    # Test code...\n</code></pre> <p>After:</p> <pre><code>def test_get_tasks():\n    mock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test\"})\n    with APIClient(credential_provider=mock_provider) as client:\n        # Use actual client with mock auth\n        pass\n\n# Or mock at HTTP level\n@responses.activate  \ndef test_get_tasks():\n    responses.add(responses.GET,\n                 \"http://api.test/tasks\",\n                 json=[{'id': 1, 'title': 'Test Task'}])\n\n    with APIClient(base_url=\"http://api.test\") as client:\n        tasks = client.resource('tasks', TaskModel)\n        result = tasks.list()\n</code></pre>"},{"location":"api/client/migration/#breaking-changes","title":"Breaking Changes","text":""},{"location":"api/client/migration/#method-names","title":"Method Names","text":"QueryAPI API Client <code>get_generic(endpoint)</code> <code>resource(name, model).list()</code> <code>get_generic(endpoint, resource_id=123)</code> <code>resource(name, model).get(123)</code> <code>post_action(endpoint, data)</code> <code>resource(name, model).action(name, data)</code> Custom methods <code>request(method, endpoint, **kwargs)</code>"},{"location":"api/client/migration/#return-types","title":"Return Types","text":"<ul> <li>QueryAPI: Returns raw dictionaries and lists</li> <li>API Client: Returns Pydantic model instances</li> </ul>"},{"location":"api/client/migration/#authentication","title":"Authentication","text":"<ul> <li>QueryAPI: Boolean flag <code>use_internal_auth</code></li> <li>API Client: Pluggable credential providers</li> </ul>"},{"location":"api/client/migration/#configuration","title":"Configuration","text":"<ul> <li>QueryAPI: Hardcoded settings and Flask coupling</li> <li>API Client: Configurable sessions with explicit dependencies</li> </ul>"},{"location":"api/client/migration/#rollback-strategy","title":"Rollback Strategy","text":"<p>If issues arise during migration:</p> <ol> <li>Keep QueryAPI Available: Don't remove QueryAPI until migration is complete</li> <li>Incremental Rollback: Roll back one endpoint at a time if needed</li> <li>Feature Flags: Use feature flags to toggle between old and new implementations</li> </ol> <pre><code>def get_tasks():\n    if settings.use_new_api_client:\n        with APIClient() as client:\n            return client.resource('tasks', TaskModel).list()\n    else:\n        query_api = QueryAPI()\n        return query_api.get_generic(\"tasks\")\n</code></pre>"},{"location":"api/client/migration/#migration-checklist","title":"Migration Checklist","text":""},{"location":"api/client/migration/#before-migration","title":"Before Migration","text":"<ul> <li> Understand current QueryAPI usage patterns</li> <li> Identify all endpoints and authentication requirements</li> <li> Set up Pydantic models for all resources</li> <li> Plan migration order (start with low-risk endpoints)</li> </ul>"},{"location":"api/client/migration/#during-migration","title":"During Migration","text":"<ul> <li> Migrate one endpoint at a time</li> <li> Test each migration thoroughly</li> <li> Update all related tests</li> <li> Update documentation and examples</li> <li> Monitor for performance issues</li> </ul>"},{"location":"api/client/migration/#after-migration","title":"After Migration","text":"<ul> <li> Remove QueryAPI imports and usage</li> <li> Clean up old authentication code</li> <li> Update CI/CD pipelines</li> <li> Train team on new patterns</li> <li> Update development guidelines</li> </ul>"},{"location":"api/client/migration/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Session Management: Remember to close clients or use context managers</li> <li>Authentication Context: Understand when Flask session vs internal auth is used</li> <li>Type Safety: Use proper Pydantic models for type checking</li> <li>Error Handling: New client may raise different exceptions</li> <li>URL Building: New client uses existing <code>utils.url_builder()</code> patterns</li> </ol>"},{"location":"api/client/migration/#support","title":"Support","text":"<p>For migration assistance:</p> <ol> <li>Documentation: Reference the full API client documentation</li> <li>Examples: See <code>examples.md</code> for common patterns</li> <li>Testing: Use mock credential providers for unit tests</li> <li>Performance: Monitor response times and connection usage</li> </ol>"},{"location":"api/client/resources/","title":"Resource Clients","text":"<p>Resource clients provide a generic interface for CRUD operations on API endpoints. They work with any Pydantic model and follow consistent patterns for data access.</p>"},{"location":"api/client/resources/#overview","title":"Overview","text":"<p>The <code>ResourceClient</code> class provides standardized operations for any API resource:</p> <pre><code>from ichrisbirch.api.client import APIClient\nfrom ichrisbirch.models import TaskModel\n\n# Get a resource client\nclient = APIClient()\ntasks = client.resource('tasks', TaskModel)\n\n# All resources support the same operations\ntask = tasks.get(123)\nall_tasks = tasks.list()\nnew_task = tasks.create({'title': 'New Task'})\nupdated_task = tasks.update(123, {'status': 'completed'})\ntasks.delete(123)\n</code></pre>"},{"location":"api/client/resources/#crud-operations","title":"CRUD Operations","text":""},{"location":"api/client/resources/#get-single-resource","title":"Get Single Resource","text":"<p>Retrieve a specific resource by ID:</p> <pre><code># Get task with ID 123\ntask = tasks.get(123)\n# Returns: TaskModel instance or None\n\n# Get with query parameters\ntask = tasks.get(123, include_details=True)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>GET</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Returns: Pydantic model instance or <code>None</code></li> </ul>"},{"location":"api/client/resources/#list-resources","title":"List Resources","text":"<p>Retrieve multiple resources with optional filtering:</p> <pre><code># Get all tasks\nall_tasks = tasks.list()\n# Returns: List[TaskModel]\n\n# Get with filters\nactive_tasks = tasks.list(status='active', limit=10)\nurgent_tasks = tasks.list(priority='high', assigned_to='user123')\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>GET</code></li> <li>URL: <code>/{resource_name}</code></li> <li>Query params: All keyword arguments become query parameters</li> <li>Returns: <code>List[ModelType]</code></li> </ul>"},{"location":"api/client/resources/#create-resource","title":"Create Resource","text":"<p>Create a new resource:</p> <pre><code># Create from dictionary\nnew_task = tasks.create({\n    'title': 'Complete documentation',\n    'description': 'Write comprehensive API docs',\n    'priority': 'high'\n})\n# Returns: TaskModel instance\n\n# Create from Pydantic model\ntask_data = TaskModel(title='Another task', priority='low')\nnew_task = tasks.create(task_data.model_dump())\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>POST</code></li> <li>URL: <code>/{resource_name}</code></li> <li>Body: JSON data</li> <li>Returns: Created model instance</li> </ul>"},{"location":"api/client/resources/#update-resource","title":"Update Resource","text":"<p>Update an existing resource:</p> <pre><code># Partial update\nupdated_task = tasks.update(123, {'status': 'completed'})\n# Returns: TaskModel instance\n\n# Full update\nupdated_task = tasks.update(123, {\n    'title': 'Updated title',\n    'description': 'Updated description',\n    'status': 'in_progress'\n})\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>PUT</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Body: JSON data (partial or full)</li> <li>Returns: Updated model instance</li> </ul>"},{"location":"api/client/resources/#delete-resource","title":"Delete Resource","text":"<p>Remove a resource:</p> <pre><code># Delete by ID\ntasks.delete(123)\n# Returns: None (or raises exception on error)\n\n# Delete with confirmation\ntasks.delete(123, confirm=True)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>DELETE</code></li> <li>URL: <code>/{resource_name}/{id}</code></li> <li>Returns: <code>None</code></li> </ul>"},{"location":"api/client/resources/#custom-actions","title":"Custom Actions","text":"<p>Resources support custom actions beyond CRUD operations:</p> <pre><code># POST action with data\nresult = tasks.action('bulk_complete', {\n    'task_ids': [1, 2, 3, 4],\n    'completion_note': 'Batch completed'\n})\n\n# GET action (no data)\nstats = tasks.action('statistics', method='GET')\n\n# Action with query parameters\nreport = tasks.action('export', method='GET', format='csv', date_range='last_week')\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: <code>POST</code> (default) or specified method</li> <li>URL: <code>/{resource_name}/{action}</code></li> <li>Body: JSON data (for POST actions)</li> <li>Query params: Additional keyword arguments</li> <li>Returns: Action-specific response</li> </ul>"},{"location":"api/client/resources/#custom-endpoints","title":"Custom Endpoints","text":"<p>For endpoints that don't follow standard resource patterns:</p> <pre><code># Custom endpoint relative to resource\nresult = tasks.custom_endpoint('GET', '/special-report')\n\n# Custom endpoint with data\nresult = tasks.custom_endpoint('POST', '/batch-operations', {\n    'operation': 'archive',\n    'filter': {'status': 'completed'}\n})\n\n# Custom endpoint with query parameters\nresult = tasks.custom_endpoint('GET', '/export', format='json', limit=100)\n</code></pre> <p>HTTP Details:</p> <ul> <li>Method: As specified</li> <li>URL: <code>/{resource_name}{endpoint}</code></li> <li>Body: JSON data (if provided)</li> <li>Query params: Additional keyword arguments</li> </ul>"},{"location":"api/client/resources/#type-safety","title":"Type Safety","text":"<p>Resource clients are fully typed with Pydantic models:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ichrisbirch.models import TaskModel, UserModel\n\n# Type hints work correctly\ntasks: ResourceClient[TaskModel] = client.resource('tasks', TaskModel)\nusers: ResourceClient[UserModel] = client.resource('users', UserModel)\n\n# Return types are properly inferred\ntask: TaskModel = tasks.get(123)  # Type: TaskModel | None\ntask_list: List[TaskModel] = tasks.list()  # Type: List[TaskModel]\n</code></pre>"},{"location":"api/client/resources/#error-handling","title":"Error Handling","text":"<p>Resource clients handle errors consistently:</p>"},{"location":"api/client/resources/#http-errors","title":"HTTP Errors","text":"<pre><code>try:\n    task = tasks.get(999)  # Non-existent ID\nexcept httpx.HTTPStatusError as e:\n    if e.response.status_code == 404:\n        print(\"Task not found\")\n    else:\n        print(f\"HTTP error: {e.response.status_code}\")\n</code></pre>"},{"location":"api/client/resources/#validation-errors","title":"Validation Errors","text":"<pre><code>try:\n    new_task = tasks.create({'invalid_field': 'value'})\nexcept httpx.HTTPStatusError as e:\n    if e.response.status_code == 422:\n        errors = e.response.json()\n        print(f\"Validation errors: {errors}\")\n</code></pre>"},{"location":"api/client/resources/#network-errors","title":"Network Errors","text":"<pre><code>try:\n    tasks.list()\nexcept httpx.ConnectError:\n    print(\"Could not connect to API\")\nexcept httpx.TimeoutException:\n    print(\"Request timed out\")\n</code></pre>"},{"location":"api/client/resources/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/client/resources/#resource-factory-pattern","title":"Resource Factory Pattern","text":"<p>Create resource clients as needed:</p> <pre><code>def get_user_tasks(user_id: str) -&gt; List[TaskModel]:\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list(assigned_to=user_id)\n</code></pre>"},{"location":"api/client/resources/#shared-resource-clients","title":"Shared Resource Clients","text":"<p>Reuse resource clients for multiple operations:</p> <pre><code>def process_tasks():\n    with APIClient() as client:\n        tasks = client.resource('tasks', TaskModel)\n\n        # Multiple operations with same client\n        pending = tasks.list(status='pending')\n        for task in pending:\n            result = tasks.action('process', {'task_id': task.id})\n            if result['success']:\n                tasks.update(task.id, {'status': 'completed'})\n</code></pre>"},{"location":"api/client/resources/#cross-resource-operations","title":"Cross-Resource Operations","text":"<p>Work with multiple resource types:</p> <pre><code>def assign_tasks_to_user(user_id: str, task_ids: List[int]):\n    with APIClient() as client:\n        users = client.resource('users', UserModel)\n        tasks = client.resource('tasks', TaskModel)\n\n        # Verify user exists\n        user = users.get(user_id)\n        if not user:\n            raise ValueError(f\"User {user_id} not found\")\n\n        # Assign tasks\n        for task_id in task_ids:\n            tasks.update(task_id, {'assigned_to': user_id})\n</code></pre>"},{"location":"api/client/resources/#model-integration","title":"Model Integration","text":"<p>Resource clients work seamlessly with Pydantic models:</p>"},{"location":"api/client/resources/#input-validation","title":"Input Validation","text":"<pre><code># Models validate input data\ntask_data = TaskModel(\n    title=\"New task\",\n    priority=\"high\",\n    due_date=datetime.now() + timedelta(days=7)\n)\n\n# Create using model data\nnew_task = tasks.create(task_data.model_dump())\n</code></pre>"},{"location":"api/client/resources/#output-parsing","title":"Output Parsing","text":"<pre><code># Responses are parsed into model instances\ntask = tasks.get(123)\n\n# Access typed attributes\nprint(task.title)  # String\nprint(task.created_at)  # datetime\nprint(task.priority)  # Enum value\n</code></pre>"},{"location":"api/client/resources/#relationship-loading","title":"Relationship Loading","text":"<pre><code># Load related data\ntask = tasks.get(123, include_assignee=True)\nprint(task.assignee.name)  # Related model data\n\n# Batch operations\ncompleted_tasks = tasks.list(status='completed', include_subtasks=True)\n</code></pre>"},{"location":"api/client/resources/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Session Reuse: Use the same APIClient for multiple operations</li> <li>Batch Operations: Use custom actions for bulk operations</li> <li>Filtering: Use query parameters to reduce data transfer</li> <li>Pagination: Handle large result sets appropriately</li> </ol> <pre><code># Efficient batch processing\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n\n    # Process in batches\n    offset = 0\n    batch_size = 100\n\n    while True:\n        batch = tasks.list(limit=batch_size, offset=offset)\n        if not batch:\n            break\n\n        # Process batch\n        for task in batch:\n            process_task(task)\n\n        offset += batch_size\n</code></pre>"},{"location":"api/client/resources/#best-practices","title":"Best Practices","text":"<ol> <li>Use Type Hints: Always specify the Pydantic model type</li> <li>Handle Errors: Catch and handle HTTP and network errors appropriately</li> <li>Session Management: Use context managers for proper cleanup</li> <li>Resource Naming: Use consistent naming that matches API endpoints</li> <li>Data Validation: Let Pydantic models handle input validation</li> <li>Custom Actions: Use actions for operations that don't fit CRUD patterns</li> </ol>"},{"location":"api/client/session/","title":"Session Management","text":"<p>The <code>APISession</code> class manages HTTP client lifecycle, authentication state, and request/response handling. It provides a persistent, configured connection to the API server.</p>"},{"location":"api/client/session/#core-concepts","title":"Core Concepts","text":""},{"location":"api/client/session/#session-lifecycle","title":"Session Lifecycle","text":"<p>Sessions manage the underlying HTTP client and provide consistent configuration across multiple requests:</p> <pre><code>from ichrisbirch.api.client import APIClient\n\n# Session is created automatically\nclient = APIClient()\n\n# Session is reused for all requests\ntasks = client.resource('tasks', TaskModel)\nusers = client.resource('users', UserModel)\n\n# Properly close when done\nclient.close()\n\n# Or use context manager\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    # Session automatically closed\n</code></pre>"},{"location":"api/client/session/#configuration","title":"Configuration","text":"<p>Sessions are configured with:</p> <ul> <li>Base URL: Default API server endpoint</li> <li>Credential Provider: Authentication strategy</li> <li>Default Headers: Headers added to all requests</li> <li>HTTP Client: Persistent connection pool</li> </ul> <pre><code># Custom configuration\nclient = APIClient(\n    base_url=\"https://api.example.com\",\n    credential_provider=custom_provider,\n    default_headers={\"User-Agent\": \"MyApp/1.0\"}\n)\n</code></pre>"},{"location":"api/client/session/#default-behavior","title":"Default Behavior","text":""},{"location":"api/client/session/#automatic-base-url-detection","title":"Automatic Base URL Detection","text":"<p>When no base URL is provided, the session uses <code>settings.api_url</code>:</p> <pre><code># Uses settings.api_url automatically\nclient = APIClient()\n\n# Explicit base URL override\nclient = APIClient(base_url=\"https://custom-api.com\")\n</code></pre>"},{"location":"api/client/session/#context-aware-authentication","title":"Context-Aware Authentication","text":"<p>Sessions automatically select appropriate authentication based on context:</p> <pre><code>def _default_provider(self) -&gt; CredentialProvider:\n    \"\"\"Select appropriate provider based on context.\"\"\"\n    if has_request_context():\n        return FlaskSessionProvider()\n    else:\n        return InternalServiceProvider()\n</code></pre> <p>In Flask Request Context:</p> <ul> <li>Uses <code>FlaskSessionProvider</code></li> <li>Extracts user credentials from Flask session</li> <li>Suitable for web requests</li> </ul> <p>Outside Flask Context:</p> <ul> <li>Uses <code>InternalServiceProvider</code></li> <li>Uses service-to-service authentication</li> <li>Suitable for background jobs, scripts</li> </ul>"},{"location":"api/client/session/#request-handling","title":"Request Handling","text":""},{"location":"api/client/session/#url-building","title":"URL Building","text":"<p>Sessions handle URL construction using the existing <code>utils.url_builder()</code> function:</p> <pre><code># Input: endpoint = \"/tasks/123\"\n# Base URL: \"https://api.example.com\"\n# Result: \"https://api.example.com/tasks/123/\"\n\nurl = utils.url_builder(self.base_url, endpoint)\n</code></pre> <p>This preserves the existing URL building patterns and ensures consistent path handling.</p>"},{"location":"api/client/session/#header-management","title":"Header Management","text":"<p>Sessions merge headers from multiple sources:</p> <ol> <li>Default headers (from session configuration)</li> <li>Authentication headers (from credential provider)</li> <li>Request-specific headers (from method call)</li> </ol> <pre><code># Session default headers\nsession = APISession(default_headers={\"User-Agent\": \"MyApp/1.0\"})\n\n# Credential provider headers\n# {\"Authorization\": \"Bearer token\"}\n\n# Request headers\nsession.request(\"GET\", \"/tasks\", headers={\"Accept\": \"application/json\"})\n\n# Final merged headers:\n# {\n#   \"User-Agent\": \"MyApp/1.0\",\n#   \"Authorization\": \"Bearer token\",\n#   \"Accept\": \"application/json\"\n# }\n</code></pre>"},{"location":"api/client/session/#http-client-management","title":"HTTP Client Management","text":"<p>Sessions use httpx for HTTP requests with these features:</p> <ul> <li>Connection pooling: Reuses connections for better performance</li> <li>Timeout handling: Configurable request timeouts</li> <li>Error handling: Proper HTTP error responses</li> <li>JSON serialization: Automatic JSON encoding/decoding</li> </ul> <pre><code># The session manages httpx.Client lifecycle\nclass APISession:\n    def __init__(self, ...):\n        self.client = httpx.Client()\n\n    def request(self, method: str, endpoint: str, **kwargs) -&gt; Any:\n        response = self.client.request(method, url, **merged_kwargs)\n        return response.json()\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/session/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/client/session/#long-running-sessions","title":"Long-Running Sessions","text":"<p>For applications that make many API calls, reuse the session:</p> <pre><code># Good: Reuse session\nclient = APIClient()\nfor item in items:\n    result = client.resource('tasks', TaskModel).create(item)\nclient.close()\n\n# Better: Use context manager\nwith APIClient() as client:\n    tasks = client.resource('tasks', TaskModel)\n    for item in items:\n        result = tasks.create(item)\n</code></pre>"},{"location":"api/client/session/#short-lived-sessions","title":"Short-Lived Sessions","text":"<p>For one-off requests, sessions can be created and discarded:</p> <pre><code># Quick operations\ndef get_user(user_id: str) -&gt; UserModel:\n    with APIClient() as client:\n        return client.resource('users', UserModel).get(user_id)\n</code></pre>"},{"location":"api/client/session/#custom-authentication","title":"Custom Authentication","text":"<p>Override default authentication for specific needs:</p> <pre><code># Service authentication\nclient = APIClient(credential_provider=InternalServiceProvider(\"background-job\"))\n\n# User authentication  \nclient = APIClient(credential_provider=UserTokenProvider(\"user123\"))\n\n# Custom authentication\nclient = APIClient(credential_provider=CustomProvider(...))\n</code></pre>"},{"location":"api/client/session/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api/client/session/#development-environment","title":"Development Environment","text":"<pre><code># Local development with debug headers\nclient = APIClient(\n    base_url=\"http://localhost:8000\",\n    default_headers={\n        \"X-Debug\": \"true\",\n        \"User-Agent\": \"Development-Client\"\n    }\n)\n</code></pre>"},{"location":"api/client/session/#production-environment","title":"Production Environment","text":"<pre><code># Production with proper timeouts and headers\nclient = APIClient(\n    default_headers={\n        \"User-Agent\": \"ichrisbirch-frontend/1.0\",\n        \"X-Request-Source\": \"flask-app\"\n    }\n)\n</code></pre>"},{"location":"api/client/session/#testing-environment","title":"Testing Environment","text":"<pre><code># Testing with mock authentication\nmock_provider = MockCredentialProvider({\"Authorization\": \"Bearer test-token\"})\nclient = APIClient(\n    base_url=\"http://test-api:8000\",\n    credential_provider=mock_provider\n)\n</code></pre>"},{"location":"api/client/session/#error-handling","title":"Error Handling","text":"<p>Sessions handle errors at multiple levels:</p>"},{"location":"api/client/session/#http-errors","title":"HTTP Errors","text":"<pre><code>try:\n    response = session.request(\"GET\", \"/invalid-endpoint\")\nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP {e.response.status_code}: {e.response.text}\")\n</code></pre>"},{"location":"api/client/session/#connection-errors","title":"Connection Errors","text":"<pre><code>try:\n    response = session.request(\"GET\", \"/tasks\")\nexcept httpx.ConnectError:\n    print(\"Could not connect to API server\")\n</code></pre>"},{"location":"api/client/session/#authentication-errors","title":"Authentication Errors","text":"<pre><code># Credential provider errors\nif not session.credential_provider.is_available():\n    raise AuthenticationError(\"No credentials available\")\n</code></pre>"},{"location":"api/client/session/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Connection Reuse: Sessions maintain persistent connections</li> <li>Request Pooling: httpx handles connection pooling automatically</li> <li>Memory Management: Close sessions when done to free resources</li> <li>Concurrent Requests: Sessions are not thread-safe; use separate sessions per thread</li> </ol>"},{"location":"api/client/session/#best-practices","title":"Best Practices","text":"<ol> <li>Use Context Managers: Always use <code>with APIClient() as client:</code> when possible</li> <li>Session Reuse: Don't create new clients for each request</li> <li>Proper Cleanup: Call <code>close()</code> or use context managers</li> <li>Error Handling: Handle both HTTP and connection errors</li> <li>Configuration: Set base_url and headers at session level, not per request</li> </ol>"},{"location":"api/client/summary/","title":"API Client Implementation Summary","text":"<p>This document summarizes the new API client architecture implementation and provides quick reference for developers.</p>"},{"location":"api/client/summary/#implementation-complete","title":"Implementation Complete \u2705","text":"<p>The new API client has been fully implemented with:</p> <ul> <li>4 core modules in <code>ichrisbirch/api/client/</code></li> <li>5 comprehensive documentation files in <code>docs/</code></li> <li>Full type safety with Pydantic model integration</li> <li>Industry-standard patterns following boto3/Stripe architecture</li> <li>Zero import errors - all dependencies resolved</li> <li>Backward compatibility - QueryAPI remains functional during migration</li> </ul>"},{"location":"api/client/summary/#quick-reference","title":"Quick Reference","text":""},{"location":"api/client/summary/#factory-functions","title":"Factory Functions","text":"<pre><code>from ichrisbirch.api.client import (\n    APIClient,              # Main client class\n    default_client,         # Context-aware (recommended)\n    internal_service_client,# Service-to-service auth\n    user_client,           # User token auth  \n    flask_session_client   # Flask session auth\n)\n</code></pre>"},{"location":"api/client/summary/#basic-usage-pattern","title":"Basic Usage Pattern","text":"<pre><code># Context-aware client (detects Flask vs non-Flask automatically)\nwith default_client() as client:\n    tasks = client.resource('tasks', TaskModel)\n\n    # CRUD operations\n    task = tasks.get(123)\n    all_tasks = tasks.list(status='active')\n    new_task = tasks.create({'title': 'New Task'})\n    updated_task = tasks.update(123, {'status': 'completed'})\n    tasks.delete(123)\n\n    # Custom actions\n    result = tasks.action('bulk_complete', {'task_ids': [1, 2, 3]})\n</code></pre>"},{"location":"api/client/summary/#file-structure","title":"File Structure","text":"<pre><code>ichrisbirch/api/client/\n\u251c\u2500\u2500 __init__.py          # Public API exports\n\u251c\u2500\u2500 auth.py              # Credential provider abstractions\n\u251c\u2500\u2500 session.py           # Session management and HTTP client\n\u251c\u2500\u2500 resource.py          # Generic resource client with CRUD\n\u2514\u2500\u2500 api.py               # Main APIClient with factory functions\n\ndocs/api/client/\n\u251c\u2500\u2500 index.md           # Main architecture overview\n\u251c\u2500\u2500 auth.md            # Credential providers guide\n\u251c\u2500\u2500 session.md         # Session management details\n\u251c\u2500\u2500 resources.md       # Resource client operations\n\u251c\u2500\u2500 migration.md       # QueryAPI migration guide\n\u251c\u2500\u2500 examples.md        # Practical usage examples\n\u2514\u2500\u2500 summary.md         # Implementation summary\n</code></pre>"},{"location":"api/client/summary/#key-features","title":"Key Features","text":""},{"location":"api/client/summary/#pluggable-authentication","title":"\ud83d\udd10 Pluggable Authentication","text":"<ul> <li>InternalServiceProvider: Service-to-service with API keys</li> <li>UserTokenProvider: User-based JWT tokens</li> <li>FlaskSessionProvider: Flask session integration</li> <li>Custom providers: Easy to extend for new auth methods</li> </ul>"},{"location":"api/client/summary/#context-aware-defaults","title":"\ud83c\udfaf Context-Aware Defaults","text":"<ul> <li>Flask request context: Automatically uses session auth</li> <li>Background jobs: Automatically uses internal service auth</li> <li>No defensive defaults: Fails fast on misconfiguration</li> </ul>"},{"location":"api/client/summary/#generic-resource-pattern","title":"\ud83d\udd04 Generic Resource Pattern","text":"<ul> <li>Single interface: All resources use same CRUD methods</li> <li>Type-safe: Full Pydantic model integration</li> <li>Extensible: Custom actions and endpoints supported</li> </ul>"},{"location":"api/client/summary/#session-management","title":"\u26a1 Session Management","text":"<ul> <li>Connection pooling: Reuses HTTP connections</li> <li>Lifecycle management: Proper cleanup with context managers  </li> <li>Configuration: Centralized base URL and headers</li> </ul>"},{"location":"api/client/summary/#architecture-benefits","title":"Architecture Benefits","text":"Aspect QueryAPI (Old) API Client (New) Pattern Custom implementation Industry standard (boto3-style) Authentication Boolean flag Pluggable providers Type Safety Raw dictionaries Pydantic models Resource Access Specific methods Generic pattern Session Management None Full lifecycle management Context Awareness Flask coupling Clean abstraction Testing Difficult to mock Easy mocking/testing Extensibility Hard to extend Easy to extend"},{"location":"api/client/summary/#migration-status","title":"Migration Status","text":"<ul> <li>\u2705 QueryAPI Enhanced: Added <code>use_internal_auth</code> option, preserved all functionality</li> <li>\u2705 New Architecture: Complete implementation with all features</li> <li>\u2705 Documentation: Comprehensive guides and examples</li> <li>\u2705 Error Handling: No import or syntax errors</li> <li>\ud83d\udfe1 Migration Pending: QueryAPI still in use, migration can begin</li> <li>\ud83d\udfe1 Testing: Needs integration testing with real API endpoints</li> </ul>"},{"location":"api/client/summary/#next-steps","title":"Next Steps","text":""},{"location":"api/client/summary/#for-immediate-use","title":"For Immediate Use","text":"<ol> <li>Import the new client: <code>from ichrisbirch.api.client import default_client</code></li> <li>Use context manager: <code>with default_client() as client:</code></li> <li>Create resource clients: <code>tasks = client.resource('tasks', TaskModel)</code></li> <li>Perform operations: <code>task = tasks.get(123)</code></li> </ol>"},{"location":"api/client/summary/#for-migration","title":"For Migration","text":"<ol> <li>Start Small: Migrate one endpoint at a time</li> <li>Test Thoroughly: Use integration tests with real API</li> <li>Monitor Performance: Compare response times</li> <li>Update Gradually: Keep QueryAPI during transition</li> </ol>"},{"location":"api/client/summary/#for-development","title":"For Development","text":"<ol> <li>Follow Patterns: Use the documented examples</li> <li>Handle Errors: Use comprehensive error handling</li> <li>Manage Sessions: Always use context managers</li> <li>Type Hints: Specify Pydantic models for resources</li> </ol>"},{"location":"api/client/summary/#configuration-required","title":"Configuration Required","text":""},{"location":"api/client/summary/#environment-variables","title":"Environment Variables","text":"<pre><code># Required for internal service authentication\nAUTH_INTERNAL_SERVICE_KEY=your_service_key_here\n\n# API base URL (if not using default)\nAPI_URL=https://your-api-server.com\n</code></pre>"},{"location":"api/client/summary/#pydantic-models","title":"Pydantic Models","text":"<p>Ensure all resources have corresponding Pydantic models:</p> <pre><code># Example model structure\nclass TaskModel(BaseModel):\n    id: Optional[int] = None\n    title: str\n    description: Optional[str] = None\n    status: str = 'pending'\n    assigned_to: Optional[str] = None\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api/client/summary/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/client/summary/#best-practices","title":"Best Practices","text":"<ul> <li>Reuse clients: Don't create new clients for each request</li> <li>Use context managers: Ensure proper cleanup</li> <li>Batch operations: Use custom actions for bulk operations</li> <li>Connection limits: Monitor concurrent connection usage</li> </ul>"},{"location":"api/client/summary/#monitoring-points","title":"Monitoring Points","text":"<ul> <li>Response times: Compare old vs new client performance</li> <li>Connection usage: Monitor HTTP connection pool</li> <li>Memory usage: Sessions should be properly closed</li> <li>Error rates: Track authentication and HTTP errors</li> </ul>"},{"location":"api/client/summary/#support-resources","title":"Support Resources","text":""},{"location":"api/client/summary/#documentation","title":"Documentation","text":"<ul> <li><code>docs/api/client/index.md</code> - Architecture overview</li> <li><code>docs/api/client/examples.md</code> - Practical examples</li> <li><code>docs/api/client/migration.md</code> - Migration guide</li> </ul>"},{"location":"api/client/summary/#code-references","title":"Code References","text":"<ul> <li><code>ichrisbirch/api/client/</code> - Implementation</li> <li><code>ichrisbirch/app/query_api.py</code> - Enhanced QueryAPI for comparison</li> <li><code>tests/</code> - Example test patterns (to be added)</li> </ul>"},{"location":"api/client/summary/#common-patterns","title":"Common Patterns","text":"<pre><code># Flask route\n@app.route('/tasks')\ndef get_tasks():\n    with default_client() as client:\n        tasks = client.resource('tasks', TaskModel)\n        return tasks.list(status='active')\n\n# Background job\ndef process_tasks():\n    with internal_service_client('processor') as client:\n        tasks = client.resource('tasks', TaskModel)\n        pending = tasks.list(status='pending')\n        # Process tasks...\n\n# Service class\nclass TaskService:\n    def __init__(self):\n        self.client = default_client()\n        self.tasks = self.client.resource('tasks', TaskModel)\n\n    def close(self):\n        self.client.close()\n</code></pre>"},{"location":"api/client/summary/#success-criteria","title":"Success Criteria","text":"<p>The implementation is considered successful when:</p> <ul> <li>\u2705 Zero Import Errors: All modules load without issues</li> <li>\u2705 Complete Documentation: All usage patterns documented</li> <li>\u2705 Type Safety: Full Pydantic integration working</li> <li>\u2705 Industry Patterns: Follows established client libraries</li> <li>\ud83c\udfaf Migration Ready: QueryAPI can be gradually replaced</li> <li>\ud83c\udfaf Performance: Equal or better than existing QueryAPI</li> <li>\ud83c\udfaf Adoption: Team comfortable with new patterns</li> </ul> <p>Status: \u2705 Implementation Complete - Ready for Migration Last Updated: [Current Date] Maintainer: Engineering Team</p>"},{"location":"devops/","title":"DevOps","text":""},{"location":"devops/#scripts-that-run-periodically","title":"Scripts that Run periodically","text":"<p><code>scripts/postgres-snapshot-to-s3.sh</code> <code>scripts/create-project-stats.sh</code></p>"},{"location":"devops/#supervisor","title":"Supervisor","text":""},{"location":"devops/#nginx","title":"NGINX","text":""},{"location":"devops/#new-server-setup","title":"New Server Setup","text":""},{"location":"devops/new_database/","title":"Setup a New Postgres Database","text":""},{"location":"devops/new_database/#schemas","title":"Schemas","text":"<p>SQLAlchemy cannot create the schemas, neither can alembic, have to create them manually first time <code>create-schemas.py</code> to add the schemas</p>"},{"location":"devops/new_database/#alembic","title":"Alembic","text":"<p>Run in <code>ichrisbirch</code></p> <p>Create the initial tables from the SQLAlchemy models (purpose of --autogenerate) <code>alembic revision --autogenerate -m 'init_tables'</code></p> <p>Run the upgrade to actually create the tables <code>alembic upgrade head</code></p>"},{"location":"devops/new_server/","title":"Setup a New Server","text":""},{"location":"devops/new_server/#installs","title":"Installs","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n\n# base installs\nsudo apt install curl git git-secret -y\n\n# NOTE: Install the postgresql-client version that matches the database, this is for pg_dump backups with the scheduler.\nsudo apt install postgresql-client-16 tmux tldr supervisor nginx neovim pipx -y\n\n# for pyenv\nsudo apt install build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev -y\n\n# for building psycopg from source\nsudo apt install python3-dev libpq-dev -y\n\n# make sure\npipx ensurepath\n\n# Install pyenv\ncurl https://pyenv.run | bash\n\necho 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc\necho 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\n\nexec $SHELL\n\n# Install python\npyenv install 3.12\npyenv global 3.12\n\n# Install poetry making sure to use pyenv python\npipx install --python $(/home/ubuntu/.pyenv/bin/pyenv which python) poetry\n\nsudo chown ubuntu /var/www\n\n##### AT THIS POINT THE AMI SHOULD BE MADE #####\n\n# Clone project\ngit clone https://github.com/datapointchris/ichrisbirch /var/www/ichrisbirch\ncd /var/www/ichrisbirch\n\n# REFER to https://docs.ichrisbirch.com/git_secret/ to get gpg key for git-secret\n\n# Install project\npoetry config virtualenvs.in-project true\n\n# Make log files for project\n./scripts/make_log_files.sh\n\n# Set up nginx and supervisor\nsudo rm /etc/nginx/sites-enabled/default\n\ncd deploy\n./deploy-nginx.sh\n\n./deploy-supervisor.sh\n</code></pre> <p>Change the elastic IP to point to the new server (if only using one server and not load balancer).</p>"},{"location":"devops/nginx/","title":"NGINX","text":""},{"location":"devops/nginx/#how-to-deploy","title":"How to Deploy","text":""},{"location":"devops/pg_cron/","title":"pg_cron","text":"<p>Location: <code>/scripts/sql/pg_cron_setup.sql</code></p> <ol> <li>pg_cron must be added to 'shared_preload_libraries'</li> <li>Reboot required</li> <li>pg_cron runs in the default postgres database, then jobs can be moved to specific databases</li> <li>For AWS RDS: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL_pg_cron.html</li> </ol>"},{"location":"devops/pg_cron/#basic-instructions","title":"Basic Instructions","text":"<ol> <li>Create a function / procedure to run</li> <li>Schedule it with a name</li> <li>Set the database name for the job to the correct db</li> <li>Check that the job details show it has run successfully</li> </ol> <p>Note</p> <p><code>pg_cron</code> is not being used anymore, in favor of <code>APScheduler</code> in the <code>ichrisbirch/scheduler</code> directory.</p>"},{"location":"devops/supervisor/","title":"Supervisor","text":""},{"location":"devops/supervisor/#how-to-deploy","title":"How to Deploy","text":""},{"location":"docker/","title":"Docker Documentation","text":"<p>This section contains comprehensive documentation for Docker containerization of the iChrisBirch application.</p>"},{"location":"docker/#overview","title":"Overview","text":"<p>The iChrisBirch application runs in a containerized environment using Docker and Docker Compose. This approach provides consistent development, testing, and production environments while simplifying deployment and scaling.</p>"},{"location":"docker/#architecture","title":"Architecture","text":"<p>The application consists of multiple services orchestrated through Docker Compose:</p> <ul> <li>API Service: FastAPI backend service handling REST API requests</li> <li>App Service: Flask frontend service serving web interface</li> <li>Scheduler Service: Background job processing service</li> <li>PostgreSQL: Database service for persistent storage</li> <li>Redis: Caching and session storage service</li> </ul>"},{"location":"docker/#documentation-structure","title":"Documentation Structure","text":""},{"location":"docker/#core-documentation","title":"Core Documentation","text":"<ul> <li>Docker Guide: Complete Docker setup and usage guide</li> <li>Docker Compose: Service orchestration and configuration</li> <li>Quick Reference: Essential commands and troubleshooting</li> </ul>"},{"location":"docker/#key-features","title":"Key Features","text":"<ul> <li>Multi-stage builds: Optimized container images for different environments</li> <li>Environment-based configuration: Separate configs for dev/test/prod</li> <li>Health checks: Automatic service health monitoring</li> <li>Volume management: Persistent data storage and development volumes</li> <li>Network isolation: Secure service communication</li> <li>Dependency management: Proper service startup ordering</li> </ul>"},{"location":"docker/#quick-start","title":"Quick Start","text":""},{"location":"docker/#development-environment","title":"Development Environment","text":"<pre><code># Start all services\ndocker-compose -f docker-compose.dev.yml up -d\n\n# View logs\ndocker-compose -f docker-compose.dev.yml logs -f\n\n# Stop services\ndocker-compose -f docker-compose.dev.yml down\n</code></pre>"},{"location":"docker/#testing-environment","title":"Testing Environment","text":"<pre><code># Start test environment\ndocker-compose -f docker-compose.test.yml up -d\n\n# Run tests\ndocker-compose -f docker-compose.test.yml exec api pytest\n\n# Cleanup\ndocker-compose -f docker-compose.test.yml down -v\n</code></pre>"},{"location":"docker/#production-environment","title":"Production Environment","text":"<pre><code># Deploy to production\ndocker-compose -f docker-compose.prod.yml up -d\n\n# Monitor services\ndocker-compose -f docker-compose.prod.yml ps\n\n# Update application\ndocker-compose -f docker-compose.prod.yml pull\ndocker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"docker/#environment-configuration","title":"Environment Configuration","text":"<p>Each environment uses dedicated configuration files:</p> <ul> <li>Development: <code>.dev.env</code> - Debug enabled, development database</li> <li>Testing: <code>.test.env</code> - Test database, isolated environment</li> <li>Production: <code>.prod.env</code> - Production database, optimized settings</li> </ul>"},{"location":"docker/#service-management","title":"Service Management","text":""},{"location":"docker/#individual-service-operations","title":"Individual Service Operations","text":"<pre><code># Start specific service\ndocker-compose -f docker-compose.dev.yml up api\n\n# View service logs\ndocker-compose -f docker-compose.dev.yml logs -f app\n\n# Execute commands in service\ndocker-compose -f docker-compose.dev.yml exec api bash\n\n# Rebuild service\ndocker-compose -f docker-compose.dev.yml build api\n</code></pre>"},{"location":"docker/#database-operations","title":"Database Operations","text":"<pre><code># Run database migrations\ndocker-compose -f docker-compose.dev.yml exec api alembic upgrade head\n\n# Access database\ndocker-compose -f docker-compose.dev.yml exec postgres psql -U postgres -d ichrisbirch\n\n# Backup database\ndocker-compose -f docker-compose.dev.yml exec postgres pg_dump -U postgres ichrisbirch &gt; backup.sql\n</code></pre>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker/#common-issues","title":"Common Issues","text":"<ol> <li>Port conflicts: Ensure ports 5000, 8000, 5432, 6379 are available</li> <li>Permission issues: Check file permissions in mounted volumes</li> <li>Service dependencies: Verify services start in correct order</li> <li>Environment variables: Confirm all required env vars are set</li> </ol>"},{"location":"docker/#health-checks","title":"Health Checks","text":"<p>All services include health checks that can be monitored:</p> <pre><code># Check service health\ndocker-compose -f docker-compose.dev.yml ps\n\n# View detailed health status\ndocker inspect --format='{{json .State.Health}}' container_name\n</code></pre>"},{"location":"docker/#security-considerations","title":"Security Considerations","text":"<ul> <li>Non-root user: All services run as non-root user</li> <li>Network isolation: Services communicate through Docker networks</li> <li>Secret management: Sensitive data managed through environment variables</li> <li>Image security: Regular base image updates and security scanning</li> </ul>"},{"location":"docker/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Multi-stage builds: Reduced image sizes</li> <li>Layer caching: Optimized Dockerfile for build performance</li> <li>Resource limits: Configured memory and CPU limits</li> <li>Volume optimization: Efficient data persistence strategies</li> </ul>"},{"location":"docker/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration: Application configuration management</li> <li>Testing: Testing strategies and setup</li> <li>DevOps: Deployment and infrastructure</li> <li>Troubleshooting: Common issues and solutions</li> </ul>"},{"location":"docker/docker-compose/","title":"Docker Compose Architecture","text":"<p>This document details how Docker Compose orchestrates the ichrisbirch application services across different environments.</p>"},{"location":"docker/docker-compose/#overview","title":"Overview","text":"<p>The ichrisbirch application uses a layered Docker Compose approach where:</p> <ol> <li>Base configuration (<code>docker-compose.yml</code>) defines core services</li> <li>Environment-specific overrides customize behavior for dev/test/prod</li> <li>Environment files (<code>.dev.env</code>, <code>.test.env</code>, <code>.prod.env</code>) inject configuration</li> </ol>"},{"location":"docker/docker-compose/#file-structure","title":"File Structure","text":"<pre><code>docker-compose.yml           # Base service definitions\ndocker-compose.dev.yml       # Development overrides\ndocker-compose.test.yml      # Testing overrides  \ndocker-compose.prod.yml      # Production overrides\n.dev.env                     # Development environment variables\n.test.env                    # Testing environment variables\n.prod.env                    # Production environment variables\n</code></pre>"},{"location":"docker/docker-compose/#base-configuration-docker-composeyml","title":"Base Configuration (<code>docker-compose.yml</code>)","text":""},{"location":"docker/docker-compose/#core-services","title":"Core Services","text":"<pre><code>services:\n  postgres:    # PostgreSQL database\n  redis:       # Redis cache/session store\n  api:         # FastAPI backend service\n  app:         # Flask frontend service\n  scheduler:   # Background job scheduler\n</code></pre>"},{"location":"docker/docker-compose/#service-communication","title":"Service Communication","text":"<p>All services communicate via Docker's internal network using service names:</p> <ul> <li><code>postgres:5432</code> - Database server</li> <li><code>redis:6379</code> - Redis server  </li> <li><code>api:8000</code> - FastAPI backend</li> <li><code>app:5000</code> - Flask frontend</li> </ul>"},{"location":"docker/docker-compose/#port-mapping-strategy","title":"Port Mapping Strategy","text":"<p>Internal Ports (container-to-container): Always the same External Ports (host access): Environment-specific</p> Service Internal Port Dev External Test External Prod External API 8000 8000 8001 8000 App 5000 5000 5001 5000 PostgreSQL 5432 5432 5434 5432 Redis 6379 6379 6380 6379"},{"location":"docker/docker-compose/#environment-overrides","title":"Environment Overrides","text":""},{"location":"docker/docker-compose/#development-override-docker-composedevyml","title":"Development Override (<code>docker-compose.dev.yml</code>)","text":"<p>Purpose: Enable development features</p> <p>Key Features:</p> <ul> <li>Volume mounts for live editing</li> </ul> <p>Example Override: services:   api:     build:       target: development     command: uvicorn ichrisbirch.wsgi_api:api --host 0.0.0.0 --port 8000 --reload --log-level debug     volumes:       - .:/app</p> <pre><code>### Testing Override (`docker-compose.test.yml`)\n\n\n\n\n\n- Test runner service\n\n\n**Example Override**:\n\n```yaml\n\n  postgres:\n    tmpfs:\n      - /var/lib/postgresql/data  # In-memory database\n\n    environment:\n      POSTGRES_DB: ichrisbirch_test\n\n\n  test-runner:\n\n    build: .\n\n    depends_on:\n\n        condition: service_healthy\n\n\n### Production Override (`docker-compose.prod.yml`)\n\n\n**Purpose**: Production-optimized configuration\n\n**Key Features**:\n- Production build target\n- Gunicorn with multiple workers\n- Health checks\n- No volume mounts (security)\n- Nginx reverse proxy\n\n**Example Override**:\n\n```yaml\nservices:\n  api:\n    build:\n      target: production\n    command: gunicorn ichrisbirch.wsgi_api:api --bind 0.0.0.0:8000 --workers 4\n    volumes: []  # No development mounts\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n</code></pre>"},{"location":"docker/docker-compose/#environment-variables","title":"Environment Variables","text":""},{"location":"docker/docker-compose/#environment-file-usage","title":"Environment File Usage","text":"<p>Each environment uses its own <code>.env</code> file:</p> <pre><code># Development\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml up\n\n# Testing  \ndocker-compose --env-file .test.env -f docker-compose.yml -f docker-compose.test.yml up\n\n# Production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up\n</code></pre>"},{"location":"docker/docker-compose/#variable-precedence","title":"Variable Precedence","text":"<ol> <li>Command line (<code>-e</code> flag)</li> <li>Shell environment (<code>export VAR=value</code>)</li> <li>Environment file (<code>--env-file .env</code>)</li> <li>Compose file (<code>environment:</code> section)</li> <li>Dockerfile (<code>ENV</code> instruction)</li> </ol>"},{"location":"docker/docker-compose/#key-configuration-variables","title":"Key Configuration Variables","text":"<pre><code># Environment identification\nENVIRONMENT=development|testing|production\n\n# Database configuration  \nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=ichrisbirch_dev\nPOSTGRES_USERNAME=postgres\nPOSTGRES_PASSWORD=postgres\n\n# Redis configuration\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_PASSWORD=\"\"\n\n# Application configuration\nFASTAPI_HOST=api\nFASTAPI_PORT=8000\nFLASK_HOST=app\nFLASK_PORT=5000\n</code></pre>"},{"location":"docker/docker-compose/#service-dependencies","title":"Service Dependencies","text":""},{"location":"docker/docker-compose/#dependency-chain","title":"Dependency Chain","text":"<pre><code>postgres (database)\n\u251c\u2500\u2500 redis (cache)\n\u251c\u2500\u2500 api (backend)\n\u2502   \u2514\u2500\u2500 app (frontend)\n\u2502   \u2514\u2500\u2500 scheduler (background jobs)\n\u2514\u2500\u2500 test-runner (testing only)\n</code></pre>"},{"location":"docker/docker-compose/#service-health-dependencies","title":"Service Health Dependencies","text":"<p>Services wait for dependencies to be healthy:</p> <pre><code>services:\n  api:\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_started\n</code></pre>"},{"location":"docker/docker-compose/#build-integration","title":"Build Integration","text":""},{"location":"docker/docker-compose/#how-compose-uses-dockerfile","title":"How Compose Uses Dockerfile","text":"<p>Docker Compose extends the Dockerfile with:</p> <ol> <li>Build Context: Sets working directory</li> <li>Target Selection: Chooses build stage</li> <li>Build Arguments: Passes variables to build</li> <li>Image Naming: Tags built images</li> </ol>"},{"location":"docker/docker-compose/#build-configuration","title":"Build Configuration","text":"<pre><code>services:\n\n      context: .              # Build context (project root)\n      dockerfile: Dockerfile  # Dockerfile location\n      target: development     # Build stage\n      args:\n        POETRY_VERSION: 1.8.3\n</code></pre>"},{"location":"docker/docker-compose/#network-architecture","title":"Network Architecture","text":""},{"location":"docker/docker-compose/#internal-network","title":"Internal Network","text":"<p>Docker Compose creates an isolated network where:</p> <ul> <li>Services communicate via service names</li> <li>No external access unless ports are mapped</li> <li>Internal DNS resolution (e.g., <code>ping postgres</code>)</li> </ul>"},{"location":"docker/docker-compose/#external-access","title":"External Access","text":"<p>Only mapped ports are accessible from host:</p> <pre><code>services:\n  api:\n    ports:\n      - \"8000:8000\"  # Host:Container\n</code></pre>"},{"location":"docker/docker-compose/#volume-management","title":"Volume Management","text":""},{"location":"docker/docker-compose/#development-volumes","title":"Development Volumes","text":"<pre><code>volumes:\n  - .:/app                    # Live code editing\n  - /app/.venv               # Exclude virtual environment\n  - postgres_data:/var/lib/postgresql/data  # Database persistence\n</code></pre>"},{"location":"docker/docker-compose/#production-volumes","title":"Production Volumes","text":"<pre><code>volumes:\n  - postgres_data:/var/lib/postgresql/data  # Database only\n  # No source code mounts for security\n</code></pre>"},{"location":"docker/docker-compose/#volume-types","title":"Volume Types","text":"<ul> <li>Bind mounts: Host directory \u2192 Container (<code>./src:/app/src</code>)</li> <li>Named volumes: Docker-managed storage (<code>postgres_data:/var/lib/postgresql/data</code>)</li> <li>Tmpfs mounts: In-memory storage (<code>tmpfs: /tmp</code>)</li> </ul>"},{"location":"docker/docker-compose/#operational-commands","title":"Operational Commands","text":""},{"location":"docker/docker-compose/#development-workflow","title":"Development Workflow","text":"<pre><code># Start development environment\n./scripts/dev-start.sh\n\n# View logs\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml logs -f\n\n# Execute commands in running container\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml exec api bash\n\n# Restart specific service\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml restart api\n</code></pre>"},{"location":"docker/docker-compose/#testing-workflow","title":"Testing Workflow","text":"<pre><code># Run tests\n./scripts/test-run.sh\n\n# Run specific test\n./scripts/test-run.sh tests/ichrisbirch/api/endpoints/test_habits.py\n\n\n# Debug test failures\ndocker-compose --env-file .test.env -f docker-compose.yml -f docker-compose.test.yml run --rm test-runner bash\n</code></pre>"},{"location":"docker/docker-compose/#production-workflow","title":"Production Workflow","text":"<pre><code># Deploy production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Check health\n\n# Scale services\n\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d --scale api=3\n</code></pre>"},{"location":"docker/docker-compose/#security-considerations","title":"Security Considerations","text":""},{"location":"docker/docker-compose/#development-security","title":"Development Security","text":"<ul> <li>Local network isolation</li> <li>Non-root container execution</li> <li> <p>Volume mounts limited to necessary files</p> </li> <li> <p>No source code mounts</p> </li> <li> <p>Secrets via environment variables</p> </li> <li>Network isolation with reverse proxy</li> <li>Health checks for monitoring</li> </ul>"},{"location":"docker/docker-compose/#environment-isolation","title":"Environment Isolation","text":"<p>Each environment has:</p> <ul> <li>Separate databases</li> <li>Isolated networks</li> <li>Environment-specific secrets</li> <li>Different external ports</li> </ul>"},{"location":"docker/docker-compose/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"docker/docker-compose/#health-checks","title":"Health Checks","text":"<pre><code>healthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n  interval: 30s\n  timeout: 10s\n  start_period: 5s\n  retries: 3\n</code></pre>"},{"location":"docker/docker-compose/#logging","title":"Logging","text":"<pre><code># View all logs\ndocker-compose logs\n\n# Follow specific service\ndocker-compose logs -f api\n\n# Filter by timestamp\ndocker-compose logs --since 2024-01-01T00:00:00 api\n</code></pre>"},{"location":"docker/docker-compose/#resource-monitoring","title":"Resource Monitoring","text":"<pre><code># Resource usage\ndocker-compose exec api top\n\n# System stats\ndocker stats\n\n# Service status\ndocker-compose ps\n</code></pre>"},{"location":"docker/docker-compose/#best-practices","title":"Best Practices","text":""},{"location":"docker/docker-compose/#1-environment-consistency","title":"1. Environment Consistency","text":"<ul> <li>Use same service names across environments</li> <li>Maintain consistent internal ports</li> <li>Only vary external configuration</li> </ul>"},{"location":"docker/docker-compose/#2-service-independence","title":"2. Service Independence","text":"<ul> <li>Each service has its own container</li> <li>Services communicate via well-defined APIs</li> <li>No shared file systems between services</li> </ul>"},{"location":"docker/docker-compose/#3-configuration-management","title":"3. Configuration Management","text":"<ul> <li>Environment variables for all configuration</li> <li>Separate <code>.env</code> files per environment</li> <li>No hardcoded values in compose files</li> </ul>"},{"location":"docker/docker-compose/#4-development-experience","title":"4. Development Experience","text":"<ul> <li>Hot-reload for rapid iteration</li> <li>Volume mounts for live editing</li> <li>Comprehensive logging and debugging</li> </ul>"},{"location":"docker/docker-compose/#5-production-readiness","title":"5. Production Readiness","text":"<ul> <li>Health checks for all services</li> <li>Proper restart policies</li> <li>Resource limits and reservations</li> <li>Security hardening</li> </ul>"},{"location":"docker/docker-compose/#summary","title":"Summary","text":"<p>This Docker Compose architecture provides:</p> <ul> <li>Environment Parity: Same services, different configurations</li> <li>Service Isolation: Each component in its own container</li> <li>Configuration Flexibility: Environment-specific overrides</li> <li>Development Efficiency: Hot-reload and live editing</li> <li>Production Readiness: Health checks and scaling</li> <li>Security: Proper isolation and access controls</li> </ul> <p>The layered approach allows for maximum flexibility while maintaining consistency across all environments.</p>"},{"location":"docker/docker-quick-reference/","title":"Docker Quick Reference","text":"<p>Quick commands and troubleshooting for the ichrisbirch Docker setup.</p>"},{"location":"docker/docker-quick-reference/#essential-commands","title":"Essential Commands","text":""},{"location":"docker/docker-quick-reference/#development","title":"Development","text":"<pre><code># Start development environment\n./scripts/dev-start.sh\n\n# Stop development environment\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml down\n\n# Rebuild and start\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml up --build\n\n# View logs\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml logs -f api\n</code></pre>"},{"location":"docker/docker-quick-reference/#testing","title":"Testing","text":"<pre><code># Run all tests\n./scripts/test-run.sh\n\n# Run specific test file\n./scripts/test-run.sh tests/ichrisbirch/api/endpoints/test_habits.py\n\n# Run with coverage\n./scripts/test-run.sh --cov=ichrisbirch --cov-report=html\n</code></pre>"},{"location":"docker/docker-quick-reference/#production","title":"Production","text":"<pre><code># Deploy production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Check status\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml ps\n\n# View production logs\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml logs -f\n</code></pre>"},{"location":"docker/docker-quick-reference/#debug-commands","title":"Debug Commands","text":""},{"location":"docker/docker-quick-reference/#container-inspection","title":"Container Inspection","text":"<pre><code># Shell into running container\ndocker-compose exec api bash\n\n# Run one-off command\ndocker-compose run --rm api python -c \"import ichrisbirch; print('OK')\"\n\n# Check environment variables\ndocker-compose exec api env | grep POSTGRES\n</code></pre>"},{"location":"docker/docker-quick-reference/#service-health","title":"Service Health","text":"<pre><code># Check if services are running\ndocker-compose ps\n\n# Test service connectivity\ndocker-compose exec api ping postgres\ndocker-compose exec api ping redis\n\n# Check port availability\ndocker-compose exec api netstat -tlnp\n</code></pre>"},{"location":"docker/docker-quick-reference/#database-access","title":"Database Access","text":"<pre><code># Connect to PostgreSQL\ndocker-compose exec postgres psql -U postgres -d ichrisbirch_dev\n\n# Check database tables\ndocker-compose exec postgres psql -U postgres -d ichrisbirch_dev -c \"\\\\dt\"\n\n# Connect to Redis\ndocker-compose exec redis redis-cli\n</code></pre>"},{"location":"docker/docker-quick-reference/#common-issues","title":"Common Issues","text":""},{"location":"docker/docker-quick-reference/#port-conflicts","title":"Port Conflicts","text":"<p>Error: <code>Port 8000 is already in use</code></p> <p>Solution:</p> <pre><code># Find process using port\nlsof -i :8000\n\n# Stop all containers\ndocker-compose down\n\n# Start with different ports\n# Edit .env file to change FASTAPI_PORT\n</code></pre>"},{"location":"docker/docker-quick-reference/#database-connection","title":"Database Connection","text":"<p>Error: <code>psycopg2.OperationalError: could not connect to server</code></p> <p>Solution:</p> <pre><code># Check if PostgreSQL is running\ndocker-compose ps postgres\n\n# Check network connectivity\ndocker-compose exec api ping postgres\n\n# Verify environment variables\ndocker-compose exec api env | grep POSTGRES\n</code></pre>"},{"location":"docker/docker-quick-reference/#permission-issues","title":"Permission Issues","text":"<p>Error: <code>Permission denied</code></p> <p>Solution:</p> <pre><code># Fix file ownership\nsudo chown -R $(id -u):$(id -g) .\n\n# Rebuild with correct permissions\ndocker-compose build --no-cache\n</code></pre>"},{"location":"docker/docker-quick-reference/#environment-files","title":"Environment Files","text":""},{"location":"docker/docker-quick-reference/#development-devenv","title":"Development (<code>.dev.env</code>)","text":"<pre><code>ENVIRONMENT=\"development\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch_dev\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#testing-testenv","title":"Testing (<code>.test.env</code>)","text":"<pre><code>ENVIRONMENT=\"testing\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch_test\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#production-prodenv","title":"Production (<code>.prod.env</code>)","text":"<pre><code>ENVIRONMENT=\"production\"\nPOSTGRES_HOST=\"postgres\"\nPOSTGRES_PORT=\"5432\"\nPOSTGRES_DB=\"ichrisbirch\"\nFASTAPI_HOST=\"api\"\nFASTAPI_PORT=\"8000\"\nFLASK_HOST=\"app\"\nFLASK_PORT=\"5000\"\n</code></pre>"},{"location":"docker/docker-quick-reference/#build-targets","title":"Build Targets","text":""},{"location":"docker/docker-quick-reference/#development-build","title":"Development Build","text":"<pre><code># Build development image\ndocker build --target development -t ichrisbirch:dev .\n\n# Features: hot-reload, dev dependencies, debugging\n</code></pre>"},{"location":"docker/docker-quick-reference/#production-build","title":"Production Build","text":"<pre><code># Build production image\ndocker build --target production -t ichrisbirch:prod .\n\n# Features: minimal size, security hardening, performance optimized\n</code></pre>"},{"location":"docker/docker-quick-reference/#service-urls","title":"Service URLs","text":"<p>http://localhost:8000/docs</p>"},{"location":"docker/docker-quick-reference/#developmenthttplocalhost5000","title":"Developmenthttp://localhost:5000","text":"<ul> <li>API Documentation: http://localhost:8000/docs</li> <li>Web Application: http://localhost:5000</li> <li>Database: localhost:5432</li> <li>Redis: localhost:6379 http://localhost:8001/docs</li> </ul>"},{"location":"docker/docker-quick-reference/#testinghttplocalhost5001","title":"Testinghttp://localhost:5001","text":"<ul> <li>API: http://localhost:8001/docs</li> <li>Web Application: http://localhost:5001</li> <li>Database: localhost:5434</li> <li>Redis: localhost:6380 http://localhost:8000</li> </ul>"},{"location":"docker/docker-quick-reference/#productionhttplocalhost5000","title":"Productionhttp://localhost:5000","text":"<ul> <li>API: http://localhost:8000</li> <li>Web Application: http://localhost:5000</li> <li>Database: localhost:5432 (if exposed)</li> <li>Redis: localhost:6379 (if exposed)</li> </ul>"},{"location":"docker/docker-quick-reference/#useful-docker-commands","title":"Useful Docker Commands","text":"<pre><code># Clean up unused containers, networks, images\ndocker system prune -a\n\n# View image sizes\ndocker images\n\n# View container resource usage\ndocker stats\n\n# Export container filesystem\ndocker export &lt;container_id&gt; &gt; backup.tar\n\n# Import container filesystem\ndocker import backup.tar ichrisbirch:backup\n</code></pre>"},{"location":"docker/docker-quick-reference/#monitoring","title":"Monitoring","text":""},{"location":"docker/docker-quick-reference/#health-checks","title":"Health Checks","text":"<pre><code># Check health status\ndocker-compose ps\n\n# Manual health check\ncurl -f http://localhost:8000/health\n</code></pre>"},{"location":"docker/docker-quick-reference/#resource-usage","title":"Resource Usage","text":"<pre><code># Container stats\ndocker stats\n\n# Detailed container info\ndocker inspect &lt;container_id&gt;\n\n# Process list in container\ndocker-compose exec api ps aux\n</code></pre>"},{"location":"docker/docker-quick-reference/#advanced-usage","title":"Advanced Usage","text":""},{"location":"docker/docker-quick-reference/#custom-commands","title":"Custom Commands","text":"<pre><code># Run database migrations\ndocker-compose run --rm api alembic upgrade head\n\n# Create new migration\ndocker-compose run --rm api alembic revision --autogenerate -m \"description\"\n\n# Run Python shell\ndocker-compose run --rm api python\n\n# Install new package\ndocker-compose run --rm api poetry add package-name\n</code></pre>"},{"location":"docker/docker-quick-reference/#volume-management","title":"Volume Management","text":"<pre><code># List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect ichrisbirch_postgres_data\n\n# Backup volume\ndocker run --rm -v ichrisbirch_postgres_data:/data -v $(pwd):/backup alpine tar czf /backup/backup.tar.gz /data\n\n# Restore volume\ndocker run --rm -v ichrisbirch_postgres_data:/data -v $(pwd):/backup alpine sh -c \"cd /data &amp;&amp; tar xzf /backup/backup.tar.gz --strip 1\"\n</code></pre> <p>This reference should help with day-to-day Docker operations and troubleshooting!</p>"},{"location":"docker/docker/","title":"Docker Architecture and Build Process","text":"<p>This document explains the Docker architecture for the ichrisbirch application, including multi-stage builds, environment-specific configurations, and integration with Docker Compose.</p> <ul> <li>Overview</li> <li>Dockerfile Architecture</li> <li>Multi-Stage Build Strategy</li> <li>Why Multi-Stage?</li> <li>Build Stages</li> <li>1. Python Base Stage (<code>python-base</code>)</li> <li>2. Builder Stage (<code>builder</code>)</li> <li>Production Environment</li> <li>docker-compose.prod.yml</li> <li>Environment File Integration</li> <li>Build Commands</li> <li>Direct Docker Build</li> <li>Docker Compose Build</li> <li>Convenience Scripts</li> <li>Service-Specific Commands</li> <li>Multiple Services from One Image</li> <li>Why One Dockerfile for Multiple Services?</li> <li>Best Practices</li> <li>1. Layer Caching</li> <li>4. Development Experience</li> <li>Common Issues<ul> <li>4. Development Hot-Reload Not Working</li> </ul> </li> <li>Check running processes</li> <li>View logs</li> <li>Check environment variables</li> <li>Performance Optimization</li> <li>Summary</li> </ul>"},{"location":"docker/docker/#overview","title":"Overview","text":"<p>The ichrisbirch application uses a multi-stage Docker build approach that creates optimized images for different environments:</p> <ul> <li>Development: Full development environment with hot-reload and debugging tools</li> <li>Production: Minimal, security-hardened runtime environment</li> <li>Testing: Containerized testing environment with all dependencies</li> </ul> <p>All three environments share the same base image and dependencies but differ in configuration, installed packages, and runtime commands.</p>"},{"location":"docker/docker/#dockerfile-architecture","title":"Dockerfile Architecture","text":""},{"location":"docker/docker/#multi-stage-build-strategy","title":"Multi-Stage Build Strategy","text":"<pre><code># Stage 1: python-base (shared foundation)\nFROM python:3.12-slim as python-base\n\n# Stage 2: builder (dependency compilation)\nFROM python-base as builder\n\n# Stage 3: development (dev tools + hot reload)\nFROM python-base as development\n\n# Stage 4: production (minimal runtime)\nFROM python-base as production\n</code></pre>"},{"location":"docker/docker/#why-multi-stage","title":"Why Multi-Stage?","text":"<ol> <li>Shared Base: All stages share the same Python runtime and basic configuration</li> <li>Build Isolation: Compilation tools are only in the builder stage</li> <li>Size Optimization: Production images exclude development dependencies</li> <li>Security: Production images have minimal attack surface</li> <li>Consistency: Same base ensures identical runtime behavior</li> </ol>"},{"location":"docker/docker/#build-stages","title":"Build Stages","text":""},{"location":"docker/docker/#1-python-base-stage-python-base","title":"1. Python Base Stage (<code>python-base</code>)","text":"<pre><code>FROM python:3.12-slim as python-base\n</code></pre> <p>Purpose: Establishes the foundation for all other stages</p> <p>Key Features:</p> <p>Environment Variables:</p> <pre><code>    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.8.3 \\\n    POETRY_HOME=\"/opt/poetry\" \\\n    APP_PATH=\"/app\"\n</code></pre>"},{"location":"docker/docker/#2-builder-stage-builder","title":"2. Builder Stage (<code>builder</code>)","text":"<pre><code>**Purpose**: Compiles dependencies and creates the virtual environment\n</code></pre> <pre><code>FROM python-base as development\n</code></pre> <p>Key Features:</p> <ul> <li>Copies virtual environment from builder</li> </ul> <p>Development-Specific:</p> <pre><code>RUN poetry install --no-root  # Includes dev dependencies\n### 4. Production Stage (`production`)\nFROM python-base as production\n</code></pre> <p>Key Features:</p> <ul> <li>Only runtime dependencies (no build tools)</li> <li> <p>Minimal system packages</p> </li> <li> <p>Security optimizations</p> </li> </ul> <pre><code>RUN poetry install --only=main --no-root  # No dev dependencies\nCMD [\"gunicorn\", \"ichrisbirch.wsgi_api:api\", \"--bind\", \"0.0.0.0:8000\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--workers\", \"4\"]\n\n## Environment Configurations\n\n\n### Development Environment\n\n\n**Image Target**: `development`\n\n\n\n- Hot-reload enabled\n\n- Volume mounts for live code editing\n\n\n# docker-compose.dev.yml\nservices:\n  api:\n\n      context: .\n\n\n    command: uvicorn ichrisbirch.wsgi_api:api --host 0.0.0.0 --port 8000 --reload\n    volumes:\n      - .:/app\n</code></pre>"},{"location":"docker/docker/#production-environment","title":"Production Environment","text":"<p>Image Target: <code>production</code></p> <p>Characteristics:</p> <ul> <li>Security hardening</li> <li>Health checks Docker Compose Usage:</li> </ul>"},{"location":"docker/docker/#docker-composeprodyml","title":"docker-compose.prod.yml","text":"<p>api:       context: .       target: production     command: gunicorn ichrisbirch.wsgi_api:api --bind 0.0.0.0:8000 --worker-class uvicorn.workers.UvicornWorker --workers 4</p> <pre><code>### Testing Environment\n\n**Image Target**: `development` (with test configuration)\n**Characteristics**:\n\n- Same as development but with test configuration\n\n- Test-specific environment variables\n- Isolated test networks\n\n## Docker Compose Integration\n\n\n### How Docker Compose Uses the Dockerfile\n\nDocker Compose extends the Dockerfile with:\n\n\n1. **Target Selection**: Chooses which stage to build\n2. **Environment Variables**: Injects configuration via `.env` files\n3. **Volume Mounts**: Overlays source code for development\n4. **Network Configuration**: Connects services\n5. **Command Overrides**: Customizes startup commands\n\n### Build Process Flow\n```mermaid\ngraph TD\n    A[docker-compose build] --&gt; B[Read Dockerfile]\n    B --&gt; C[Build python-base stage]\n    D --&gt; E{Target specified?}\n    E --&gt;|development| F[Build development stage]\n    E --&gt;|production| G[Build production stage]\n    F --&gt; H[Apply docker-compose overrides]\n    G --&gt; H\n    H --&gt; I[Start services]\n</code></pre>"},{"location":"docker/docker/#environment-file-integration","title":"Environment File Integration","text":"<p>Each environment uses its own <code>.env</code> file:</p> <pre><code># Development\ndocker-compose --env-file .dev.env -f docker-compose.yml -f docker-compose.dev.yml up\n\n# Testing\ndocker-compose --env-file .test.env -f docker-compose.yml -f docker-compose.test.yml up\n\n# Production\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up\n</code></pre>"},{"location":"docker/docker/#build-commands","title":"Build Commands","text":""},{"location":"docker/docker/#direct-docker-build","title":"Direct Docker Build","text":"<pre><code># Build development image\ndocker build --target development -t ichrisbirch:dev .\n\n# Build production image\ndocker build --target production -t ichrisbirch:prod .\n\n# Build with build arguments\ndocker build --target production --build-arg POETRY_VERSION=1.8.3 -t ichrisbirch:prod .\n</code></pre>"},{"location":"docker/docker/#docker-compose-build","title":"Docker Compose Build","text":"<pre><code># Build development environment\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml build\n\n# Build production environment\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml build\n\n# Build with no cache\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml build --no-cache\n</code></pre>"},{"location":"docker/docker/#convenience-scripts","title":"Convenience Scripts","text":"<pre><code># Development\n./scripts/dev-start.sh\n\n# Testing\n./scripts/test-run.sh\n\n# Production (manual)\ndocker-compose --env-file .prod.env -f docker-compose.yml -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"docker/docker/#service-specific-commands","title":"Service-Specific Commands","text":""},{"location":"docker/docker/#multiple-services-from-one-image","title":"Multiple Services from One Image","text":"<p>The same Docker image can run different services by overriding the command:</p> <pre><code># docker-compose.yml\nservices:\n  # FastAPI Backend\n  api:\n    build: .\n    command: uvicorn ichrisbirch.wsgi_api:api --host 0.0.0.0 --port 8000\n\n  # Flask Frontend\n  app:\n    build: .\n    command: flask --app ichrisbirch.wsgi_app:app run --host 0.0.0.0 --port 5000\n\n\n  # Scheduler\n  scheduler:\n    build: .\n    command: python -m ichrisbirch.wsgi_scheduler\n</code></pre>"},{"location":"docker/docker/#why-one-dockerfile-for-multiple-services","title":"Why One Dockerfile for Multiple Services?","text":"<ol> <li>Consistency: Same dependencies and runtime environment</li> <li>Efficiency: Single build process, shared layers</li> <li>Maintainability: One file to update for all services</li> <li>Resource Optimization: Shared base images reduce storage</li> </ol>"},{"location":"docker/docker/#best-practices","title":"Best Practices","text":""},{"location":"docker/docker/#1-layer-caching","title":"1. Layer Caching","text":"<p>Dependencies are installed before copying source code:</p> <pre><code>COPY pyproject.toml poetry.lock* ./\n\n\nCOPY --chown=appuser:appuser ichrisbirch/ ./ichrisbirch/\n</code></pre> <ul> <li> <p>Non-root user for all processes</p> </li> <li> <p>Proper file permissions</p> </li> <li> <p>Multi-stage builds exclude build dependencies from production</p> </li> <li>Cleanup package caches after installation</li> </ul>"},{"location":"docker/docker/#4-development-experience","title":"4. Development Experience","text":"<ul> <li>Volume mounts for live code editing</li> </ul>"},{"location":"docker/docker/#common-issues","title":"Common Issues","text":"<pre><code># Rebuild without cache\ndocker-compose build --no-cache\n# Check Poetry version\ndocker run --rm ichrisbirch:dev poetry --version\n\n\n**Solution**:\n```dockerfile\n# Or fix permissions\n\ndocker run --rm ichrisbirch:dev python -c \"import psycopg2; print('OK')\"\n\n# Verify service communication\ndocker-compose exec api ping postgres\n</code></pre>"},{"location":"docker/docker/#4-development-hot-reload-not-working","title":"4. Development Hot-Reload Not Working","text":"<p>Error: Changes not reflected in running container</p> <p>Solution: Verify volume mounts in docker-compose.dev.yml:</p> <pre><code>volumes:\n  - .:/app\n  - /app/.venv  # Exclude virtual environment\n</code></pre>"},{"location":"docker/docker/#check-running-processes","title":"Check running processes","text":"<pre><code>docker-compose exec api ps aux\n</code></pre>"},{"location":"docker/docker/#view-logs","title":"View logs","text":"<pre><code>docker-compose exec api bash\n</code></pre>"},{"location":"docker/docker/#check-environment-variables","title":"Check environment variables","text":"<pre><code>docker-compose exec api env | grep -E \"(POSTGRES|REDIS|FASTAPI)\"\n</code></pre>"},{"location":"docker/docker/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Clean up unused images\ndocker system prune -a\n\n# Check image sizes\ndocker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"\n</code></pre>"},{"location":"docker/docker/#summary","title":"Summary","text":"<p>This Docker architecture provides:</p> <ul> <li>Flexibility: Same image, different configurations</li> <li>Security: Non-root execution, minimal attack surface</li> <li>Efficiency: Multi-stage builds, layer caching</li> <li> <p>Consistency: Identical environments across development, testing, and production</p> </li> <li> <p>Maintainability: Single Dockerfile for all services</p> </li> </ul> <p>The integration with Docker Compose allows for easy environment switching while maintaining consistency in the underlying application runtime.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This comprehensive troubleshooting guide documents common issues, their root causes, attempted solutions, and final resolutions encountered during the development and operation of the iChrisBirch project.</p>"},{"location":"troubleshooting/#overview","title":"Overview","text":"<p>The troubleshooting documentation is organized by component and includes:</p> <ul> <li>Problem description: Clear statement of the issue</li> <li>Root cause analysis: Technical explanation of why the issue occurs</li> <li>Attempted solutions: What was tried and why it didn't work</li> <li>Final resolution: Working solution with implementation details</li> <li>Prevention: How to avoid the issue in the future</li> </ul>"},{"location":"troubleshooting/#quick-reference","title":"Quick Reference","text":""},{"location":"troubleshooting/#common-issues-by-component","title":"Common Issues by Component","text":"Component Common Issues Quick Solutions Docker Build failures, container networking Check Dockerfile syntax, network configuration Poetry to UV Migration Dependency management, virtual environments Follow migration checklist Testing Test failures, pytest not found, Docker issues Verify test dependencies, rebuild images Database Connection errors, schema issues Check connection strings, run migrations Development Environment Setup problems, tooling conflicts Follow setup guide step-by-step"},{"location":"troubleshooting/#recent-critical-issues-july-2025","title":"Recent Critical Issues (July 2025)","text":"<p>Docker Network Conflicts During Testing:</p> <ul> <li>Issue: Test runs fail with \"failed to set up container networking: network not found\" errors</li> <li>Resolution: Implemented comprehensive cleanup function with pre/post cleanup and multiple fallback strategies</li> <li>Details: Docker Network Conflicts</li> </ul> <p>Testing Infrastructure Failures:</p> <ul> <li><code>error: Failed to spawn: 'pytest'</code> \u2192 Missing <code>--group test</code> in Dockerfile</li> <li><code>service has neither an image nor a build context</code> \u2192 Add build directive to compose services  </li> <li><code>ModuleNotFoundError: tests.utils.environment</code> \u2192 Fix import paths in conftest.py</li> <li>Docker network conflicts \u2192 Run comprehensive Docker cleanup</li> </ul> <p>See Testing Issues - Recent Critical Issues for detailed solutions.</p>"},{"location":"troubleshooting/#emergency-fixes","title":"Emergency Fixes","text":"<p>For urgent production issues:</p> <ol> <li>Service Down: Check deployment issues</li> <li>Database Connection: See database troubleshooting</li> <li>Test Failures: Review testing diagnostics</li> </ol>"},{"location":"troubleshooting/#how-to-use-this-guide","title":"How to Use This Guide","text":"<ol> <li>Identify the component where the issue is occurring</li> <li>Check the quick reference for immediate solutions</li> <li>Read the detailed troubleshooting page for comprehensive guidance</li> <li>Follow the resolution steps with provided code examples</li> <li>Apply prevention measures to avoid future occurrences</li> </ol>"},{"location":"troubleshooting/#contributing","title":"Contributing","text":"<p>When you encounter and solve a new issue:</p> <ol> <li>Document it in the appropriate section</li> <li>Include error messages, logs, and code snippets</li> <li>Explain the root cause and why the solution works</li> <li>Add prevention tips for future reference</li> </ol>"},{"location":"troubleshooting/#legacy-issues","title":"Legacy Issues","text":"<p>Historical troubleshooting information has been migrated from the original troubleshooting.md file and integrated into the appropriate component-specific pages while maintaining all the original solutions and context.</p>"},{"location":"troubleshooting/database-issues/","title":"Database Troubleshooting","text":"<p>This document covers database-related issues encountered during development, testing, and deployment of the iChrisBirch project.</p>"},{"location":"troubleshooting/database-issues/#connection-issues","title":"Connection Issues","text":""},{"location":"troubleshooting/database-issues/#database-connection-refused","title":"Database Connection Refused","text":"<p>Problem: Application cannot connect to PostgreSQL database.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.OperationalError: connection to server at \"localhost\" (127.0.0.1) port 5432 refused</code></li> <li><code>FATAL: database \"ichrisbirch\" does not exist</code></li> <li><code>FATAL: role \"ichrisbirch\" does not exist</code></li> </ul> <p>Common Causes and Solutions:</p>"},{"location":"troubleshooting/database-issues/#1-wrong-database-host","title":"1. Wrong Database Host","text":"<pre><code># Wrong: Using localhost in containerized environment\nDATABASE_URL = \"postgresql://user:pass@localhost:5432/db\"\n\n# Correct: Using Docker service name\nDATABASE_URL = \"postgresql://user:pass@postgres:5432/db\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-database-service-not-ready","title":"2. Database Service Not Ready","text":"<p>Add health checks to docker-compose:</p> <pre><code>services:\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n\n  app:\n    depends_on:\n      postgres:\n        condition: service_healthy\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-missing-database-or-user","title":"3. Missing Database or User","text":"<pre><code>-- Connect as superuser and create database/user\nCREATE USER ichrisbirch WITH PASSWORD 'password';\nCREATE DATABASE ichrisbirch OWNER ichrisbirch;\nGRANT ALL PRIVILEGES ON DATABASE ichrisbirch TO ichrisbirch;\n</code></pre>"},{"location":"troubleshooting/database-issues/#schema-and-table-issues","title":"Schema and Table Issues","text":"<p>Problem: Tables or schemas don't exist.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.errors.InvalidSchemaName: schema \"ichrisbirch_test\" does not exist</code></li> <li><code>sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation \"users\" does not exist</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-run-database-migrations","title":"1. Run Database Migrations","text":"<pre><code># In development\ndocker-compose exec app uv run alembic upgrade head\n\n# In testing\ndocker-compose -f docker-compose.test.yml run test-runner uv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-set-schema-environment-variable","title":"2. Set Schema Environment Variable","text":"<pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    environment:\n      - POSTGRES_DB_SCHEMA=ichrisbirch_test\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-create-schema-manually","title":"3. Create Schema Manually","text":"<pre><code>-- Connect to database and create schema\nCREATE SCHEMA IF NOT EXISTS ichrisbirch_test;\nALTER USER ichrisbirch SET search_path TO ichrisbirch_test;\n</code></pre>"},{"location":"troubleshooting/database-issues/#migration-issues","title":"Migration Issues","text":""},{"location":"troubleshooting/database-issues/#alembic-migration-failures","title":"Alembic Migration Failures","text":"<p>Problem: Database migrations fail to run or create inconsistent state.</p> <p>Common Issues:</p>"},{"location":"troubleshooting/database-issues/#1-migration-revision-conflicts","title":"1. Migration Revision Conflicts","text":"<pre><code># Error: Multiple heads in migration history\nFAILED: Multiple head revisions are present for given argument 'head'\n\n# Resolution: Merge migration heads\nuv run alembic merge -m \"merge migration heads\" head_1 head_2\nuv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-missing-migration-dependencies","title":"2. Missing Migration Dependencies","text":"<pre><code># Error: Can't locate revision identified by 'abc123'\n# Resolution: Check migration file exists and revision ID is correct\nls ichrisbirch/alembic/versions/\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-manual-schema-changes","title":"3. Manual Schema Changes","text":"<pre><code># If manual changes were made, mark as current\nuv run alembic stamp head\n\n# Or create new migration from current state\nuv run alembic revision --autogenerate -m \"sync with manual changes\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#schema-synchronization","title":"Schema Synchronization","text":"<p>Problem: Development and test databases have different schemas.</p> <p>Resolution:</p> <pre><code># Reset test database to match development\ndocker-compose -f docker-compose.test.yml down -v\ndocker-compose -f docker-compose.test.yml up -d postgres\n\n# Wait for database to be ready\nsleep 10\n\n# Run migrations\ndocker-compose -f docker-compose.test.yml run test-runner uv run alembic upgrade head\n</code></pre>"},{"location":"troubleshooting/database-issues/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/database-issues/#slow-query-performance","title":"Slow Query Performance","text":"<p>Problem: Database queries are slow, causing application timeouts.</p> <p>Diagnosis:</p> <pre><code>-- Enable query logging in PostgreSQL\nALTER SYSTEM SET log_statement = 'all';\nALTER SYSTEM SET log_min_duration_statement = 100;  -- Log queries &gt; 100ms\nSELECT pg_reload_conf();\n\n-- Check for missing indexes\nSELECT schemaname, tablename, attname, n_distinct, correlation\nFROM pg_stats\nWHERE schemaname = 'ichrisbirch_test'\nORDER BY n_distinct DESC;\n</code></pre> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-add-database-indexes","title":"1. Add Database Indexes","text":"<pre><code>-- Create indexes for frequently queried columns\nCREATE INDEX CONCURRENTLY idx_habits_user_id ON habits(user_id);\nCREATE INDEX CONCURRENTLY idx_habits_created_at ON habits(created_at);\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-optimize-sqlalchemy-queries","title":"2. Optimize SQLAlchemy Queries","text":"<pre><code># Bad: N+1 query problem\nfor habit in session.query(Habit).all():\n    print(habit.user.username)  # Separate query for each habit\n\n# Good: Use joinedload to eager load relationships\nfrom sqlalchemy.orm import joinedload\n\nhabits = session.query(Habit).options(joinedload(Habit.user)).all()\nfor habit in habits:\n    print(habit.user.username)  # No additional queries\n</code></pre>"},{"location":"troubleshooting/database-issues/#connection-pool-issues","title":"Connection Pool Issues","text":"<p>Problem: Application runs out of database connections.</p> <p>Error Messages:</p> <ul> <li><code>QueuePool limit of size 20 overflow 10 reached</code></li> <li><code>remaining connection slots are reserved for non-replication superuser connections</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-configure-connection-pool","title":"1. Configure Connection Pool","text":"<pre><code># In database configuration\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\n    DATABASE_URL,\n    pool_size=10,          # Maximum persistent connections\n    max_overflow=20,       # Additional overflow connections\n    pool_timeout=30,       # Timeout for getting connection\n    pool_recycle=1800,     # Recycle connections every 30 minutes\n    pool_pre_ping=True     # Validate connections before use\n)\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-ensure-proper-connection-cleanup","title":"2. Ensure Proper Connection Cleanup","text":"<pre><code># Use context managers for database sessions\nfrom ichrisbirch.database import get_sqlalchemy_session\n\n# Good: Automatic cleanup\ndef get_user_habits(user_id: int):\n    with get_sqlalchemy_session() as session:\n        return session.query(Habit).filter(Habit.user_id == user_id).all()\n    # Session automatically closed\n\n# Bad: Manual cleanup required\ndef get_user_habits_bad(user_id: int):\n    session = SessionLocal()\n    try:\n        return session.query(Habit).filter(Habit.user_id == user_id).all()\n    finally:\n        session.close()  # Easy to forget!\n</code></pre>"},{"location":"troubleshooting/database-issues/#data-integrity-issues","title":"Data Integrity Issues","text":""},{"location":"troubleshooting/database-issues/#foreign-key-constraint-violations","title":"Foreign Key Constraint Violations","text":"<p>Problem: Data operations fail due to referential integrity constraints.</p> <p>Error Messages:</p> <ul> <li><code>psycopg2.errors.ForeignKeyViolation: insert or update on table \"habits\" violates foreign key constraint</code></li> <li><code>psycopg2.errors.IntegrityError: duplicate key value violates unique constraint</code></li> </ul> <p>Resolution:</p>"},{"location":"troubleshooting/database-issues/#1-check-data-dependencies","title":"1. Check Data Dependencies","text":"<pre><code>-- Verify referenced records exist\nSELECT u.id, u.username FROM users u WHERE u.id = 123;\n\n-- Check for constraint violations\nSELECT * FROM habits WHERE user_id NOT IN (SELECT id FROM users);\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-handle-dependencies-in-code","title":"2. Handle Dependencies in Code","text":"<pre><code># Ensure user exists before creating habit\nfrom ichrisbirch.database import get_sqlalchemy_session\nfrom ichrisbirch.models import User, Habit\n\ndef create_habit(user_id: int, habit_data: dict):\n    with get_sqlalchemy_session() as session:\n        # Verify user exists\n        user = session.get(User, user_id)\n        if not user:\n            raise ValueError(f\"User {user_id} does not exist\")\n\n        # Create habit\n        habit = Habit(user_id=user_id, **habit_data)\n        session.add(habit)\n        session.commit()\n        return habit\n</code></pre>"},{"location":"troubleshooting/database-issues/#data-corruption-issues","title":"Data Corruption Issues","text":"<p>Problem: Database contains inconsistent or corrupted data.</p> <p>Diagnosis:</p> <pre><code>-- Check for data inconsistencies\nSELECT\n    h.id, h.user_id, u.id as actual_user_id\nFROM habits h\nLEFT JOIN users u ON h.user_id = u.id\nWHERE u.id IS NULL;\n\n-- Verify constraints\nSELECT conname, contype FROM pg_constraint WHERE contype = 'f';\n</code></pre> <p>Resolution:</p> <pre><code># Backup database before repairs\ndocker-compose exec postgres pg_dump -U ichrisbirch ichrisbirch &gt; backup.sql\n\n# Run integrity checks\ndocker-compose exec postgres psql -U ichrisbirch -d ichrisbirch -c \"\nVACUUM ANALYZE;\nREINDEX DATABASE ichrisbirch;\n\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#environment-specific-issues","title":"Environment-Specific Issues","text":""},{"location":"troubleshooting/database-issues/#development-vs-production-differences","title":"Development vs Production Differences","text":"<p>Problem: Database works in development but fails in production.</p> <p>Common Causes:</p>"},{"location":"troubleshooting/database-issues/#1-environment-variable-differences","title":"1. Environment Variable Differences","text":"<pre><code># Check environment variables in containers\ndocker-compose exec app env | grep -i postgres\ndocker-compose -f docker-compose.prod.yml exec app env | grep -i postgres\n</code></pre>"},{"location":"troubleshooting/database-issues/#2-different-postgresql-versions","title":"2. Different PostgreSQL Versions","text":"<pre><code># Pin PostgreSQL version in docker-compose\nservices:\n  postgres:\n    image: postgres:15.4  # Specific version instead of 'latest'\n</code></pre>"},{"location":"troubleshooting/database-issues/#3-missing-extensions","title":"3. Missing Extensions","text":"<pre><code>-- Check installed extensions\nSELECT * FROM pg_extension;\n\n-- Install required extensions\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"troubleshooting/database-issues/#ssl-connection-issues","title":"SSL Connection Issues","text":"<p>Problem: SSL connection errors in production.</p> <p>Error Messages:</p> <ul> <li><code>FATAL: SSL connection is required</code></li> <li><code>SSL error: certificate verify failed</code></li> </ul> <p>Resolution:</p> <pre><code># Update connection string for SSL\nDATABASE_URL = \"postgresql://user:pass@host:5432/db?sslmode=require\"\n\n# For development with self-signed certificates\nDATABASE_URL = \"postgresql://user:pass@host:5432/db?sslmode=require&amp;sslcert=client.crt&amp;sslkey=client.key&amp;sslrootcert=ca.crt\"\n</code></pre>"},{"location":"troubleshooting/database-issues/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"troubleshooting/database-issues/#database-backup-issues","title":"Database Backup Issues","text":"<p>Problem: Unable to create or restore database backups.</p> <p>Backup Creation:</p> <pre><code># Create backup with custom format\ndocker-compose exec postgres pg_dump \\\n  -U ichrisbirch \\\n  -d ichrisbirch \\\n  -f /backup/ichrisbirch_$(date +%Y%m%d_%H%M%S).dump \\\n  --format=custom \\\n  --verbose\n\n# Create SQL backup\ndocker-compose exec postgres pg_dump \\\n  -U ichrisbirch \\\n  -d ichrisbirch \\\n  -f /backup/ichrisbirch_$(date +%Y%m%d_%H%M%S).sql \\\n  --verbose\n</code></pre> <p>Backup Restoration:</p> <pre><code># Restore from custom format\ndocker-compose exec postgres pg_restore \\\n  -U ichrisbirch \\\n  -d ichrisbirch_restored \\\n  --clean --create \\\n  --verbose \\\n  /backup/ichrisbirch_20231201_120000.dump\n\n# Restore from SQL\ndocker-compose exec postgres psql \\\n  -U ichrisbirch \\\n  -d ichrisbirch_restored \\\n  -f /backup/ichrisbirch_20231201_120000.sql\n</code></pre>"},{"location":"troubleshooting/database-issues/#monitoring-and-diagnosis","title":"Monitoring and Diagnosis","text":""},{"location":"troubleshooting/database-issues/#database-health-checks","title":"Database Health Checks","text":"<p>Create a database health check script:</p> <pre><code># scripts/db_health_check.py\nimport sys\nfrom ichrisbirch.config import settings\nfrom ichrisbirch.database import get_sqlalchemy_session\nfrom sqlalchemy import text\n\ndef check_database_health():\n    \"\"\"Comprehensive database health check.\"\"\"\n    checks = []\n\n    try:\n        with get_sqlalchemy_session() as session:\n            # Test basic connectivity\n            session.execute(text(\"SELECT 1\"))\n            checks.append((\"Connection\", \"OK\"))\n\n            # Check table existence\n            result = session.execute(text(\"\"\"\n                SELECT COUNT(*) FROM information_schema.tables\n                WHERE table_schema = 'public'\n            \"\"\"))\n            table_count = result.scalar()\n            checks.append((\"Tables\", f\"{table_count} tables\"))\n\n            # Check recent activity\n            result = session.execute(text(\"SELECT COUNT(*) FROM users\"))\n            user_count = result.scalar()\n            checks.append((\"Users\", f\"{user_count} users\"))\n\n            # Check for locks\n            result = session.execute(text(\"\"\"\n                SELECT COUNT(*) FROM pg_locks\n                WHERE NOT granted\n            \"\"\"))\n            lock_count = result.scalar()\n            checks.append((\"Blocked Queries\", f\"{lock_count} blocked\"))\n\n    except Exception as e:\n        print(f\"Database health check failed: {e}\")\n        return False\n\n    # Print results\n    print(\"Database Health Check Results:\")\n    for check, result in checks:\n        print(f\"  {check}: {result}\")\n\n    return True\n\nif __name__ == \"__main__\":\n    if not check_database_health():\n        sys.exit(1)\n</code></pre>"},{"location":"troubleshooting/database-issues/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>-- Monitor active connections\nSELECT\n    state,\n    COUNT(*) as connection_count,\n    MAX(now() - state_change) as max_duration\nFROM pg_stat_activity\nWHERE state IS NOT NULL\nGROUP BY state;\n\n-- Check slow queries\nSELECT\n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Monitor database size\nSELECT\n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n</code></pre>"},{"location":"troubleshooting/database-issues/#prevention-checklist","title":"Prevention Checklist","text":"<p>Database management best practices:</p> <ul> <li> Use health checks in Docker Compose</li> <li> Pin PostgreSQL version in containers</li> <li> Run migrations in controlled manner</li> <li> Monitor connection pool usage</li> <li> Regular backup validation</li> <li> Set up query performance monitoring</li> <li> Document schema changes</li> <li> Test migrations on copy of production data</li> <li> Use transactions for data modifications</li> <li> Implement proper error handling for database operations</li> </ul>"},{"location":"troubleshooting/deployment-issues/","title":"Deployment Troubleshooting","text":"<p>This document covers issues encountered during deployment and production operations of the iChrisBirch project.</p>"},{"location":"troubleshooting/deployment-issues/#service-recovery","title":"Service Recovery","text":"<p>When services are down in production, follow this emergency recovery procedure:</p>"},{"location":"troubleshooting/deployment-issues/#quick-service-recovery","title":"Quick Service Recovery","text":""},{"location":"troubleshooting/deployment-issues/#1-check-service-status","title":"1. Check Service Status","text":"<pre><code># Check all containers\ndocker ps -a\n\n# Check specific service\ndocker-compose -f docker-compose.prod.yml ps app\n\n# Check logs for errors\ndocker-compose -f docker-compose.prod.yml logs --tail=50 app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#2-restart-failed-services","title":"2. Restart Failed Services","text":"<pre><code># Restart single service\ndocker-compose -f docker-compose.prod.yml restart app\n\n# Restart all services\ndocker-compose -f docker-compose.prod.yml restart\n\n# Full recreation if needed\ndocker-compose -f docker-compose.prod.yml down\ndocker-compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#3-verify-recovery","title":"3. Verify Recovery","text":"<pre><code># Test endpoints\ncurl -f http://localhost/health\ncurl -f http://localhost/api/health\n\n# Check service logs\ndocker-compose -f docker-compose.prod.yml logs -f app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#build-and-deployment-failures","title":"Build and Deployment Failures","text":""},{"location":"troubleshooting/deployment-issues/#docker-build-failures","title":"Docker Build Failures","text":"<p>Problem: Production builds fail during deployment.</p> <p>Common Causes:</p> <ol> <li>Dependency conflicts</li> <li>Missing environment variables</li> <li>Resource limitations</li> </ol> <p>Diagnosis:</p> <pre><code># Build with verbose output\ndocker-compose -f docker-compose.prod.yml build --no-cache --progress=plain\n\n# Check build context\ndocker build --dry-run .\n\n# Verify Dockerfile syntax\ndocker build --target=production .\n</code></pre> <p>Resolution:</p> <pre><code># Clear Docker cache\ndocker system prune -f\ndocker builder prune -f\n\n# Rebuild with no cache\ndocker-compose -f docker-compose.prod.yml build --no-cache\n\n# Check resource usage\ndocker system df\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#environment-variable-issues","title":"Environment Variable Issues","text":"<p>Problem: Services fail due to missing or incorrect environment variables.</p> <p>Diagnosis:</p> <pre><code># Check environment in container\ndocker-compose -f docker-compose.prod.yml exec app env | grep -i postgres\n\n# Verify environment file\ncat .env.prod\n</code></pre> <p>Resolution:</p> <p>Ensure all required variables are set:</p> <pre><code># .env.prod\nDATABASE_URL=postgresql://user:pass@postgres:5432/ichrisbirch\nAPI_URL=https://yourdomain.com/api\nFLASK_ENV=production\nSECRET_KEY=your-secret-key-here\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#ssltls-certificate-issues","title":"SSL/TLS Certificate Issues","text":""},{"location":"troubleshooting/deployment-issues/#certificate-expiration","title":"Certificate Expiration","text":"<p>Problem: SSL certificates expire causing HTTPS errors.</p> <p>Diagnosis:</p> <pre><code># Check certificate expiration\necho | openssl s_client -connect yourdomain.com:443 | openssl x509 -noout -dates\n\n# Check Let's Encrypt certificates\nsudo certbot certificates\n</code></pre> <p>Resolution:</p> <pre><code># Renew certificates\nsudo certbot renew\n\n# Test renewal\nsudo certbot renew --dry-run\n\n# Restart nginx to load new certificates\ndocker-compose -f docker-compose.prod.yml restart nginx\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#certificate-installation-issues","title":"Certificate Installation Issues","text":"<p>Problem: New certificates not being recognized.</p> <p>Resolution:</p> <pre><code># Update nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -t\n\n# Reload nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -s reload\n\n# Restart nginx service\ndocker-compose -f docker-compose.prod.yml restart nginx\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#database-deployment-issues","title":"Database Deployment Issues","text":""},{"location":"troubleshooting/deployment-issues/#migration-failures","title":"Migration Failures","text":"<p>Problem: Database migrations fail during deployment.</p> <p>Safe Migration Process:</p> <pre><code># 1. Backup database first\ndocker-compose -f docker-compose.prod.yml exec postgres pg_dump \\\n  -U ichrisbirch ichrisbirch &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# 2. Test migrations on backup\ndocker-compose -f docker-compose.test.yml run test-runner \\\n  uv run alembic upgrade head\n\n# 3. Apply to production\ndocker-compose -f docker-compose.prod.yml exec app \\\n  uv run alembic upgrade head\n\n# 4. Verify migration\ndocker-compose -f docker-compose.prod.yml exec app \\\n  uv run python -c \"\nfrom ichrisbirch.database import get_sqlalchemy_session\nwith get_sqlalchemy_session() as session:\n    print('Migration successful')\n\"\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#database-connection-issues","title":"Database Connection Issues","text":"<p>Problem: Application cannot connect to production database.</p> <p>Diagnosis:</p> <pre><code># Test database connectivity\ndocker-compose -f docker-compose.prod.yml exec app \\\n  pg_isready -h postgres -p 5432\n\n# Check database logs\ndocker-compose -f docker-compose.prod.yml logs postgres\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/deployment-issues/#high-resource-usage","title":"High Resource Usage","text":"<p>Problem: Production services consuming too many resources.</p> <p>Monitoring:</p> <pre><code># Check container resource usage\ndocker stats\n\n# Check system resources\nhtop\ndf -h\nfree -h\n</code></pre> <p>Resolution:</p> <pre><code># docker-compose.prod.yml - Set resource limits\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '0.5'\n        reservations:\n          memory: 512M\n          cpus: '0.25'\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#slow-response-times","title":"Slow Response Times","text":"<p>Problem: Application responding slowly to requests.</p> <p>Diagnosis:</p> <pre><code># Check response times\ncurl -w \"@curl-format.txt\" -o /dev/null -s http://yourdomain.com/\n\n# Where curl-format.txt contains:\n#     time_namelookup:  %{time_namelookup}\\n\n#        time_connect:  %{time_connect}\\n\n#     time_appconnect:  %{time_appconnect}\\n\n#    time_pretransfer:  %{time_pretransfer}\\n\n#       time_redirect:  %{time_redirect}\\n\n#  time_starttransfer:  %{time_starttransfer}\\n\n#                     ----------\\n\n#          time_total:  %{time_total}\\n\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#load-balancer-and-proxy-issues","title":"Load Balancer and Proxy Issues","text":""},{"location":"troubleshooting/deployment-issues/#nginx-configuration-problems","title":"Nginx Configuration Problems","text":"<p>Problem: Nginx proxy not routing requests correctly.</p> <p>Common Issues:</p> <ol> <li>Upstream server unavailable</li> <li>Wrong proxy configuration</li> <li>SSL termination issues</li> </ol> <p>Diagnosis:</p> <pre><code># Test nginx configuration\ndocker-compose -f docker-compose.prod.yml exec nginx nginx -t\n\n# Check nginx logs\ndocker-compose -f docker-compose.prod.yml logs nginx\n\n# Test upstream connectivity\ndocker-compose -f docker-compose.prod.yml exec nginx \\\n  curl -f http://app:8000/health\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"troubleshooting/deployment-issues/#automated-backup-failures","title":"Automated Backup Failures","text":"<p>Problem: Scheduled backups are failing.</p> <p>Check Backup Status:</p> <pre><code># Check backup cron job\ncrontab -l\n\n# Check backup logs\ntail -f /var/log/backup.log\n\n# Test backup script manually\n./scripts/backup.sh\n</code></pre> <p>Backup Recovery Process:</p> <pre><code># List available backups\nls -la /backup/\n\n# Test backup integrity\npg_restore --list backup_20231201_120000.dump\n\n# Restore from backup if needed\ndocker-compose -f docker-compose.prod.yml down\ndocker volume rm ichrisbirch_postgres_data\ndocker-compose -f docker-compose.prod.yml up -d postgres\n\n# Wait for postgres to be ready\nsleep 30\n\n# Restore data\ndocker-compose -f docker-compose.prod.yml exec postgres \\\n  pg_restore -U ichrisbirch -d ichrisbirch \\\n  /backup/backup_20231201_120000.dump\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"troubleshooting/deployment-issues/#health-check-failures","title":"Health Check Failures","text":"<p>Problem: Health check endpoints returning errors.</p> <p>Diagnosis:</p> <pre><code># Test health endpoints\ncurl -f http://yourdomain.com/health\ncurl -f http://yourdomain.com/api/health\n\n# Check internal health\ndocker-compose -f docker-compose.prod.yml exec app \\\n  curl -f http://localhost:8000/health\n</code></pre> <p>Health Check Implementation:</p> <pre><code># In your application\nfrom fastapi import FastAPI, HTTPException\nfrom ichrisbirch.database import get_sqlalchemy_session\n\napp = FastAPI()\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Comprehensive health check.\"\"\"\n    try:\n        # Test database connection\n        with get_sqlalchemy_session() as session:\n            session.execute(\"SELECT 1\")\n\n        return {\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"timestamp\": datetime.utcnow().isoformat()\n        }\n    except Exception as e:\n        raise HTTPException(status_code=503, detail=f\"Unhealthy: {str(e)}\")\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#log-management","title":"Log Management","text":"<p>Problem: Logs growing too large or not being rotated.</p> <p>Log Rotation Setup:</p> <pre><code># Create logrotate configuration\nsudo tee /etc/logrotate.d/ichrisbirch &lt;&lt; EOF\n/var/log/ichrisbirch/*.log {\n    daily\n    rotate 30\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 0644 root root\n    postrotate\n        docker-compose -f docker-compose.prod.yml restart app\n    endscript\n}\nEOF\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#disaster-recovery-procedures","title":"Disaster Recovery Procedures","text":""},{"location":"troubleshooting/deployment-issues/#complete-service-recovery","title":"Complete Service Recovery","text":"<p>When everything is down:</p> <pre><code># 1. Check system resources\ndf -h\nfree -h\ndocker system df\n\n# 2. Stop all services\ndocker-compose -f docker-compose.prod.yml down\n\n# 3. Clean up if needed\ndocker system prune -f\n\n# 4. Restore from backup if necessary\n# (Follow backup recovery process above)\n\n# 5. Start services in order\ndocker-compose -f docker-compose.prod.yml up -d postgres\nsleep 30\ndocker-compose -f docker-compose.prod.yml up -d app\ndocker-compose -f docker-compose.prod.yml up -d nginx\n\n# 6. Verify services\n./scripts/verify_deployment.sh\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#data-recovery","title":"Data Recovery","text":"<p>If data is corrupted or lost:</p> <pre><code># 1. Stop application immediately\ndocker-compose -f docker-compose.prod.yml stop app\n\n# 2. Assess damage\ndocker-compose -f docker-compose.prod.yml exec postgres \\\n  psql -U ichrisbirch -c \"SELECT COUNT(*) FROM users;\"\n\n# 3. Restore from most recent backup\n# (Use backup recovery procedure)\n\n# 4. Verify data integrity\n# (Run data validation queries)\n\n# 5. Resume service\ndocker-compose -f docker-compose.prod.yml start app\n</code></pre>"},{"location":"troubleshooting/deployment-issues/#prevention-and-monitoring","title":"Prevention and Monitoring","text":""},{"location":"troubleshooting/deployment-issues/#deployment-checklist","title":"Deployment Checklist","text":"<p>Before each deployment:</p> <ul> <li> Run tests in staging environment</li> <li> Backup production database</li> <li> Verify environment variables</li> <li> Test migration scripts</li> <li> Check disk space and resources</li> <li> Verify SSL certificate validity</li> <li> Update deployment documentation</li> </ul>"},{"location":"troubleshooting/deployment-issues/#monitoring-setup","title":"Monitoring Setup","text":"<p>Essential monitoring:</p> <pre><code># System monitoring\nhtop\niotop\nnethogs\n\n# Docker monitoring\ndocker stats\ndocker system df\n\n# Application monitoring\ncurl -f http://yourdomain.com/health\n</code></pre> <p>Automated monitoring script:</p> <pre><code>#!/bin/bash\n# scripts/monitor.sh\n\n# Check service health\ncheck_service() {\n    if curl -f -s http://localhost/$1/health &gt; /dev/null; then\n        echo \"\u2713 $1 service healthy\"\n    else\n        echo \"\u2717 $1 service unhealthy\"\n        # Send alert here\n    fi\n}\n\ncheck_service \"api\"\ncheck_service \"\"  # Main app\n\n# Check disk space\nDISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')\nif [ \"$DISK_USAGE\" -gt 80 ]; then\n    echo \"\u26a0\ufe0f  Disk usage high: ${DISK_USAGE}%\"\nfi\n\n# Check memory\nMEMORY_USAGE=$(free | awk 'NR==2{print $3/$2 * 100.0}')\nif (( $(echo \"$MEMORY_USAGE &gt; 80\" | bc -l) )); then\n    echo \"\u26a0\ufe0f  Memory usage high: ${MEMORY_USAGE}%\"\nfi\n</code></pre> <p>Run monitoring periodically:</p> <pre><code># Add to crontab\n*/5 * * * * /path/to/scripts/monitor.sh &gt;&gt; /var/log/monitoring.log 2&gt;&amp;1\n</code></pre>"},{"location":"troubleshooting/development-issues/","title":"Development Environment Troubleshooting","text":"<p>This document covers issues related to setting up and maintaining the development environment for the iChrisBirch project.</p>"},{"location":"troubleshooting/development-issues/#initial-setup-issues","title":"Initial Setup Issues","text":""},{"location":"troubleshooting/development-issues/#python-version-compatibility","title":"Python Version Compatibility","text":"<p>Problem: Wrong Python version installed or being used.</p> <p>Symptoms:</p> <ul> <li><code>Python version not supported</code> errors</li> <li>Package installation failures</li> <li>Syntax errors with modern Python features</li> </ul> <p>Resolution:</p> <ol> <li>Verify Python version:</li> </ol> <pre><code>python --version  # Should be 3.12.x\nwhich python      # Verify correct Python binary\n</code></pre> <ol> <li>Install correct Python version:</li> </ol> <pre><code># macOS with Homebrew\nbrew install python@3.12\n\n# Ubuntu/Debian\nsudo apt update\nsudo apt install python3.12 python3.12-venv\n\n# Using pyenv (recommended)\npyenv install 3.12.0\npyenv local 3.12.0\n</code></pre>"},{"location":"troubleshooting/development-issues/#uv-package-manager-setup","title":"UV Package Manager Setup","text":"<p>Problem: UV not installed or not working correctly.</p> <p>Installation:</p> <pre><code># Install UV\npip install uv\n\n# Verify installation\nuv --version\n\n# Update UV\npip install --upgrade uv\n</code></pre> <p>Common UV Issues:</p> <ol> <li>Cache corruption:</li> </ol> <pre><code># Clear UV cache\nuv cache clean\n\n# Reinstall dependencies\nrm uv.lock\nuv sync\n</code></pre> <ol> <li>Virtual environment issues:</li> </ol> <pre><code># Recreate virtual environment\nrm -rf .venv\nuv sync\n</code></pre>"},{"location":"troubleshooting/development-issues/#docker-setup-problems","title":"Docker Setup Problems","text":"<p>Problem: Docker not installed or not working properly.</p> <p>Basic Docker Setup:</p> <pre><code># Verify Docker installation\ndocker --version\ndocker-compose --version\n\n# Test Docker functionality\ndocker run hello-world\n\n# Check Docker daemon status\ndocker info\n</code></pre> <p>Common Docker Issues:</p> <ol> <li>Permission issues on Linux:</li> </ol> <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and back in, or restart\n</code></pre> <ol> <li>Docker daemon not running:</li> </ol> <pre><code># Start Docker service (Linux)\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# macOS - Start Docker Desktop application\n</code></pre>"},{"location":"troubleshooting/development-issues/#environment-configuration-issues","title":"Environment Configuration Issues","text":""},{"location":"troubleshooting/development-issues/#environment-variables-not-loading","title":"Environment Variables Not Loading","text":"<p>Problem: Application can't find required environment variables.</p> <p>Diagnosis:</p> <pre><code># Check if environment file exists\nls -la .env*\n\n# Verify environment variables are loaded\ndocker-compose exec app env | grep -i postgres\n</code></pre> <p>Resolution:</p> <ol> <li>Create environment files:</li> </ol> <pre><code># Copy example files\ncp .env.example .env\ncp .env.test.example .env.test\n\n# Edit with your values\nvim .env\n</code></pre> <ol> <li>Verify Docker Compose loads environment:</li> </ol> <pre><code># docker-compose.yml\nservices:\n  app:\n    env_file:\n      - .env\n    environment:\n      - NODE_ENV=development\n</code></pre>"},{"location":"troubleshooting/development-issues/#configuration-conflicts","title":"Configuration Conflicts","text":"<p>Problem: Different configuration values between environments.</p> <p>Common Issues:</p> <ol> <li>Database URL mismatches</li> <li>API endpoint differences</li> <li>Debug settings conflicts</li> </ol> <p>Resolution:</p> <p>Create environment-specific configuration:</p> <pre><code># ichrisbirch/config.py\nimport os\nfrom typing import Literal\n\nclass Settings:\n    def __init__(self):\n        self.environment: Literal[\"development\", \"testing\", \"production\"] = \\\n            os.environ[\"ENVIRONMENT\"]\n\n        # Environment-specific settings\n        if self.environment == \"testing\":\n            self.database_url = os.environ[\"TEST_DATABASE_URL\"]\n            self.debug = True\n        elif self.environment == \"production\":\n            self.database_url = os.environ[\"PROD_DATABASE_URL\"]\n            self.debug = False\n        else:  # development\n            self.database_url = os.environ[\"DEV_DATABASE_URL\"]\n            self.debug = True\n\nsettings = Settings()\n</code></pre>"},{"location":"troubleshooting/development-issues/#ide-and-editor-issues","title":"IDE and Editor Issues","text":""},{"location":"troubleshooting/development-issues/#vs-code-extension-problems","title":"VS Code Extension Problems","text":"<p>Problem: Python extension not working or missing features.</p> <p>Resolution:</p> <ol> <li>Install required extensions:</li> </ol> <pre><code>code --install-extension ms-python.python\ncode --install-extension ms-python.pylint\ncode --install-extension ms-python.black-formatter\n</code></pre> <ol> <li>Configure Python interpreter:</li> </ol> <pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./.venv/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.formatting.provider\": \"black\"\n}\n</code></pre>"},{"location":"troubleshooting/development-issues/#import-resolution-issues","title":"Import Resolution Issues","text":"<p>Problem: IDE can't find imports or shows false errors.</p> <p>Common Causes:</p> <ol> <li>Wrong Python interpreter selected</li> <li>Package not installed in editable mode</li> <li>Missing <code>__init__.py</code> files</li> </ol> <p>Resolution:</p> <pre><code># Install package in editable mode\nuv sync\n\n# Verify package is installed\nuv run python -c \"import ichrisbirch; print(ichrisbirch.__file__)\"\n\n# Check Python path\nuv run python -c \"import sys; print('\\n'.join(sys.path))\"\n</code></pre>"},{"location":"troubleshooting/development-issues/#development-workflow-issues","title":"Development Workflow Issues","text":""},{"location":"troubleshooting/development-issues/#git-configuration-problems","title":"Git Configuration Problems","text":"<p>Problem: Git not configured or authentication issues.</p> <p>Setup:</p> <pre><code># Configure Git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n# Configure SSH key for GitHub\nssh-keygen -t ed25519 -C \"your.email@example.com\"\ncat ~/.ssh/id_ed25519.pub  # Add to GitHub\n\n# Test SSH connection\nssh -T git@github.com\n</code></pre>"},{"location":"troubleshooting/development-issues/#pre-commit-hook-issues","title":"Pre-commit Hook Issues","text":"<p>Problem: Pre-commit hooks fail or don't run.</p> <p>Setup:</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n\n# Skip hooks if needed (emergency only)\ngit commit --no-verify\n</code></pre>"},{"location":"troubleshooting/development-issues/#code-formatting-conflicts","title":"Code Formatting Conflicts","text":"<p>Problem: Different formatting tools producing conflicting results.</p> <p>Resolution:</p> <p>Create consistent configuration:</p> <pre><code># pyproject.toml\n[tool.black]\nline-length = 88\ntarget-version = ['py312']\n\n[tool.isort]\nprofile = \"black\"\nline_length = 88\n\n[tool.pylint.messages_control]\nmax-line-length = 88\n</code></pre>"},{"location":"troubleshooting/development-issues/#performance-and-resource-issues","title":"Performance and Resource Issues","text":""},{"location":"troubleshooting/development-issues/#slow-development-environment","title":"Slow Development Environment","text":"<p>Problem: Local development is slow or unresponsive.</p> <p>Common Causes:</p> <ol> <li>Insufficient resources for Docker</li> <li>Too many services running</li> <li>Large log files</li> </ol> <p>Resolution:</p> <ol> <li>Optimize Docker resources:</li> </ol> <pre><code># docker-compose.yml - limit resource usage\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n        reservations:\n          memory: 256M\n</code></pre> <ol> <li>Selective service startup:</li> </ol> <pre><code># Start only essential services\ndocker-compose up app postgres\n\n# Start additional services as needed\ndocker-compose up redis nginx\n</code></pre> <ol> <li>Clean up logs:</li> </ol> <pre><code># Rotate log files\ndocker-compose logs --no-color &gt; logs/app.log\ndocker-compose down\ndocker system prune -f\n</code></pre>"},{"location":"troubleshooting/development-issues/#port-conflicts","title":"Port Conflicts","text":"<p>Problem: Ports already in use by other applications.</p> <p>Diagnosis:</p> <pre><code># Check what's using specific port\nlsof -i :8000\nnetstat -tulpn | grep :8000\n\n# Kill process using port\nkill -9 $(lsof -t -i:8000)\n</code></pre> <p>Resolution:</p> <ol> <li>Change ports in docker-compose:</li> </ol> <pre><code>services:\n  app:\n    ports:\n      - \"8001:8000\"  # Use different external port\n</code></pre> <ol> <li>Use different ports for different projects:</li> </ol> <pre><code># Project-specific environment variables\necho \"APP_PORT=8001\" &gt;&gt; .env\necho \"POSTGRES_PORT=5433\" &gt;&gt; .env\n</code></pre>"},{"location":"troubleshooting/development-issues/#debugging-and-troubleshooting-tools","title":"Debugging and Troubleshooting Tools","text":""},{"location":"troubleshooting/development-issues/#development-debugging","title":"Development Debugging","text":"<p>Enable Debug Mode:</p> <pre><code># In your application\nimport logging\n\n# Set up debug logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n# Add debug prints\nlogger.debug(f\"Settings loaded: {settings}\")\nlogger.debug(f\"Database URL: {settings.database_url}\")\n</code></pre>"},{"location":"troubleshooting/development-issues/#container-debugging","title":"Container Debugging","text":"<p>Access container shell:</p> <pre><code># Get shell in running container\ndocker-compose exec app bash\n\n# Run container with override\ndocker-compose run --rm app bash\n\n# Debug specific service\ndocker-compose run --rm --entrypoint=\"\" app bash\n</code></pre>"},{"location":"troubleshooting/development-issues/#network-debugging","title":"Network Debugging","text":"<p>Test service connectivity:</p> <pre><code># Test from host to container\ncurl http://localhost:8000/health\n\n# Test container to container\ndocker-compose exec app curl http://api:8000/health\n\n# Check DNS resolution\ndocker-compose exec app nslookup postgres\n</code></pre>"},{"location":"troubleshooting/development-issues/#environment-validation-script","title":"Environment Validation Script","text":"<p>Create a validation script to check environment setup:</p> <pre><code>#!/usr/bin/env python3\n# scripts/validate_dev_env.py\n\"\"\"\nDevelopment environment validation script.\nRun this to verify your development setup is correct.\n\"\"\"\n\nimport sys\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef check_command(command: str, name: str) -&gt; bool:\n    \"\"\"Check if a command is available.\"\"\"\n    try:\n        result = subprocess.run(\n            [command, \"--version\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            print(f\"\u2713 {name}: {result.stdout.strip()}\")\n            return True\n    except FileNotFoundError:\n        pass\n\n    print(f\"\u2717 {name}: Not found\")\n    return False\n\ndef check_file(filepath: str, name: str) -&gt; bool:\n    \"\"\"Check if a file exists.\"\"\"\n    if Path(filepath).exists():\n        print(f\"\u2713 {name}: Found\")\n        return True\n    else:\n        print(f\"\u2717 {name}: Missing\")\n        return False\n\ndef check_docker_compose() -&gt; bool:\n    \"\"\"Check if Docker Compose services can start.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"docker-compose\", \"config\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            print(\"\u2713 Docker Compose: Configuration valid\")\n            return True\n        else:\n            print(f\"\u2717 Docker Compose: Configuration error\\n{result.stderr}\")\n            return False\n    except Exception as e:\n        print(f\"\u2717 Docker Compose: {e}\")\n        return False\n\ndef main():\n    \"\"\"Run all validation checks.\"\"\"\n    print(\"\ud83d\udd0d Validating development environment...\\n\")\n\n    checks = [\n        # Core tools\n        (lambda: check_command(\"python3\", \"Python\"), \"Python 3\"),\n        (lambda: check_command(\"uv\", \"UV Package Manager\"), \"UV\"),\n        (lambda: check_command(\"docker\", \"Docker\"), \"Docker\"),\n        (lambda: check_command(\"docker-compose\", \"Docker Compose\"), \"Docker Compose\"),\n        (lambda: check_command(\"git\", \"Git\"), \"Git\"),\n\n        # Configuration files\n        (lambda: check_file(\".env\", \".env file\"), \"Environment file\"),\n        (lambda: check_file(\"pyproject.toml\", \"pyproject.toml\"), \"Project config\"),\n        (lambda: check_file(\"docker-compose.yml\", \"docker-compose.yml\"), \"Docker Compose config\"),\n\n        # Docker validation\n        (check_docker_compose, \"Docker Compose validation\"),\n    ]\n\n    passed = 0\n    total = len(checks)\n\n    for check_func, name in checks:\n        if check_func():\n            passed += 1\n        print()\n\n    # Summary\n    print(f\"\ud83d\udcca Validation Summary: {passed}/{total} checks passed\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 Development environment is ready!\")\n        return 0\n    else:\n        print(\"\u274c Some issues need to be resolved before development\")\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>Run the validation script:</p> <pre><code># Make executable\nchmod +x scripts/validate_dev_env.py\n\n# Run validation\n./scripts/validate_dev_env.py\n</code></pre>"},{"location":"troubleshooting/development-issues/#quick-setup-guide","title":"Quick Setup Guide","text":"<p>For new developers, here's a quick setup checklist:</p> <pre><code># 1. Clone repository\ngit clone https://github.com/username/ichrisbirch.git\ncd ichrisbirch\n\n# 2. Copy environment files\ncp .env.example .env\ncp .env.test.example .env.test\n\n# 3. Install UV\npip install uv\n\n# 4. Install dependencies\nuv sync\n\n# 5. Start development environment\ndocker-compose up -d\n\n# 6. Run validation\n./scripts/validate_dev_env.py\n\n# 7. Run tests to verify setup\ndocker-compose -f docker-compose.test.yml up test-runner\n</code></pre>"},{"location":"troubleshooting/development-issues/#common-development-commands","title":"Common Development Commands","text":"<p>Keep these handy for daily development:</p> <pre><code># Start development environment\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f app\n\n# Run tests\ndocker-compose -f docker-compose.test.yml up test-runner\n\n# Access application shell\ndocker-compose exec app bash\n\n# Install new package\nuv add package-name\n\n# Run database migrations\ndocker-compose exec app uv run alembic upgrade head\n\n# Format code\nuv run black ichrisbirch/\nuv run isort ichrisbirch/\n\n# Lint code\nuv run pylint ichrisbirch/\n\n# Stop all services\ndocker-compose down\n\n# Clean up Docker resources\ndocker system prune -f\ndocker-compose down -v  # Remove volumes too\n</code></pre>"},{"location":"troubleshooting/docker-issues/","title":"Docker Configuration Troubleshooting","text":"<p>This document covers common Docker-related issues encountered during development and deployment of the iChrisBirch project, including build failures, networking problems, and container orchestration issues.</p>"},{"location":"troubleshooting/docker-issues/#multi-stage-build-issues","title":"Multi-Stage Build Issues","text":""},{"location":"troubleshooting/docker-issues/#missing-executables-in-container-stages","title":"Missing Executables in Container Stages","text":"<p>Problem: Files copied from previous stages don't work or are missing.</p> <p>Symptoms:</p> <ul> <li><code>/app/.venv/bin/pytest: No such file or directory</code></li> <li>Commands work in builder stage but fail in runtime stage</li> <li>Broken symlinks after <code>COPY --from=builder</code></li> </ul> <p>Root Cause: Package managers like Poetry create complex directory structures with symlinks that don't survive Docker layer copying.</p> <p>Resolution: Use UV instead of Poetry for better Docker compatibility:</p> <pre><code># Good: UV creates self-contained installations\nFROM python:3.12-slim as builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\nFROM builder as development\nCOPY . .\nRUN uv sync --frozen --group dev --group test\n</code></pre> <p>Prevention:</p> <ul> <li>Test each stage individually: <code>docker build --target=development .</code></li> <li>Verify executables exist: <code>docker run --rm image which pytest</code></li> <li>Use package managers designed for containers</li> </ul>"},{"location":"troubleshooting/docker-issues/#container-networking-issues","title":"Container Networking Issues","text":""},{"location":"troubleshooting/docker-issues/#service-communication-failures","title":"Service Communication Failures","text":"<p>Problem: Services can't communicate with each other in Docker Compose.</p> <p>Symptoms:</p> <ul> <li><code>Connection refused</code> errors between services</li> <li>Services start but can't reach database/API</li> <li>Works locally but fails in containers</li> </ul> <p>Common Causes:</p> <ol> <li>Wrong hostname: Using <code>localhost</code> instead of service name</li> <li>Port mapping confusion: Mixing internal and external ports</li> <li>Network isolation: Services on different networks</li> </ol> <p>Resolution:</p> <pre><code># docker-compose.yml\nservices:\n  app:\n    environment:\n      - DATABASE_URL=postgresql://user:pass@postgres:5432/db\n      # Use service name 'postgres', not 'localhost'\n    networks:\n      - app-network\n\n  postgres:\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n    driver: bridge\n</code></pre> <p>Debugging Steps:</p> <pre><code># Check service connectivity\ndocker-compose exec app ping postgres\n\n# Verify environment variables\ndocker-compose exec app env | grep DATABASE\n\n# Check port availability\ndocker-compose exec app nc -zv postgres 5432\n</code></pre>"},{"location":"troubleshooting/docker-issues/#build-context-and-performance","title":"Build Context and Performance","text":""},{"location":"troubleshooting/docker-issues/#large-build-contexts","title":"Large Build Contexts","text":"<p>Problem: Docker builds are slow due to large context.</p> <p>Symptoms:</p> <ul> <li><code>Sending build context to Docker daemon</code> takes minutes</li> <li>Builds timeout or run out of space</li> <li>Unnecessary files in containers</li> </ul> <p>Resolution:</p> <p>Create comprehensive <code>.dockerignore</code>:</p> <pre><code># Version control\n.git/\n.gitignore\n\n# Python\n__pycache__/\n*.pyc\n.pytest_cache/\n\n# Development\n.vscode/\n*.log\nnode_modules/\n\n# Documentation\ndocs/\n*.md\n\n# CI/CD\n.github/\n\n# Local development\n.env.local\ndocker-compose.override.yml\n</code></pre> <p>Performance Tips:</p> <ul> <li>Put frequently changing files (source code) after stable dependencies</li> <li>Use multi-stage builds to separate build dependencies from runtime</li> <li>Cache dependency installation layers</li> </ul>"},{"location":"troubleshooting/docker-issues/#layer-caching-issues","title":"Layer Caching Issues","text":"<p>Problem: Dependencies reinstall on every build despite no changes.</p> <p>Common Mistake:</p> <pre><code># Bad: Any file change invalidates dependency cache\nCOPY . .\nRUN uv sync --frozen\n</code></pre> <p>Correct Approach:</p> <pre><code># Good: Cache dependencies separately\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\n# Source code in separate layer\nCOPY . .\nRUN uv sync --frozen --group dev\n</code></pre>"},{"location":"troubleshooting/docker-issues/#environment-variable-management","title":"Environment Variable Management","text":""},{"location":"troubleshooting/docker-issues/#missing-environment-variables","title":"Missing Environment Variables","text":"<p>Problem: Services fail to start with configuration errors.</p> <p>Symptoms:</p> <ul> <li>Services exit immediately after starting</li> <li>\"Environment variable not set\" errors</li> <li>Database connection failures</li> </ul> <p>Resolution:</p> <ol> <li>Use environment files:</li> </ol> <pre><code># docker-compose.yml\nservices:\n  app:\n    env_file:\n      - .env\n      - .env.local  # Optional overrides\n</code></pre> <ol> <li>Provide defaults:</li> </ol> <pre><code>services:\n  app:\n    environment:\n      - POSTGRES_DB=${POSTGRES_DB:-ichrisbirch}\n      - DEBUG=${DEBUG:-false}\n</code></pre> <ol> <li>Validate environment:</li> </ol> <pre><code># In your application startup\nfrom ichrisbirch.config import settings\n\n# Let it fail fast if misconfigured\nassert settings.database_url, \"DATABASE_URL must be set\"\n</code></pre>"},{"location":"troubleshooting/docker-issues/#volume-and-persistence-issues","title":"Volume and Persistence Issues","text":""},{"location":"troubleshooting/docker-issues/#database-data-loss","title":"Database Data Loss","text":"<p>Problem: Database data disappears when containers restart.</p> <p>Cause: No persistent volume configured for database.</p> <p>Resolution:</p> <pre><code>services:\n  postgres:\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n    driver: local\n</code></pre>"},{"location":"troubleshooting/docker-issues/#file-permission-problems","title":"File Permission Problems","text":"<p>Problem: Files created in containers have wrong ownership.</p> <p>Symptoms:</p> <ul> <li>Permission denied when accessing files from host</li> <li>Files owned by root instead of user</li> <li>Build failures due to permission issues</li> </ul> <p>Resolution:</p> <pre><code># Match host user ID in development\nARG UID=1000\nARG GID=1000\nRUN groupadd -g $GID appgroup &amp;&amp; \\\n    useradd -u $UID -g $GID -m appuser\nUSER appuser\n</code></pre> <p>Or in docker-compose for development:</p> <pre><code>services:\n  app:\n    user: \"${UID:-1000}:${GID:-1000}\"\n</code></pre>"},{"location":"troubleshooting/docker-issues/#service-orchestration-issues","title":"Service Orchestration Issues","text":""},{"location":"troubleshooting/docker-issues/#service-startup-order","title":"Service Startup Order","text":"<p>Problem: Services start before dependencies are ready.</p> <p>Symptoms:</p> <ul> <li>Application fails to connect to database during startup</li> <li>Race conditions in service initialization</li> <li>Containers restart repeatedly</li> </ul> <p>Resolution:</p> <p>Use <code>depends_on</code> with health checks:</p> <pre><code>services:\n  app:\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  postgres:\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n</code></pre>"},{"location":"troubleshooting/docker-issues/#resource-constraints","title":"Resource Constraints","text":"<p>Problem: Services crash due to insufficient resources.</p> <p>Resolution:</p> <p>Set resource limits:</p> <pre><code>services:\n  app:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n        reservations:\n          memory: 256M\n</code></pre>"},{"location":"troubleshooting/docker-issues/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/docker-issues/#container-inspection","title":"Container Inspection","text":"<pre><code># Check running containers\ndocker-compose ps\n\n# View logs\ndocker-compose logs app\ndocker-compose logs -f --tail=100 app\n\n# Execute commands in container\ndocker-compose exec app bash\ndocker-compose exec app uv run python -c \"import sys; print(sys.path)\"\n\n# Inspect container configuration\ndocker inspect $(docker-compose ps -q app)\n</code></pre>"},{"location":"troubleshooting/docker-issues/#network-debugging","title":"Network Debugging","text":"<pre><code># List networks\ndocker network ls\n\n# Inspect network configuration\ndocker network inspect ichrisbirch_default\n\n# Test connectivity between services\ndocker-compose exec app ping postgres\ndocker-compose exec app nc -zv postgres 5432\n</code></pre>"},{"location":"troubleshooting/docker-issues/#build-debugging","title":"Build Debugging","text":"<pre><code># Build with verbose output\ndocker-compose build --progress=plain --no-cache\n\n# Build specific stage\ndocker build --target=development .\n\n# Inspect intermediate stages\ndocker build --rm=false .\ndocker run -it &lt;intermediate-id&gt; bash\n</code></pre>"},{"location":"troubleshooting/docker-issues/#common-error-messages","title":"Common Error Messages","text":""},{"location":"troubleshooting/docker-issues/#no-such-file-or-directory","title":"\"No such file or directory\"","text":"<p>Error: <code>/app/.venv/bin/pytest: No such file or directory</code></p> <p>Cause: Broken symlinks in virtual environment after Docker layer copy.</p> <p>Fix: Use UV instead of Poetry, ensure proper multi-stage build.</p>"},{"location":"troubleshooting/docker-issues/#connection-refused","title":"\"Connection refused\"","text":"<p>Error: <code>psycopg2.OperationalError: connection to server at \"localhost\" (127.0.0.1) port 5432 refused</code></p> <p>Cause: Using <code>localhost</code> instead of Docker service name.</p> <p>Fix: Update connection string to use service name: <code>postgres:5432</code></p>"},{"location":"troubleshooting/docker-issues/#port-already-in-use","title":"\"Port already in use\"","text":"<p>Error: <code>Error starting userland proxy: listen tcp 0.0.0.0:5432: bind: address already in use</code></p> <p>Cause: Port conflict with host system or another container.</p> <p>Fix: Change external port mapping or stop conflicting service.</p>"},{"location":"troubleshooting/docker-issues/#prevention-checklist","title":"Prevention Checklist","text":"<ul> <li> Use <code>.dockerignore</code> to exclude unnecessary files</li> <li> Test each Docker stage individually</li> <li> Use service names for inter-service communication</li> <li> Configure health checks for critical services</li> <li> Set up persistent volumes for data</li> <li> Document required environment variables</li> <li> Test builds on clean systems (CI/CD)</li> <li> Monitor resource usage and set limits</li> </ul>"},{"location":"troubleshooting/poetry-uv-migration/","title":"Poetry to UV Migration Troubleshooting","text":"<p>This document chronicles the complete migration from Poetry to UV package management, including all issues encountered, solutions attempted, and the final working configuration.</p>"},{"location":"troubleshooting/poetry-uv-migration/#background","title":"Background","text":"<p>The iChrisBirch project originally used Poetry for dependency management but encountered significant issues with Docker containerization, particularly with virtual environment handling and pytest execution in containerized environments.</p> <p>Poetry Docker Incompatibility</p> <p>Poetry's virtual environment copying between Docker stages is fundamentally broken in containerized environments, leading to missing executables and broken symlinks.</p>"},{"location":"troubleshooting/poetry-uv-migration/#issue-timeline","title":"Issue Timeline","text":""},{"location":"troubleshooting/poetry-uv-migration/#initial-problem","title":"Initial Problem","text":"<p>Error Encountered:</p> <pre><code>docker-compose -f docker-compose.test.yml up test-runner\n# Output: /app/.venv/bin/pytest: No such file or directory\n</code></pre> <p>Root Cause Analysis: Poetry creates virtual environments with complex symlink structures that don't survive Docker's multi-stage build copying. The <code>.venv/bin/pytest</code> executable was missing despite Poetry claiming successful installation.</p>"},{"location":"troubleshooting/poetry-uv-migration/#attempted-solutions-that-failed","title":"Attempted Solutions (That Failed)","text":""},{"location":"troubleshooting/poetry-uv-migration/#1-fix-poetry-docker-configuration","title":"1. Fix Poetry Docker Configuration","text":"<p>What We Tried:</p> <pre><code># Attempted fix in Dockerfile\nENV POETRY_VENV_IN_PROJECT=1\nENV POETRY_NO_INTERACTION=1\nCOPY --from=builder /app/.venv /app/.venv\n</code></pre> <p>Why It Failed: Poetry's virtual environment structure with symlinks doesn't copy correctly between Docker layers. The symlinks become broken references.</p>"},{"location":"troubleshooting/poetry-uv-migration/#2-manual-pytest-installation","title":"2. Manual Pytest Installation","text":"<p>What We Tried:</p> <pre><code>RUN .venv/bin/pip install pytest pytest-cov\n</code></pre> <p>Why It Failed: The <code>.venv/bin/pip</code> itself was a broken symlink, creating a circular dependency problem.</p>"},{"location":"troubleshooting/poetry-uv-migration/#3-poetry-path-modifications","title":"3. Poetry Path Modifications","text":"<p>What We Tried:</p> <pre><code>ENV PATH=\"/app/.venv/bin:$PATH\"\nRUN which pytest  # Still not found\n</code></pre> <p>Why It Failed: Setting PATH doesn't fix broken symlinks; the underlying executable files were never properly copied.</p>"},{"location":"troubleshooting/poetry-uv-migration/#final-resolution-complete-migration-to-uv","title":"Final Resolution: Complete Migration to UV","text":""},{"location":"troubleshooting/poetry-uv-migration/#why-uv-was-chosen","title":"Why UV Was Chosen","text":"<ol> <li>Better Docker Support: UV handles containerized environments correctly</li> <li>Faster Installation: UV is significantly faster than Poetry</li> <li>PEP 621 Compliance: Uses modern Python packaging standards</li> <li>Simplified Dependencies: Cleaner dependency group management</li> </ol>"},{"location":"troubleshooting/poetry-uv-migration/#migration-steps","title":"Migration Steps","text":""},{"location":"troubleshooting/poetry-uv-migration/#1-convert-pyprojecttoml","title":"1. Convert pyproject.toml","text":"<p>Before (Poetry format):</p> <pre><code>[tool.poetry]\nname = \"ichrisbirch\"\nversion = \"0.1.0\"\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nfastapi = \"^0.104.1\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.4.3\"\n</code></pre> <p>After (UV/PEP 621 format):</p> <pre><code>[project]\nname = \"ichrisbirch\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"fastapi&gt;=0.104.1\",\n]\n\n[dependency-groups]\ndev = [\n    \"pytest&gt;=7.4.3\",\n]\ntest = [\n    \"pytest&gt;=7.4.3\",\n    \"pytest-cov&gt;=4.1.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#2-update-dockerfile","title":"2. Update Dockerfile","text":"<p>Before (Poetry):</p> <pre><code>FROM python:3.12-slim as builder\nRUN pip install poetry\nCOPY pyproject.toml poetry.lock ./\nRUN poetry config venv.in-project true &amp;&amp; poetry install --no-root\n</code></pre> <p>After (UV):</p> <pre><code>FROM python:3.12-slim as builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY pyproject.toml uv.lock ./\nRUN uv sync --frozen --no-dev\n\nFROM builder as development\nCOPY . .\nRUN uv sync --frozen --group dev --group test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#3-update-docker-compose","title":"3. Update Docker Compose","text":"<p>Before (Poetry):</p> <pre><code>test-runner:\n  build:\n    target: development\n  command: [\"poetry\", \"run\", \"pytest\", \"-vv\", \"--cov=ichrisbirch\"]\n</code></pre> <p>After (UV):</p> <pre><code>test-runner:\n  build:\n    target: development\n  command: [\"uv\", \"run\", \"pytest\", \"-vv\", \"--cov=ichrisbirch\"]\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#4-update-cicd-workflows","title":"4. Update CI/CD Workflows","text":"<p>Before (Poetry):</p> <pre><code>- name: Install Poetry\n  run: pip install poetry\n- name: Install dependencies\n  run: poetry install\n- name: Run tests\n  run: poetry run pytest\n</code></pre> <p>After (UV):</p> <pre><code>- name: Install UV\n  run: pip install uv\n- name: Install dependencies\n  run: uv sync --frozen --group dev --group test\n- name: Run tests\n  run: uv run pytest\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#additional-issues-resolved","title":"Additional Issues Resolved","text":""},{"location":"troubleshooting/poetry-uv-migration/#database-schema-creation","title":"Database Schema Creation","text":"<p>Issue: Tests failing because database schema wasn't created in test environment.</p> <p>Root Cause: <code>POSTGRES_DB_SCHEMA</code> environment variable missing from test configuration.</p> <p>Solution:</p> <pre><code># docker-compose.test.yml\nservices:\n  test-runner:\n    environment:\n      - POSTGRES_DB_SCHEMA=ichrisbirch_test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#package-source-code-access","title":"Package Source Code Access","text":"<p>Issue: UV couldn't access package source code for local development.</p> <p>Solution: Ensure source code is copied before <code>uv sync</code>:</p> <pre><code>FROM builder as development\nCOPY . .  # Copy source before sync\nRUN uv sync --frozen --group dev --group test\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#verification-steps","title":"Verification Steps","text":"<p>After migration, verify everything works:</p> <pre><code># 1. Check UV installation\ndocker-compose -f docker-compose.test.yml build\n\n# 2. Verify pytest is available\ndocker-compose -f docker-compose.test.yml run test-runner uv run which pytest\n\n# 3. Run actual tests\ndocker-compose -f docker-compose.test.yml up test-runner\n\n# 4. Check test database connection\ndocker-compose -f docker-compose.test.yml run test-runner uv run python -c \"\nfrom ichrisbirch.config import settings\nprint(f'Database URL: {settings.database_url}')\n\"\n</code></pre>"},{"location":"troubleshooting/poetry-uv-migration/#benefits-achieved","title":"Benefits Achieved","text":"<ol> <li>Working Tests: Docker test environment now functions correctly</li> <li>Faster Builds: UV installation is significantly faster than Poetry</li> <li>Simpler Configuration: Fewer environment variables and cleaner setup</li> <li>Better CI/CD: More reliable GitHub Actions workflows</li> <li>Future-Proof: Using modern PEP 621 standards</li> </ol>"},{"location":"troubleshooting/poetry-uv-migration/#prevention","title":"Prevention","text":"<p>To avoid similar issues in the future:</p> <ol> <li>Test Docker Builds Regularly: Don't let Docker configurations drift</li> <li>Use UV for New Projects: UV has better containerization support</li> <li>Validate Multi-Stage Builds: Ensure all stages have required executables</li> <li>Monitor Dependency Management: Keep dependency management tools updated</li> <li>Document Environment Variables: All required variables must be documented</li> </ol> <p>Migration Checklist</p> <p>When migrating package managers:</p> <ul> <li> Convert pyproject.toml format</li> <li> Update all Dockerfiles</li> <li> Modify Docker Compose files</li> <li> Update CI/CD workflows</li> <li> Update deployment scripts</li> <li> Test all environments (dev, test, prod)</li> <li> Update documentation</li> </ul>"}]}